"""
Powerful PostgreSQL Vector Database Store with Custom Table Support.
"""
from typing import (
    Any,
    Dict,
    Iterable,
    List,
    Tuple,
    Union,
    Optional,
    Sequence
)
from collections.abc import Callable
import asyncio
import uuid
import traceback
import numpy as np
# SQL Alchemy
import sqlalchemy
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, String, ARRAY, Float, JSON, text, func, inspect
from sqlalchemy.dialects.postgresql import JSON, JSONB, JSONPATH, UUID, insert
from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine
from sqlalchemy.future import select
from sqlalchemy.ext.asyncio import AsyncSession
# PgVector
from pgvector.sqlalchemy import Vector  # type: ignore
# Langchain
from langchain_core.embeddings import Embeddings
from langchain.docstore.document import Document
from langchain.memory import VectorStoreRetrieverMemory
from langchain_community.vectorstores.pgembedding import PGEmbedding
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_postgres.vectorstores import (
    PGVector,
    _get_embedding_collection_store,
    _results_to_docs
)
from langchain_postgres._utils import maximal_marginal_relevance
from datamodel.parsers.json import json_encoder  # pylint: disable=E0611
from navconfig.logging import logging
from .abstract import AbstractStore
from ..conf import default_sqlalchemy_pg


Base = declarative_base()


# Define the async classmethods to be attached to our ORM model.
async def aget_by_name(cls, session: AsyncSession, name: str) -> Optional["CustomEmbeddingStore"]:
    return cls(cmetadata={})

class PgVector(PGVector):
    """
    PgVector extends PGVector so that it uses an existing table from a specified schema.
    Enhanced PgVector that supports:
    - Configurable embedding column name
    - Different distance strategies (COSINE, L2/EUCLIDEAN, MAX_INNER_PRODUCT)
    - Working with existing tables from specified schemas

    When instantiating, you provide:
    - connection: an AsyncEngine (or synchronous engine) to your PostgreSQL database.
    - schema: the database schema where your table lives.
    - table_name: the name of the table that stores the embeddings.
    - embedding_length: the dimension of the embedding vectors.
    - embeddings: your embedding function/model (which must provide embed_query).

    This implementation overrides the _get_embedding_collection_store method to return a tuple of
    ORM model classes that both refer to your table. It validates (using SQLAlchemyâ€™s inspector)
    that the table contains the required columns: 'id', 'embedding', 'document', and 'cmetadata'.

    The returned ORM models can then be used by PGVectorâ€™s built-in similarity search and retriever.
    """
    def __init__(
        self,
        embeddings: Embeddings,
        *,
        table_name: str = None,
        schema: str = 'public',
        collection_name: Optional[str] = None,
        id_column: str = 'id',
        embedding_column: str = 'embedding',
        distance_strategy: DistanceStrategy = DistanceStrategy.COSINE,
        score_threshold: Optional[float] = None,
        use_uuid: bool = False,
        **kwargs
    ) -> None:
        self.table_name = table_name
        self.schema = schema
        self._id_column: str = id_column
        self._embedding_column: str = embedding_column
        self._distance_strategy = distance_strategy
        self._score_threshold: float = score_threshold
        self._schema_based: bool = False
        if self.table_name:
            self._schema_based: bool = True
        elif '.' in collection_name:
            self.schema, self.table_name = collection_name.split('.')
            self._schema_based: bool = True
        self._use_uuid: bool = use_uuid
        super().__init__(
            embeddings=embeddings,
            collection_name=collection_name,
            distance_strategy=distance_strategy,
            **kwargs
        )
        self.logger = logging.getLogger(name='PgVector')

    def get_id_column(self, use_uuid: bool) -> sqlalchemy.Column:
        """
        Return the ID column definition based on whether to use UUID or not.
        If use_uuid is True, the ID column will be a PostgreSQL UUID type with
        server-side generation using uuid_generate_v4().
        If use_uuid is False, the ID column will be a String type with a default
        value generated by Python's uuid.uuid4() function.
        """
        if use_uuid:
            # DB will auto-generate UUID; SQLAlchemy should not set a default!
            return sqlalchemy.Column(
                sqlalchemy.dialects.postgresql.UUID(as_uuid=True),
                primary_key=True,
                index=True,
                unique=True,
                server_default=sqlalchemy.text('uuid_generate_v4()')
            )
        else:
            # Python generates UUID (as string)
            return sqlalchemy.Column(
                sqlalchemy.String,
                primary_key=True,
                index=True,
                unique=True,
                default=lambda: str(uuid.uuid4())
            )

    async def _get_embedding_collection_store(
        self,
        table: str,
        schema: str,
        dimension: int = 384,
        **kwargs
    ) -> Tuple[type, type]:
        """
        Return custom ORM model classes (EmbeddingStore, CollectionStore)
        that both reference the same table.

        In this custom implementation, both the "collection" and "embedding" stores
        are represented by a single table.
        The table is expected to have the following columns:
        - id: unique identifier (String)
        - embedding: the vector column (Vector(dimension))
        - document: text column containing the document
        - cmetadata: JSONB column for metadata

        Raises an error if the table does not have the required schema.
        """
        # Dynamically create the model class.
        attrs = {
            '__tablename__': table,
            '__table_args__': {"schema": schema},
            self._id_column: self.get_id_column(use_uuid=self._use_uuid),
            self._embedding_column: sqlalchemy.Column(Vector(dimension)),
            'collection_id': sqlalchemy.Column(
                sqlalchemy.dialects.postgresql.UUID(as_uuid=True),
                default='00000000-0000-0000-0000-000000000000',
                nullable=False,
                index=True
            ),
            'text': sqlalchemy.Column(sqlalchemy.String, nullable=True),
            'document': sqlalchemy.Column(sqlalchemy.String, nullable=True),
            'cmetadata': sqlalchemy.Column(JSONB, nullable=True),
            # Attach the async classmethods.
            'aget_by_name': classmethod(aget_by_name),
            # '__aquery_collection': classmethod(__aquery_collection),
            # 'aget_or_create': classmethod(aget_or_create)
        }
        EmbeddingStore = type("CustomEmbeddingStore", (Base,), attrs)
        EmbeddingStore.__name__ = "EmbeddingStore"
        return (EmbeddingStore, EmbeddingStore)

    async def _PgVector__aquery_collection(
        self,
        session: AsyncSession,
        embedding: List[float],
        k: int = 4,
        score_threshold: Optional[float] = None,
        filter: Optional[Dict[str, str]] = None,
    ) -> Sequence[Any]:
        """
        Query directly from the current table (no collection filtering/join).
        """
        filter_by = []

        # If you want to support JSONB metadata filtering
        if filter:
            if self.use_jsonb:
                filter_clauses = self._create_filter_clause(filter)
                if filter_clauses is not None:
                    filter_by.append(filter_clauses)
            else:
                filter_clauses = self._create_filter_clause_json_deprecated(filter)
                filter_by.extend(filter_clauses)

        # Build distance expression as usual
        distance_expr = self.distance_strategy(embedding).label("distance")
        self.logger.debug(f"Compiled distance expr â†’ {distance_expr}")
        stmt = (
            select(
                self.EmbeddingStore,
                distance_expr
            )
            .filter(*filter_by)
            .order_by(sqlalchemy.asc(distance_expr))
            .limit(k)
        )

        if score_threshold is not None:
            stmt = stmt.filter(distance_expr < score_threshold)

        results: Sequence[Any] = (await session.execute(stmt)).all()
        return results

    async def amax_marginal_relevance_search_with_score_by_vector(
        self,
        embedding: List[float],
        k: int = 4,
        fetch_k: int = 20,
        lambda_mult: float = 0.5,
        filter: Optional[Dict[str, str]] = None,
        **kwargs: Any,
    ) -> List[Tuple[Document, float]]:
        await self.__apost_init__()
        async with self._make_async_session() as session:
            results = await self.__aquery_collection(
                session=session, embedding=embedding, k=fetch_k, filter=filter
            )
            embedding_list = [
                getattr(result.EmbeddingStore, self._embedding_column)
                for result in results
            ]
            mmr_selected = maximal_marginal_relevance(
                np.array(embedding, dtype=np.float32),
                embedding_list,
                k=k,
                lambda_mult=lambda_mult,
            )

            candidates = self._results_to_docs_and_scores(results)
            return [r for i, r in enumerate(candidates) if i in mmr_selected]

    @property
    def distance_strategy(self) -> Any:
        """
        Return the appropriate distance function based on the configured strategy
        and embedding column name.
        """
        # Get the embedding column from the EmbeddingStore model
        embedding_column = getattr(self.EmbeddingStore, self._embedding_column)
        self.logger.debug(
            f"PgVector: using distance strategy â†’ {self._distance_strategy}"
        )
        if self._distance_strategy == DistanceStrategy.EUCLIDEAN_DISTANCE:
            return embedding_column.l2_distance
        elif self._distance_strategy == DistanceStrategy.COSINE:
            return embedding_column.cosine_distance
        elif self._distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
            return embedding_column.max_inner_product
        elif self._distance_strategy == DistanceStrategy.DOT_PRODUCT:
            return embedding_column.dot_product
        else:
            raise ValueError(
                f"Got unexpected value for distance: {self._distance_strategy}. "
                f"Should be one of {', '.join([ds.value for ds in DistanceStrategy])}."
            )

    async def __apost_init__(
        self,
    ) -> None:
        """Async initialize the store (use lazy approach)."""
        if self._async_init:  # Warning: possible race condition
            return
        self._async_init = True
        if self._schema_based:
            ebstore, cstore = await self._get_embedding_collection_store(
                table=self.table_name,
                schema=self.schema,
                dimension=self._embedding_length
            )
        else:
            ebstore, cstore = _get_embedding_collection_store(
                self._embedding_length
            )
        self.CollectionStore = cstore
        self.EmbeddingStore = ebstore

        if not self._schema_based:
            await self.acreate_tables_if_not_exists()
            await self.acreate_collection()

    async def asimilarity_search(
        self,
        query: str,
        k: int = 4,
        limit: Optional[int] = None,
        score_threshold: Optional[float] = None,
        filter: Optional[dict] = None,
        **kwargs: Any,
    ) -> List[Document]:
        """Run similarity search with PGVector with distance.

        Args:
            query (str): Query text to search for.
            k (int): Number of results to return. Defaults to 4.
            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.

        Returns:
            List of Documents most similar to the query.
        """
        k = limit if limit is not None else k
        await self.__apost_init__()  # Lazy async init
        embedding = await self.embeddings.aembed_query(query)
        docs = await self.asimilarity_search_by_vector(
            embedding=embedding,
            k=k,
            score_threshold=score_threshold or self._score_threshold,
            filter=filter,
        )
        self.logger.debug(
            f"Found {len(docs)} documents for query '{query}' with k={k}."
        )
        return docs

    async def asimilarity_search_by_vector(
        self,
        embedding: List[float],
        k: int = 4,
        score_threshold: Optional[float] = None,
        filter: Optional[dict] = None,
        **kwargs: Any,
    ) -> List[Document]:
        """Return docs most similar to embedding vector.

        Args:
            embedding: Embedding to look up documents similar to.
            k: Number of Documents to return. Defaults to 4.
            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.

        Returns:
            List of Documents most similar to the query vector.
        """
        assert self._async_engine, "This method must be called with async_mode"
        await self.__apost_init__()  # Lazy async init
        docs_and_scores = await self.asimilarity_search_with_score_by_vector(
            embedding=embedding, k=k, score_threshold=score_threshold, filter=filter
        )
        return _results_to_docs(docs_and_scores)

    async def asimilarity_search_with_score_by_vector(
        self,
        embedding: List[float],
        k: int = 4,
        score_threshold: Optional[float] = None,
        filter: Optional[dict] = None,
    ) -> List[Tuple[Document, float]]:
        await self.__apost_init__()  # Lazy async init
        score = score_threshold or self._score_threshold
        async with self._make_async_session() as session:  # type: ignore[arg-type]
            results = await self._aquery_collection(
                session=session, embedding=embedding, k=k, score_threshold=score, filter=filter
            )
            return self._results_to_docs_and_scores(results)

    async def _aquery_collection(
        self,
        session: AsyncSession,
        embedding: List[float],
        k: int = 4,
        score_threshold: Optional[float] = None,
        filter: Optional[Dict[str, str]] = None,
    ) -> List[Tuple[Document, float]]:
        """
        Enhanced search method that uses the configurable distance strategy.
        """
        async with self._make_async_session() as session:
            filter_by = []
            if filter:
                if self.use_jsonb:
                    filter_clause = self._create_filter_clause(filter)
                    if filter_clause is not None:
                        filter_by.append(filter_clause)
                else:
                    filter_clauses = self._create_filter_clause_json_deprecated(filter)
                    filter_by.extend(filter_clauses)

            # Use the distance_strategy property which now respects the configurable column
            distance_expr = self.distance_strategy(embedding).label("distance")
            self.logger.debug(f"Compiled distance expr â†’ {distance_expr}")
            stmt = (
                sqlalchemy.select(
                    self.EmbeddingStore,
                    distance_expr
                )
                .filter(*filter_by)
            )

            if score_threshold is not None:
                stmt = stmt.filter(distance_expr < score_threshold)
            else:
                stmt = stmt.order_by(distance_expr).limit(k)

            stmt = stmt.order_by(sqlalchemy.asc(distance_expr))
            results: Sequence[Any] = (await session.execute(stmt)).all()
            return results

    def _results_to_docs_and_scores(self, results: Any) -> List[Tuple[Document, float]]:
        """Return docs and scores from results using configurable id column."""
        docs = [
            (
                Document(
                    id=str(getattr(result.EmbeddingStore, self._id_column)),
                    page_content=result.EmbeddingStore.document,
                    metadata=result.EmbeddingStore.cmetadata,
                ),
                result.distance if self.embeddings is not None else None,
            )
            for result in results
        ]
        return docs

    # Helper method to change distance strategy after initialization
    def set_distance_strategy(self, strategy: DistanceStrategy) -> None:
        """
        Change the distance strategy after initialization.

        Args:
            strategy: New DistanceStrategy to use
        """
        if strategy not in [DistanceStrategy.COSINE, DistanceStrategy.DOT_PRODUCT, DistanceStrategy.EUCLIDEAN_DISTANCE, DistanceStrategy.MAX_INNER_PRODUCT]:
            raise ValueError(f"Invalid distance strategy: {strategy}")
        self._distance_strategy = strategy

    # Helper method to change embedding column after initialization
    def set_embedding_column(self, column_name: str) -> None:
        """
        Change the embedding column name after initialization.
        Note: This requires recreating the ORM models.

        Args:
            column_name: New embedding column name
        """
        self._embedding_column = column_name
        # Force recreation of ORM models on next operation
        self._async_init = False

    async def aadd_embeddings(
        self,
        texts: Sequence[str],
        embeddings: List[List[float]],
        metadatas: Optional[List[dict]] = None,
        ids: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> List[str]:
        """Async add embeddings to the vectorstore.

        Args:
            texts: Iterable of strings to add to the vectorstore.
            embeddings: List of list of embedding vectors.
            metadatas: List of metadatas associated with the texts.
            ids: Optional list of ids for the texts.
                If not provided, will generate a new id for each text.
            kwargs: vectorstore specific parameters
        """
        await self.__apost_init__()  # Lazy async init

        if not metadatas:
            metadatas = [{} for _ in texts]

        async with self._make_async_session() as session:  # type: ignore[arg-type]
            # Build data dictionary using configurable column names
            data = []
            if self._use_uuid:
                # Auto-generate case - don't include ID column
                for txt, metadata, embedding in zip(texts, metadatas, embeddings):
                    row_data = {
                        self._embedding_column: embedding,
                        'text': txt,
                        'document': txt,
                        'cmetadata': metadata or {},
                    }
                    if not self._schema_based:
                        collection = await self.aget_collection(session)
                        if not collection:
                            raise ValueError("Collection not found")
                        row_data['collection_id'] = collection.uuid
                    else:
                        row_data['collection_id'] = uuid.UUID('00000000-0000-0000-0000-000000000000')

                    data.append(row_data)
            else:
                # Standard case, generate IDs or use provided
                if ids is None:
                    ids_ = [str(uuid.uuid4()) for _ in texts]
                else:
                    ids_ = []
                    for id_val in ids:
                        if id_val is None:
                            ids_.append(str(uuid.uuid4()))
                        else:
                            ids_.append(str(id_val))
                for txt, metadata, embedding, id in zip(texts, metadatas, embeddings, ids_):
                    row_data = {
                        self._id_column: id,
                        self._embedding_column: embedding,
                        'text': txt,
                        'document': txt,
                        'cmetadata': metadata or {},
                    }
                    data.append(row_data)

                # Only add collection_id if not using schema-based approach
                if not self._schema_based:
                    collection = await self.aget_collection(session)
                    if not collection:
                        raise ValueError("Collection not found")
                    row_data['collection_id'] = collection.uuid
                else:
                    # For schema-based approach, use default collection_id
                    row_data['collection_id'] = '00000000-0000-0000-0000-000000000000'

            self.logger.notice(
                f"Writing {len(data)} rows to pgvector (should be {len(texts)})"
            )
            stmt = insert(self.EmbeddingStore).values(data)
            try:
                if not self._use_uuid:
                    # Only do upsert if you control the ID
                    # Use configurable ID column for conflict detection
                    on_conflict_stmt = stmt.on_conflict_do_update(
                        index_elements=[self._id_column],
                        set_={
                            self._embedding_column: stmt.excluded.__getattr__(self._embedding_column),
                            'document': stmt.excluded.document,
                            'text': stmt.excluded.document,
                            'cmetadata': stmt.excluded.cmetadata,
                        },
                    )
                    await session.execute(on_conflict_stmt)
                    await session.commit()
                else:
                    # No upsert, just insert (let DB handle id)
                    await session.execute(stmt)
                    await session.commit()
            except sqlalchemy.exc.IntegrityError as e:
                # Handle integrity errors (e.g., duplicate IDs)
                self.logger.error(f"Integrity error while adding embeddings: {e}")
                await session.rollback()
                raise
            except Exception as e:
                # Handle other exceptions
                self.logger.error(f"Error while adding embeddings: {e}")
                await session.rollback()
                raise

        # Convert UUID objects back to strings for return value
        if not self._use_uuid:
            return [str(id_val) for id_val in ids_]
        else:
            return []

    async def aadd_texts(
        self,
        texts: Iterable[str],
        metadatas: Optional[List[dict]] = None,
        ids: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> List[str]:
        """Run more texts through the embeddings and add to the vectorstore.

        Args:
            texts: Iterable of strings to add to the vectorstore.
            metadatas: Optional list of metadatas associated with the texts.
            ids: Optional list of ids for the texts.
                If not provided, will generate a new id for each text.
            kwargs: vectorstore specific parameters

        Returns:
            List of ids from adding the texts into the vectorstore.
        """
        await self.__apost_init__()  # Lazy async init
        texts_ = list(texts)

        # Use self.embeddings instead of self.embedding_function
        try:
            embeddings = await self.embeddings.aembed_documents(texts_)
        except Exception as e:
            self.logger.error(f"Error generating embeddings: {e}")

        return await self.aadd_embeddings(
            texts=texts_,
            embeddings=list(embeddings),
            metadatas=list(metadatas) if metadatas else None,
            ids=list(ids) if ids else None,
            **kwargs,
        )

    async def aadd_documents(
        self,
        documents: List[Document],
        ids: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> List[str]:
        """Add documents to the vectorstore.

        Args:
            documents: List of Document objects to add to the vectorstore.
            ids: Optional list of ids for the documents.
                If not provided, will generate a new id for each document.
            kwargs: vectorstore specific parameters

        Returns:
            List of ids from adding the documents into the vectorstore.
        """
        await self.__apost_init__()  # Lazy async init

        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]

        # Use document IDs if available, otherwise use provided ids
        doc_ids = []
        for i, doc in enumerate(documents):
            if hasattr(doc, 'id') and doc.id:
                doc_ids.append(doc.id)
            elif ids and i < len(ids) and ids[i]:
                doc_ids.append(ids[i])
            else:
                doc_ids.append(str(uuid.uuid4()))

        return await self.aadd_texts(
            texts=texts,
            metadatas=metadatas,
            ids=doc_ids,
            **kwargs,
        )

class PgvectorStore(AbstractStore):
    """Pgvector Store Class.

    Using PostgreSQL + PgVector to saving vectors in database.
    """
    def __init__(
        self,
        table: str = None,
        schema: str = 'public',
        id_column: str = 'id',
        embedding_column: str = 'embedding',
        distance_strategy: str = 'COSINE',
        embedding_model: Union[dict, str] = None,
        embedding: Union[dict, Callable] = None,
        **kwargs
    ):
        super().__init__(
            embedding_model=embedding_model,
            embedding=embedding,
            **kwargs
        )
        self.table: str = table
        self.schema: str = schema
        if self.schema is not None:
            self.collection_name = f"{self.schema}.{self.collection_name}"
        if self.table and not self.collection_name:
            self.collection_name = f"{self.schema}.{self.table}"
        self.database: str = kwargs.get('database', None)
        self._id_column = id_column
        self._embedding_column: str = embedding_column
        self.dimension: int = kwargs.get('dimension', 384)
        self.dsn = kwargs.get('dsn', default_sqlalchemy_pg)
        self._drop: bool = kwargs.pop('drop', False)
        self._connection: AsyncEngine = None
        # Convert string to DistanceStrategy enum
        strategy_mapping = {
            'COSINE': DistanceStrategy.COSINE,
            'L2': DistanceStrategy.EUCLIDEAN_DISTANCE,
            'EUCLIDEAN': DistanceStrategy.EUCLIDEAN_DISTANCE,
            'IP': DistanceStrategy.MAX_INNER_PRODUCT,
            'MAX_INNER_PRODUCT': DistanceStrategy.MAX_INNER_PRODUCT,
            'DOT_PRODUCT': DistanceStrategy.DOT_PRODUCT,
        }
        # By Default use COSINE distance strategy
        self.distance_strategy = strategy_mapping.get(
            distance_strategy.upper(),
            DistanceStrategy.COSINE
        )
        self.vector: PgVector = None  # Store the vector instance
        self._context_depth = 0  # Track context depth

    def set_distance_strategy(self, strategy: DistanceStrategy) -> None:
        """
        Change the distance strategy after initialization.

        Args:
            strategy: New DistanceStrategy to use
        """
        if strategy not in [DistanceStrategy.COSINE, DistanceStrategy.DOT_PRODUCT, DistanceStrategy.EUCLIDEAN_DISTANCE, DistanceStrategy.MAX_INNER_PRODUCT]:
            raise ValueError(f"Invalid distance strategy: {strategy}")
        self.distance_strategy = strategy
        if self.vector is not None:
            self.vector.set_distance_strategy(strategy)

    async def execute_sql(
        self,
        sql: Union[str, sqlalchemy.text],
        params: Optional[dict] = None,
        fetch: bool = False
    ) -> Any:
        """
        Execute raw SQL using the existing connection.

        Args:
            sql: SQL query string or sqlalchemy.text object
            params: Optional parameters for the query
            fetch: Whether to fetch results (for SELECT queries)

        Returns:
            Query result if fetch=True, otherwise CursorResult
        """
        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )

        if isinstance(sql, str):
            sql = sqlalchemy.text(sql)

        async with self._connection.begin() as conn:
            result = await conn.execute(sql, params or {})
            if fetch:
                if hasattr(result, 'fetchall'):
                    return result.fetchall()
                elif hasattr(result, 'scalar'):
                    return result.scalar()
            return result

    async def collection_exists(self, collection: str = None) -> bool:
        """Check if a collection exists in the database."""
        if not collection:
            collection = self.collection_name

        # Split schema and table if provided as "schema.table"
        if "." in collection:
            schema, table = collection.split(".", 1)
        else:
            # Default to 'public' schema if not provided
            schema = "public"
            table = collection
        async with self._connection.connect() as conn:
            # âœ… Check if the collection (table) exists
            check_query = """
            SELECT EXISTS (
                SELECT 1
                FROM information_schema.tables
                WHERE table_schema = :schema
                AND table_name = :table
            );
            """
            result = await conn.execute(
                sqlalchemy.text(check_query),
                {"schema": schema, "table": table}
            )
            exists = result.scalar_one()
            return bool(exists)

    async def table_exists(self, table: str, schema: str) -> bool:
        """Check if a collection exists in the database."""
        async with self._connection.connect() as conn:
            # âœ… Check if the collection (table) exists
            check_query = """
            SELECT EXISTS (
                SELECT 1
                FROM information_schema.tables
                WHERE table_schema = :schema
                AND table_name = :table
            );
            """
            result = await conn.execute(
                sqlalchemy.text(check_query),
                {"schema": schema, "table": table}
            )
            exists = result.scalar()
            return bool(exists)

    async def is_connection_alive(self) -> bool:
        """
        Check if the PostgreSQL connection is still alive and functional.

        Returns:
            bool: True if connection is alive, False otherwise
        """
        if not self._connection:
            return False

        if not self._connected:
            return False

        try:
            # Test the connection with a simple query
            async with self._connection.connect() as conn:
                result = await conn.execute(sqlalchemy.text("SELECT 1"))
                result.scalar()
                return True
        except Exception as e:
            self.logger.warning(f"Connection health check failed: {e}")
            return False

    async def ensure_connection(self, force_reconnect: bool = False) -> bool:
        """
        Ensure that the connection is alive and reconnect if necessary.

        Args:
            force_reconnect: If True, force a reconnection even if connection appears alive

        Returns:
            bool: True if connection is ready, False if reconnection failed
        """
        if force_reconnect:
            self.logger.info(
                "Forcing reconnection to PostgreSQL..."
            )
            await self.disconnect()
            self._connected = False
            self._connection = None

        # Check if we need to establish a connection
        if self._connected and not await self.is_connection_alive():
            self.logger.info("Connection is not alive, attempting to reconnect...")
            self._connected = False
            self._connection = None
        try:
            await self.connection()
            self.logger.info("Successfully reconnected to PostgreSQL")
            return True
        except Exception as e:
            self.logger.error(f"Failed to reconnect to PostgreSQL: {e}")
            return False

    async def reconnect(self) -> bool:
        """
        Force a reconnection to PostgreSQL.

        Returns:
            bool: True if reconnection successful, False otherwise
        """
        return await self.ensure_connection(force_reconnect=True)

    async def get_connection_info(self) -> dict:
        """
        Get information about the current connection status.

        Returns:
            dict: Connection information including status, pool info, etc.
        """
        info = {
            "connected": self._connected,
            "has_engine": self._connection is not None,
            "dsn": self.dsn,
            "schema": self.schema,
            "table": self.table,
            "alive": False,
            "pool_info": None
        }

        if self._connection:
            try:
                # Check if connection is alive
                info["alive"] = await self.is_connection_alive()

                # Get pool information
                pool = self._connection.pool
                if pool:
                    info["pool_info"] = {
                        "size": pool.size(),
                        "checked_in": pool.checkedin(),
                        "checked_out": pool.checkedout(),
                        "overflow": pool.overflow(),
                        "invalid": pool.invalid()
                    }
            except Exception as e:
                info["error"] = str(e)

        return info

    async def connection(self, alias: str = None):
        """Connection to PgVector Store.

        Args:
            alias (str): Database alias.

        Returns:
            Callable: PgVector connection.

        """
        if not self.dsn:
            self.dsn = default_sqlalchemy_pg
        self._connection = create_async_engine(
            self.dsn,
            future=True,
            echo=False,
            # echo_pool='debug',
        )
        async with self._connection.begin() as conn:
            if getattr(self, "_drop", False):
                self.vector = PgVector(
                    embeddings=self._embed_.embedding,
                    table_name=self.table,
                    schema=self.schema,
                    collection_name=self.collection_name,
                    embedding_length=self.dimension,
                    connection=self._connection,
                    id_column=self._id_column,
                    embedding_column=self._embedding_column,
                    distance_strategy=self.distance_strategy,
                    async_mode=True,
                    use_jsonb=True,
                    create_extension=False
                )
                # await self.vector.adrop_tables()
            # if not await self.collection_exists(self.collection_name):
            #     await self.create_collection(self.collection_name)
        self._connected = True
        return self._connection

    def engine(self):
        return self._connection

    async def disconnect(self) -> None:
        """
        Closing the Connection on DuckDB
        """
        try:
            if self._connection:
                await self._connection.dispose()
        except Exception as err:
            raise RuntimeError(
                message=f"{__name__!s}: Closing Error: {err!s}"
            ) from err
        finally:
            self._connection = None
            self._connected = False
        print("âœ… Connection closed successfully.")

    # Enhanced context manager methods
    async def __aenter__(self):
        """Enhanced async context manager entry with connection validation."""
        # Ensure connection is alive before proceeding
        if not await self.ensure_connection():
            raise RuntimeError(
                "Failed to establish connection to PostgreSQL"
            )

        self._context_depth += 1
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Enhanced async context manager exit."""
        self._context_depth -= 1

        # Only disconnect if we're exiting the outermost context
        if self._context_depth <= 0:
            await self.disconnect()
            self._context_depth = 0

    async def create_collection(self, collection: str) -> None:
        """Create a new collection in the database."""
        async with self._connection.connect() as conn:
            # âœ… Create the collection in PgVector
            _embed_ = self._embed_ or self.create_embedding(
                embedding_model=self.embedding_model
            )
            self._client = PgVector(
                embeddings=_embed_.embedding,
                embedding_length=self.dimension,
                collection_name=self.collection_name,
                connection=self._connection,
                use_jsonb=True,
                create_extension=False
            )

    def get_vector(
        self,
        table: Optional[str] = None,
        schema: Optional[str] = None,
        collection: Union[str, None] = None,
        embedding: Optional[Callable] = None,
        metric_type: Optional[str] = None,
        embedding_column: Optional[str] = None,
        score_threshold: Optional[float] = None,
        use_uuid: bool = False,
        **kwargs
    ) -> PGVector:
        """
        This function retrieves a vector from the specified collection using the provided embedding.
        If no collection is specified, it uses the default collection name.
        If no embedding is provided, it creates a new embedding using the specified embedding model.

        Parameters:
        - collection (Union[str, None]): The name of the collection from which to retrieve the vector.
        - embedding (Optional[Callable]): The embedding function to use for vector retrieval.
        - kwargs: Additional keyword arguments to pass to the PGVector constructor.

        Returns:
        - PGVector: The retrieved vector from the specified collection.
        """
        if not table:
            table = self.table
        if not schema:
            schema = self.schema
        if not collection:
            collection = self.collection_name
        if self.vector is not None:
            return self.vector
        if embedding is not None:
            _embed_ = embedding
        else:
            _embed_ = self.create_embedding(
                embedding_model=self.embedding_model
            )
        if not metric_type:
            metric_type = self.distance_strategy
        if not embedding_column:
            embedding_column = self._embedding_column
        self.vector = PgVector(
            connection=self._connection,
            table_name=table,
            schema=schema,
            id_column=self._id_column,
            embedding_column=embedding_column,
            collection_name=collection,
            embedding_length=self.dimension,
            distance_strategy=metric_type,
            score_threshold=score_threshold,
            embeddings=_embed_.embedding,
            logger=self.logger,
            async_mode=True,
            use_jsonb=True,
            create_extension=False,
            use_uuid=use_uuid,
            **kwargs
        )
        return self.vector

    def memory_retriever(
        self,
        documents: Optional[List[Document]] = None,
        num_results: int = 5
    ) -> VectorStoreRetrieverMemory:
        _embed_ = self._embed_ or self.create_embedding(
            embedding_model=self.embedding_model
        )
        vectordb = PgVector.from_documents(
            documents or [],
            embedding=_embed_.embedding,
            connection=self._connection,
            collection_name=self.collection_name,
            embedding_length=self.dimension,
            use_jsonb=True,
            async_mode=True,
            create_extension=False,
        )
        retriever = PgVector.as_retriever(
            vectordb,
            search_kwargs=dict(k=num_results)
        )
        return VectorStoreRetrieverMemory(retriever=retriever)

    async def from_documents(
        self,
        documents: List[Document],
        table: Optional[str] = None,
        schema: Optional[str] = 'public',
        embedding_column: Optional[str] = None,
        collection: Union[str, None] = None,
        **kwargs
    ) -> None:
        """Save Documents as Vectors in VectorStore."""
        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )
        _embed_ = self._embed_ or self.create_embedding(
            embedding_model=self.embedding_model
        )
        if not collection:
            collection = self.collection_name
        vectordb = await PgVector.afrom_documents(
            documents,
            connection=self._connection,
            table_name=table,
            schema=schema,
            id_column=self._id_column,
            embedding_column=embedding_column or self._embedding_column,
            collection_name=collection,
            embedding=_embed_.embedding,
            embedding_length=self.dimension,
            distance_strategy=self.distance_strategy,
            use_jsonb=True,
            async_mode=True,
        )
        return vectordb

    async def add_documents(
        self,
        documents: List[Document],
        collection: Union[str, None] = None,
        use_uuid: bool = False,
        **kwargs
    ) -> None:
        """Save Documents as Vectors in VectorStore."""
        if not collection:
            collection = self.collection_name
        if not self._connected or not await self.ensure_connection():
            raise RuntimeError("Cannot establish connection to PostgreSQL")
        vectordb = self.get_vector(
            collection=collection,
            use_uuid=use_uuid,
            **kwargs
        )
        # Asynchronously add documents to PGVector
        if use_uuid:
            # Let database auto-generate UUIDs
            await vectordb.aadd_documents(documents, ids=None)
        else:
            doc_ids = []
            for doc in documents:
                if hasattr(doc, 'id') and doc.id:
                    doc_ids.append(doc.id)
                else:
                    doc_ids.append(None)  # Will be auto-generated
            await vectordb.aadd_documents(documents, ids=doc_ids)

    async def similarity_search(
        self,
        query: str,
        table: Optional[str] = None,
        schema: Optional[str] = None,
        collection: Union[str, None] = None,
        limit: int = 2,
        score_threshold: Optional[float] = None,
        search_strategy: str = "auto",
        filter: Optional[dict] = None,
        **kwargs
    ) -> List[Document]:
        """
        Search for similar documents in VectorStore.

        Args:
            search_strategy:
                - "auto": Automatically choose best strategy
                - "exact": Exact ID/code matching
                - "semantic": Semantic similarity search
                - "location": Location-focused search
        """
        if not table:
            table = self.table
        if not schema:
            schema = self.schema
        if collection is None:
            collection = self.collection_name

        # print(f"ðŸ” Starting similarity_search with query: '{query}'")
        # print(f"ðŸ“Š Context depth: {self._context_depth}")
        # print(f"ðŸ”Œ Connected: {self._connected}")
        # print(f"ðŸ—„ï¸ Connection: {self._connection}")
        # print(f"ðŸ” Smart search: '{query}' using strategy: {search_strategy}")
        # Just ensure connection exists, don't create nested context
        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )

        vector_db = self.get_vector(table=table, schema=schema, collection=collection, **kwargs)
        return await vector_db.asimilarity_search(
            query,
            k=limit,
            score_threshold=score_threshold,
            filter=filter
        )

    async def similarity_search_with_score(
        self,
        query: str,
        table: Optional[str] = None,
        schema: Optional[str] = None,
        collection: Union[str, None] = None,
        limit: int = 2,
    ) -> List[Document]:
        """Search for similar documents in VectorStore with scores."""
        if not table:
            table = self.table
        if not schema:
            schema = self.schema
        if collection is None:
            collection = self.collection_name

        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )

        vector_db = self.get_vector(table=table, schema=schema, collection=collection)
        return await vector_db.asimilarity_search_with_score(query, k=limit)

    async def prepare_embedding_table(
        self,
        tablename: str,
        conn: AsyncEngine = None,
        embedding_column: str = 'embedding',
        document_column: str = 'document',
        metadata_column: str = 'cmetadata',
        dimension: int = None,
        id_column: str = 'id',
        use_jsonb: bool = True,
        drop_columns: bool = False,
        create_all_indexes: bool = True,
        **kwargs
    ):
        """
        Prepare a Postgres Table as an embedding table in PostgreSQL with advanced features.
        This method prepares a table with the following columns:
        - id: unique identifier (String)
        - embedding: the vector column (Vector(dimension) or JSONB)
        - document: text column containing the document
        - collection_id: UUID column for collection identification.
        - metadata: JSONB column for metadata
        - Additional columns based on the provided `columns` list
        - Enhanced indexing strategies for efficient querying
        - Support for multiple distance strategies (COSINE, L2, IP, etc.)
        Args:
        - tablename (str): Name of the table to create.
        - embedding_column (str): Name of the column for storing embeddings.
        - document_column (str): Name of the column for storing document text.
        - metadata_column (str): Name of the column for storing metadata.
        - dimension (int): Dimension of the embedding vector.
        - id_column (str): Name of the column for storing unique identifiers.
        - use_jsonb (bool): Whether to use JSONB for metadata storage.
        - drop_columns (bool): Whether to drop existing columns.
        - create_all_indexes (bool): Whether to create all distance strategies.
    """
        # Drop existing columns if requested
        if drop_columns:
            for column in (document_column, embedding_column, metadata_column):
                await conn.execute(
                    sqlalchemy.text(
                        f'ALTER TABLE {tablename} DROP COLUMN IF EXISTS {column};'
                    )
                )
        # Create metadata column as a jsonb field
        if use_jsonb:
            await conn.execute(
                sqlalchemy.text(
                    f'ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS {metadata_column} JSONB;'
                )
            )
        # Use pgvector type
        await conn.execute(
            sqlalchemy.text(
                f'ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS {embedding_column} vector({dimension});'
            )
        )
        # Create additional columns
        for col_name, col_type in [
            (document_column, 'TEXT'),
            (id_column, 'varchar'),
        ]:
            await conn.execute(
                sqlalchemy.text(
                    f'ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS {col_name} {col_type};'
                )
            )
        # Create the Collection Column:
        await conn.execute(
            sqlalchemy.text(
                f"ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS collection_id UUID;"
            )
        )
        await conn.execute(
            sqlalchemy.text(
                f"UPDATE {tablename} SET collection_id = '00000000-0000-0000-0000-000000000000';"
            )
        )
        await conn.execute(
            sqlalchemy.text(
                f"ALTER TABLE {tablename} ALTER COLUMN collection_id SET NOT NULL;"
            )
        )
        # âœ… CREATE COMPREHENSIVE INDEXES
        if create_all_indexes:
            print("ðŸ”§ Creating indexes for all distance strategies...")

            # COSINE index (most common for text embeddings)
            await conn.execute(
                sqlalchemy.text(
                    f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_cosine "
                    f"ON {tablename} USING ivfflat ({embedding_column} vector_cosine_ops);"
                )
            )
            print("âœ… Created COSINE index")
            # L2/Euclidean index
            await conn.execute(
                sqlalchemy.text(
                    f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_l2 "
                    f"ON {tablename} USING ivfflat ({embedding_column} vector_l2_ops);"
                )
            )
            print("âœ… Created L2 index")

            # Inner Product index
            try:
                await conn.execute(
                    sqlalchemy.text(
                        f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_ip "
                        f"ON {tablename} USING ivfflat ({embedding_column} vector_ip_ops);"
                    )
                )
                print("âœ… Created Inner Product index")
            except Exception as e:
                print(f"âš ï¸ Inner Product index creation failed: {e}")

            # HNSW indexes for better performance (requires more memory)
            try:
                await conn.execute(
                    sqlalchemy.text(
                        f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_hnsw_cosine "
                        f"ON {tablename} USING hnsw ({embedding_column} vector_cosine_ops);"
                    )
                )
                print("âœ… Created HNSW COSINE index")
                await conn.execute(
                    sqlalchemy.text(
                        f"""CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_hnsw_l2
                            ON {tablename} USING hnsw ({embedding_column} vector_l2_ops) WITH (
                            m = 16,  -- graph connectivity (higher â†’ better recall, more memory)
                            ef_construction = 200 -- controls indexing time vs. recall
                        );"""
                    )
                )
                print("âœ… Created HNSW EUCLIDEAN index")
            except Exception as e:
                print(f"âš ï¸ HNSW index creation failed (this is optional): {e}")

        else:
            # Create index only for current strategy
            distance_strategy_ops = {
                DistanceStrategy.COSINE: "vector_cosine_ops",
                DistanceStrategy.EUCLIDEAN_DISTANCE: "vector_l2_ops",
                DistanceStrategy.MAX_INNER_PRODUCT: "vector_ip_ops",
                DistanceStrategy.DOT_PRODUCT: "vector_ip_ops"
            }

            ops = distance_strategy_ops.get(self.distance_strategy, "vector_cosine_ops")
            strategy_name = str(self.distance_strategy).rsplit('.', maxsplit=1)[-1].lower()

            await conn.execute(
                sqlalchemy.text(
                    f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_{strategy_name} "
                    f"ON {tablename} USING ivfflat ({embedding_column} {ops});"
                )
            )
            print(f"âœ… Created {strategy_name.upper()} index")

        # Create JSONB indexes for better performance
        await self._create_jsonb_indexes(
            conn,
            tablename,
            metadata_column,
            id_column
        )
        return True

    async def create_embedding_table(
        self,
        table: str,
        columns: List[str],
        schema: str = 'public',
        embedding_column: str = 'embedding',
        document_column: str = 'document',
        metadata_column: str = 'cmetadata',
        dimension: int = None,
        id_column: str = 'id',
        use_jsonb: bool = True,
        drop_columns: bool = True,
        create_all_indexes: bool = True,
        **kwargs
    ):
        """
        Create an embedding table in PostgreSQL with advanced features.
        This method creates a table with the following columns:
        - id: unique identifier (String)
        - embedding: the vector column (Vector(dimension) or JSONB)
        - document: text column containing the document
        - cmetadata: JSONB column for metadata
        - Additional columns based on the provided `columns` list
        - Enhanced indexing strategies for efficient querying
        - Support for multiple distance strategies (COSINE, L2, IP, etc.)
        Args:
        - table (str): Name of the table to create.
        - columns (List[str]): List of column names to include in the table.
        - schema (str): Database schema where the table will be created.
        - embedding_column (str): Name of the column for storing embeddings.
        - document_column (str): Name of the column for storing document text.
        - metadata_column (str): Name of the column for storing metadata.
        - dimension (int): Dimension of the embedding vector.
        - id_column (str): Name of the column for storing unique identifiers.
        - use_jsonb (bool): Whether to use JSONB for metadata storage.
        - drop_columns (bool): Whether to drop existing columns.
        - create_all_indexes (bool): Whether to create all distance strategies.

        Enhanced embedding table creation with JSONB strategy for better semantic search.

        This approach creates multiple document representations:
        1. Primary search content (emphasizing store ID)
        2. Location-based content
        3. Structured metadata for filtering
        4. Multiple embedding variations
        """
        tablename = f'{schema}.{table}'
        cols = ', '.join(columns)
        _qry = f'SELECT {cols} FROM {tablename};'
        dimension = dimension or self.dimension

        # Generate a sample embedding to determine its dimension
        sample_vector = self._embed_.embedding.embed_query("sample text")
        vector_dim = len(sample_vector)
        self.logger.notice(
            f"USING EMBED {self._embed_} with dimension {vector_dim}"
        )

        if vector_dim != dimension:
            raise ValueError(
                f"Expected embedding dimension {dimension}, but got {vector_dim}"
            )

        async with self._connection.begin() as conn:
            result = await conn.execute(sqlalchemy.text(_qry))
            rows = result.fetchall()

            await self.prepare_embedding_table(
                tablename=tablename,
                embedding_column=embedding_column,
                document_column=document_column,
                metadata_column=metadata_column,
                dimension=dimension,
                id_column=id_column,
                use_jsonb=use_jsonb,
                drop_columns=drop_columns,
                create_all_indexes=create_all_indexes,
                **kwargs
            )

            # Populate the embedding data
            for i, row in enumerate(rows):
                _id = getattr(row, id_column)
                metadata = {col: getattr(row, col) for col in columns}
                data = await self._create_metadata_structure(metadata, id_column, _id)

                # Generate embedding
                searchable_text = data['structured_search']
                print(f"ðŸ” Row {i + 1}/{len(rows)} - {_id}")
                print(f"   Text: {searchable_text[:100]}...")

                vector = self._embed_.embedding.embed_query(searchable_text)
                vector_str = "[" + ",".join(str(v) for v in vector) + "]"

                await conn.execute(
                    sqlalchemy.text(f"""
                        UPDATE {tablename}
                        SET {embedding_column} = :embeddings,
                            {document_column} = :document,
                            {metadata_column} = :metadata
                        WHERE {id_column} = :id
                    """),
                    {
                        "embeddings": vector_str,
                        "document": searchable_text,
                        "metadata": json_encoder(data),
                        "id": _id
                    }
                )

        print("âœ… Updated Table embeddings with comprehensive indexes.")

    def _create_natural_searchable_text(
        self,
        metadata: dict,
        id_column: str,
        record_id: str
    ) -> str:
        """
        Create well-structured, natural language text with proper separation.

        This creates clean, readable text that embedding models can understand better.
        """
        # Start with the ID in multiple formats for exact matching
        text_parts = [
            f"ID: {record_id}",
            f"Identifier: {record_id}",
            id_column + ": " + record_id
        ]

        # Process each field to create natural language descriptions
        for key, value in metadata.items():
            if value is None or value == '':
                continue
            clean_value = value.strip() if isinstance(value, str) else str(value)
            text_parts.append(f"{key}: {clean_value}")
            # Add the field in natural language format
            clean_key = key.replace('_', ' ').title()
            text_parts.append(f"{clean_key}={clean_value}")

        # Join with spaces and clean up
        searchable_text = ', '.join(text_parts) + '.'

        return searchable_text

    def _create_structured_search_text(self, metadata: dict, id_column: str, record_id: str) -> str:
        """
        Create a more structured but still readable search text.

        This emphasizes key-value relationships while staying readable.
        """
        # ID section with emphasis
        kv_sections = [
            f"ID: {record_id}",
            f"Identifier: {record_id}",
            id_column + ": " + record_id
        ]

        # Key-value sections with clean separation
        for key, value in metadata.items():
            if value is None or value == '':
                continue

            # Clean key-value representation
            clean_key = key.replace('_', ' ').title()
            kv_sections.append(f"{clean_key}: {value}")
            kv_sections.append(f"{key}: {value}")

        # Combine with proper separation
        return ' | '.join(kv_sections)

    async def _create_metadata_structure(
        self,
        metadata: dict,
        id_column: str,
        _id: str
    ):
        """Create a structured metadata representation for the document."""
        # Create a structured metadata representation
        enhanced_metadata = {
            "id": _id,
            id_column: _id,
            "_variants": [
                _id,
                _id.lower(),
                _id.upper()
            ]
        }
        for key, value in metadata.items():
            enhanced_metadata[key] = value
            # Create searchable variants for key fields
            if value and isinstance(value, str):
                variants = [value, value.lower(), value.upper()]
                # Add variants without special characters
                clean_value = ''.join(c for c in str(value) if c.isalnum() or c.isspace())
                if clean_value != value:
                    variants.append(clean_value)
                enhanced_metadata[f"_{key}_variants"] = list(set(variants))
        # create a full-text search field of searchable content
        enhanced_metadata['searchable_content'] = self._create_natural_searchable_text(
            metadata, id_column, _id
        )

        # Also create a structured search text that emphasizes important fields
        enhanced_metadata['structured_search'] = self._create_structured_search_text(
            metadata, id_column, _id
        )

        return enhanced_metadata

    async def _create_jsonb_indexes(
        self,
        conn,
        tablename: str,
        metadata_col: str,
        id_column: str
    ):
        """Create optimized JSONB indexes for better search performance."""

        print("ðŸ”§ Creating JSONB indexes on Metadata for optimized search...")

        # Index for ID searches
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_{id_column}
                ON {tablename} USING BTREE (({metadata_col}->>'{id_column}'));
            """)
        )
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_id
                ON {tablename} USING BTREE (({metadata_col}->>'id'));
            """)
        )

        # GIN index for full-text search on searchable content
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_fulltext
                ON {tablename} USING GIN (to_tsvector('english', {metadata_col}->>'searchable_content'));
            """)
        )

        # GIN index for JSONB structure searches
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_metadata_gin
                ON {tablename} USING GIN ({metadata_col});
            """)
        )

        print("âœ… Created optimized JSONB indexes")

    async def delete_documents(
        self,
        documents: Optional[List[Document]] = None,
        pk: str = 'source_type',
        values: Optional[Union[str, List[str]]] = None,
        table: Optional[str] = None,
        schema: Optional[str] = None,
        collection: Optional[str] = None,
        **kwargs
    ) -> int:
        """
        Delete documents from the vector store based on metadata field values.

        Args:
            documents: List of documents whose metadata values will be used for deletion.
                    If provided, the pk field values will be extracted from these documents.
            pk: The metadata field name to use for deletion (default: 'source_type')
            values: Specific values to delete. Can be a single string or list of strings.
                If provided, this takes precedence over extracting from documents.
            table: Override table name
            schema: Override schema name
            collection: Override collection name

        Returns:
            int: Number of documents deleted

        Examples:
            # Delete all documents with source_type 'papers'
            deleted_count = await store.delete_documents(values='papers')

            # Delete documents with multiple source types
            deleted_count = await store.delete_documents(values=['papers', 'reports'])

            # Delete based on documents' metadata
            docs_to_delete = [Document(page_content="test", metadata={"source_type": "papers"})]
            deleted_count = await store.delete_documents(documents=docs_to_delete)

            # Delete by different metadata field
            deleted_count = await store.delete_documents(pk='category', values='obsolete')
        """
        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )

        # Determine table details
        if not table:
            table = self.table
        if not schema:
            schema = self.schema
        if not collection:
            collection = self.collection_name

        # Extract values to delete
        delete_values = []

        if values is not None:
            # Use provided values
            if isinstance(values, str):
                delete_values = [values]
            else:
                delete_values = list(values)
        elif documents:
            # Extract values from documents metadata
            for doc in documents:
                if hasattr(doc, 'metadata') and doc.metadata and pk in doc.metadata:
                    value = doc.metadata[pk]
                    if value and value not in delete_values:
                        delete_values.append(value)
        else:
            raise ValueError("Either 'documents' or 'values' parameter must be provided")

        if not delete_values:
            self.logger.warning(f"No values found for field '{pk}' to delete")
            return 0

        # Construct table name with schema
        if schema and schema != 'public':
            full_table_name = f"{schema}.{table}"
        else:
            full_table_name = table

        deleted_count = 0

        try:
            async with self._connection.begin() as conn:
                for value in delete_values:
                    # Use JSONB operator to find matching metadata
                    delete_query = sqlalchemy.text(f"""
                        DELETE FROM {full_table_name}
                        WHERE cmetadata->>:pk = :value
                    """)

                    result = await conn.execute(
                        delete_query,
                        {"pk": pk, "value": str(value)}
                    )

                    rows_deleted = result.rowcount
                    deleted_count += rows_deleted

                    self.logger.info(
                        f"Deleted {rows_deleted} documents with {pk}='{value}' from {full_table_name}"
                    )

            self.logger.info(f"Total deleted: {deleted_count} documents")
            return deleted_count

        except Exception as e:
            self.logger.error(f"Error deleting documents: {e}")
            raise RuntimeError(f"Failed to delete documents: {e}") from e

    async def delete_documents_by_filter(
        self,
        filter_dict: Dict[str, Union[str, List[str]]],
        table: Optional[str] = None,
        schema: Optional[str] = None,
        collection: Optional[str] = None,
        **kwargs
    ) -> int:
        """
        Delete documents based on multiple metadata field conditions.

        Args:
            filter_dict: Dictionary of field_name: value(s) pairs for deletion criteria
            table: Override table name
            schema: Override schema name
            collection: Override collection name

        Returns:
            int: Number of documents deleted

        Example:
            # Delete documents with source_type='papers' AND category='research'
            deleted = await store.delete_documents_by_filter({
                'source_type': 'papers',
                'category': 'research'
            })

            # Delete documents with source_type in ['papers', 'reports']
            deleted = await store.delete_documents_by_filter({
                'source_type': ['papers', 'reports']
            })
        """
        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )

        if not filter_dict:
            raise ValueError("filter_dict cannot be empty")

        # Determine table details
        if not table:
            table = self.table
        if not schema:
            schema = self.schema
        if not collection:
            collection = self.collection_name

        # Construct table name with schema
        if schema and schema != 'public':
            full_table_name = f"{schema}.{table}"
        else:
            full_table_name = table

        # Build WHERE conditions
        where_conditions = []
        params = {}

        for field, values in filter_dict.items():
            if isinstance(values, (list, tuple)):
                # Handle multiple values with IN operator
                placeholders = []
                for i, value in enumerate(values):
                    param_name = f"{field}_{i}"
                    placeholders.append(f":{param_name}")
                    params[param_name] = str(value)

                condition = f"cmetadata->>'{field}' IN ({', '.join(placeholders)})"
                where_conditions.append(condition)
            else:
                # Handle single value
                param_name = f"{field}_single"
                where_conditions.append(f"cmetadata->>'{field}' = :{param_name}")
                params[param_name] = str(values)

        # Combine conditions with AND
        where_clause = " AND ".join(where_conditions)

        delete_query = sqlalchemy.text(f"""
            DELETE FROM {full_table_name}
            WHERE {where_clause}
        """)

        try:
            async with self._connection.begin() as conn:
                result = await conn.execute(delete_query, params)
                deleted_count = result.rowcount

                self.logger.info(
                    f"Deleted {deleted_count} documents from {full_table_name} "
                    f"with filter: {filter_dict}"
                )

                return deleted_count

        except Exception as e:
            self.logger.error(f"Error deleting documents by filter: {e}")
            raise RuntimeError(f"Failed to delete documents by filter: {e}") from e

    async def delete_all_documents(
        self,
        table: Optional[str] = None,
        schema: Optional[str] = None,
        collection: Optional[str] = None,
        confirm: bool = False,
        **kwargs
    ) -> int:
        """
        Delete ALL documents from the vector store table.

        WARNING: This will delete all data in the table!

        Args:
            table: Override table name
            schema: Override schema name
            collection: Override collection name
            confirm: Must be set to True to proceed with deletion

        Returns:
            int: Number of documents deleted
        """
        if not confirm:
            raise ValueError(
                "This operation will delete ALL documents. "
                "Set confirm=True to proceed."
            )

        if not self._connected or not self._connection:
            raise RuntimeError(
                "Store is not connected. Use 'async with store:' context manager."
            )

        # Determine table details
        if not table:
            table = self.table
        if not schema:
            schema = self.schema
        if not collection:
            collection = self.collection_name

        # Construct table name with schema
        if schema and schema != 'public':
            full_table_name = f"{schema}.{table}"
        else:
            full_table_name = table

        try:
            async with self._connection.begin() as conn:
                # First count existing documents
                count_query = sqlalchemy.text(f"SELECT COUNT(*) FROM {full_table_name}")
                count_result = await conn.execute(count_query)
                total_docs = count_result.scalar()

                if total_docs == 0:
                    self.logger.info(f"No documents to delete from {full_table_name}")
                    return 0

                # Delete all documents
                delete_query = sqlalchemy.text(f"DELETE FROM {full_table_name}")
                result = await conn.execute(delete_query)
                deleted_count = result.rowcount

                self.logger.warning(
                    f"DELETED ALL {deleted_count} documents from {full_table_name}"
                )

                return deleted_count

        except Exception as e:
            self.logger.error(f"Error deleting all documents: {e}")
            raise RuntimeError(f"Failed to delete all documents: {e}") from e

    # Utility method for connection monitoring
    async def monitor_connection(self, interval: int = 30, max_attempts: int = 3) -> None:
        """
        Monitor connection health and auto-reconnect if needed.

        Args:
            interval: Check interval in seconds
            max_attempts: Maximum reconnection attempts before giving up
        """
        reconnect_attempts = 0

        while True:
            try:
                if not await self.is_connection_alive():
                    self.logger.warning(
                        f"Connection lost, attempting reconnect (attempt {reconnect_attempts + 1}/{max_attempts})"
                    )

                    if await self.ensure_connection(force_reconnect=True):
                        self.logger.info("Connection restored successfully")
                        reconnect_attempts = 0
                    else:
                        reconnect_attempts += 1
                        if reconnect_attempts >= max_attempts:
                            self.logger.error(f"Failed to reconnect after {max_attempts} attempts")
                            break

                await asyncio.sleep(interval)

            except Exception as e:
                self.logger.error(f"Connection monitoring error: {e}")
                await asyncio.sleep(interval)
