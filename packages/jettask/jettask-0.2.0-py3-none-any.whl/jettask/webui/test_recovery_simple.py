#!/usr/bin/env python
"""ç®€åŒ–çš„æ¢å¤æœºåˆ¶æµ‹è¯•è„šæœ¬"""

import asyncio
import json
import logging
import os
import signal
import sys
import time
from typing import Dict, List, Optional
from datetime import datetime, timezone

import redis.asyncio as redis
from redis.asyncio import Redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text

from jettask.webui.config import PostgreSQLConfig, RedisConfig
from jettask.core.consumer_manager import ConsumerManager, ConsumerStrategy
from jettask.utils.serializer import dumps_str

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class RecoveryTester:
    """æ¢å¤æœºåˆ¶æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.pg_config = PostgreSQLConfig(
            host=os.getenv('JETTASK_PG_HOST', 'localhost'),
            port=int(os.getenv('JETTASK_PG_PORT', '5432')),
            database=os.getenv('JETTASK_PG_DB', 'jettask'),
            user=os.getenv('JETTASK_PG_USER', 'jettask'),
            password=os.getenv('JETTASK_PG_PASSWORD', '123456'),
        )
        
        self.redis_config = RedisConfig(
            host=os.getenv('REDIS_HOST', 'localhost'),
            port=int(os.getenv('REDIS_PORT', '6379')),
            db=int(os.getenv('REDIS_DB', '0')),
            password=os.getenv('REDIS_PASSWORD'),
        )
        
        self.prefix = "jettask"
        self.redis_client: Optional[Redis] = None
        self.async_engine = None
        self.AsyncSessionLocal = None
        
    async def setup(self):
        """åˆå§‹åŒ–è¿æ¥"""
        self.redis_client = await redis.Redis(
            host=self.redis_config.host,
            port=self.redis_config.port,
            db=self.redis_config.db,
            password=self.redis_config.password,
            decode_responses=False
        )
        
        if self.pg_config.dsn.startswith('postgresql://'):
            dsn = self.pg_config.dsn.replace('postgresql://', 'postgresql+psycopg://', 1)
        else:
            dsn = self.pg_config.dsn
            
        self.async_engine = create_async_engine(dsn, pool_size=20, echo=False)
        
        self.AsyncSessionLocal = sessionmaker(
            self.async_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
    async def cleanup(self):
        """æ¸…ç†è¿æ¥"""
        if self.redis_client:
            await self.redis_client.aclose()
        if self.async_engine:
            await self.async_engine.dispose()
    
    async def test_queue_recovery(self):
        """æµ‹è¯•é˜Ÿåˆ—æ¶ˆæ¯çš„æ¢å¤æœºåˆ¶"""
        logger.info("=" * 60)
        logger.info("æµ‹è¯• _consume_queues çš„æ¢å¤æœºåˆ¶")
        logger.info("=" * 60)
        
        queue_name = 'RECOVERY_TEST_QUEUE'
        stream_key = f"{self.prefix}:QUEUE:{queue_name}"
        consumer_group = f"{self.prefix}_pg_consumer1"
        
        # 1. æ¸…ç†æ—§æ•°æ®
        logger.info("\n1. æ¸…ç†æ—§æ•°æ®...")
        try:
            await self.redis_client.delete(stream_key)
        except:
            pass
        
        # 2. å‘é€æµ‹è¯•æ¶ˆæ¯
        logger.info("\n2. å‘é€æµ‹è¯•æ¶ˆæ¯åˆ°é˜Ÿåˆ—...")
        task_ids = []
        for i in range(5):
            task_data = {
                'name': f'recovery_test_task_{i}',
                'queue': queue_name,
                'priority': 1,
                'trigger_time': time.time(),
                'test_id': f'queue_recovery_{i}'
            }
            
            msg_id = await self.redis_client.xadd(
                stream_key,
                {'data': dumps_str(task_data)}
            )
            task_ids.append(msg_id)
            logger.info(f"  - å‘é€æ¶ˆæ¯ {i}: {msg_id}")
        
        # 3. åˆ›å»ºæ¶ˆè´¹è€…ç»„
        logger.info("\n3. åˆ›å»ºæ¶ˆè´¹è€…ç»„...")
        try:
            await self.redis_client.xgroup_create(
                stream_key, consumer_group, id='0', mkstream=True
            )
            logger.info(f"  - åˆ›å»ºæ¶ˆè´¹è€…ç»„: {consumer_group}")
        except:
            pass
        
        # 4. æ¨¡æ‹Ÿconsumer_1è¯»å–æ¶ˆæ¯ä½†ä¸ACKï¼ˆçªç„¶æŒ‚æ‰ï¼‰
        logger.info("\n4. æ¨¡æ‹Ÿ consumer_1 è¯»å–æ¶ˆæ¯ä½†ä¸ACK...")
        consumer_1 = "test_consumer_1"
        
        messages = await self.redis_client.xreadgroup(
            consumer_group,
            consumer_1,
            {stream_key: '>'},
            count=5,
            block=1000
        )
        
        if messages:
            logger.info(f"  - consumer_1 è¯»å–äº† {len(messages[0][1])} æ¡æ¶ˆæ¯")
            for msg_id, data in messages[0][1]:
                logger.info(f"    - æ¶ˆæ¯ID: {msg_id}")
        
        # 5. æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n5. æ£€æŸ¥pendingæ¶ˆæ¯...")
        pending_info = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"  - æ€»pendingæ¶ˆæ¯æ•°: {pending_info['pending']}")
        
        if pending_info['pending'] > 0:
            detailed = await self.redis_client.xpending_range(
                stream_key, consumer_group,
                min='-', max='+', count=10
            )
            for msg in detailed:
                logger.info(f"    - æ¶ˆæ¯ {msg['message_id']}: consumer={msg['consumer']}, times_delivered={msg['times_delivered']}")
        
        # 6. æ¨¡æ‹Ÿconsumer_2æ¥ç®¡pendingæ¶ˆæ¯
        logger.info("\n6. æ¨¡æ‹Ÿ consumer_2 å°è¯•æ¥ç®¡pendingæ¶ˆæ¯...")
        consumer_2 = "test_consumer_2"
        
        # ä½¿ç”¨XCLAIMæ¥ç®¡æ¶ˆæ¯
        if pending_info['pending'] > 0:
            # è·å–æ‰€æœ‰pendingæ¶ˆæ¯çš„ID
            pending_msg_ids = [msg['message_id'] for msg in detailed]
            
            # XCLAIMæ¥ç®¡æ¶ˆæ¯ï¼ˆidleæ—¶é—´è®¾ä¸º0è¡¨ç¤ºç«‹å³æ¥ç®¡ï¼‰
            claimed = await self.redis_client.xclaim(
                stream_key,
                consumer_group,
                consumer_2,
                min_idle_time=0,  # ç«‹å³æ¥ç®¡
                message_ids=pending_msg_ids
            )
            
            logger.info(f"  - consumer_2 æ¥ç®¡äº† {len(claimed)} æ¡æ¶ˆæ¯")
            
            # ACKæ¶ˆæ¯
            if claimed:
                msg_ids_to_ack = [msg[0] for msg in claimed]
                await self.redis_client.xack(stream_key, consumer_group, *msg_ids_to_ack)
                logger.info(f"  - consumer_2 ACKäº† {len(msg_ids_to_ack)} æ¡æ¶ˆæ¯")
        
        # 7. å†æ¬¡æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n7. å†æ¬¡æ£€æŸ¥pendingæ¶ˆæ¯...")
        pending_info_after = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"  - æ€»pendingæ¶ˆæ¯æ•°: {pending_info_after['pending']}")
        
        # 8. éªŒè¯ç»“æœ
        logger.info("\n" + "=" * 60)
        if pending_info['pending'] > 0 and pending_info_after['pending'] == 0:
            logger.info("âœ“ é˜Ÿåˆ—æ¶ˆæ¯æ¢å¤æµ‹è¯•é€šè¿‡ï¼pendingæ¶ˆæ¯æˆåŠŸè¢«æ¥ç®¡å’Œå¤„ç†")
        else:
            logger.warning(f"âœ— é˜Ÿåˆ—æ¶ˆæ¯æ¢å¤æµ‹è¯•å¤±è´¥ã€‚æ¢å¤å‰: {pending_info['pending']}ï¼Œæ¢å¤å: {pending_info_after['pending']}")
        
        return pending_info['pending'] > 0 and pending_info_after['pending'] == 0
    
    async def test_task_changes_recovery(self):
        """æµ‹è¯•TASK_CHANGESçš„æ¢å¤æœºåˆ¶"""
        logger.info("\n" + "=" * 60)
        logger.info("æµ‹è¯• _consume_task_changes çš„æ¢å¤æœºåˆ¶")
        logger.info("=" * 60)
        
        change_stream_key = f"{self.prefix}:TASK_CHANGES"
        consumer_group = f"{self.prefix}_changes_consumer"
        
        # 1. æ¸…ç†æ—§æ•°æ®
        logger.info("\n1. æ¸…ç†æ—§æ•°æ®...")
        try:
            await self.redis_client.delete(change_stream_key)
        except:
            pass
        
        # 2. å‘é€ä»»åŠ¡å˜æ›´äº‹ä»¶
        logger.info("\n2. å‘é€ä»»åŠ¡å˜æ›´äº‹ä»¶...")
        event_ids = []
        for i in range(5):
            event_data = {
                'event_id': f'task_change_test_{i}',
                'event_type': 'task_updated',
                'timestamp': str(time.time())
            }
            
            msg_id = await self.redis_client.xadd(
                change_stream_key,
                event_data
            )
            event_ids.append(msg_id)
            logger.info(f"  - å‘é€äº‹ä»¶ {i}: {msg_id}")
            
            # åŒæ—¶åˆ›å»ºå¯¹åº”çš„ä»»åŠ¡æ•°æ®
            task_key = f"{self.prefix}:TASK:task_change_test_{i}"
            task_data = {
                'status': 'completed',
                'completed_at': str(time.time()),
                'result': json.dumps({'success': True})
            }
            await self.redis_client.hset(task_key, mapping={
                k: dumps_str(v) if not isinstance(v, (str, bytes)) else v
                for k, v in task_data.items()
            })
        
        # 3. åˆ›å»ºæ¶ˆè´¹è€…ç»„
        logger.info("\n3. åˆ›å»ºæ¶ˆè´¹è€…ç»„...")
        try:
            await self.redis_client.xgroup_create(
                change_stream_key, consumer_group, id='0', mkstream=True
            )
            logger.info(f"  - åˆ›å»ºæ¶ˆè´¹è€…ç»„: {consumer_group}")
        except:
            pass
        
        # 4. æ¨¡æ‹Ÿconsumer_1è¯»å–æ¶ˆæ¯ä½†ä¸ACK
        logger.info("\n4. æ¨¡æ‹Ÿ consumer_1 è¯»å–æ¶ˆæ¯ä½†ä¸ACK...")
        consumer_1 = "changes_consumer_1"
        
        messages = await self.redis_client.xreadgroup(
            consumer_group,
            consumer_1,
            {change_stream_key: '>'},
            count=5,
            block=1000
        )
        
        if messages:
            logger.info(f"  - consumer_1 è¯»å–äº† {len(messages[0][1])} æ¡äº‹ä»¶")
            for msg_id, data in messages[0][1]:
                logger.info(f"    - äº‹ä»¶ID: {msg_id}")
        
        # 5. æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n5. æ£€æŸ¥pendingäº‹ä»¶...")
        pending_info = await self.redis_client.xpending(change_stream_key, consumer_group)
        logger.info(f"  - æ€»pendingäº‹ä»¶æ•°: {pending_info['pending']}")
        
        if pending_info['pending'] > 0:
            detailed = await self.redis_client.xpending_range(
                change_stream_key, consumer_group,
                min='-', max='+', count=10
            )
            for msg in detailed:
                logger.info(f"    - äº‹ä»¶ {msg['message_id']}: consumer={msg['consumer']}, times_delivered={msg['times_delivered']}")
        
        # 6. æ¨¡æ‹Ÿconsumer_2æ¥ç®¡pendingæ¶ˆæ¯
        logger.info("\n6. æ¨¡æ‹Ÿ consumer_2 æ¥ç®¡pendingäº‹ä»¶...")
        consumer_2 = "changes_consumer_2"
        
        if pending_info['pending'] > 0:
            pending_msg_ids = [msg['message_id'] for msg in detailed]
            
            claimed = await self.redis_client.xclaim(
                change_stream_key,
                consumer_group,
                consumer_2,
                min_idle_time=0,
                message_ids=pending_msg_ids
            )
            
            logger.info(f"  - consumer_2 æ¥ç®¡äº† {len(claimed)} æ¡äº‹ä»¶")
            
            # ACKæ¶ˆæ¯
            if claimed:
                msg_ids_to_ack = [msg[0] for msg in claimed]
                await self.redis_client.xack(change_stream_key, consumer_group, *msg_ids_to_ack)
                logger.info(f"  - consumer_2 ACKäº† {len(msg_ids_to_ack)} æ¡äº‹ä»¶")
        
        # 7. å†æ¬¡æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n7. å†æ¬¡æ£€æŸ¥pendingäº‹ä»¶...")
        pending_info_after = await self.redis_client.xpending(change_stream_key, consumer_group)
        logger.info(f"  - æ€»pendingäº‹ä»¶æ•°: {pending_info_after['pending']}")
        
        # 8. éªŒè¯ç»“æœ
        logger.info("\n" + "=" * 60)
        if pending_info['pending'] > 0 and pending_info_after['pending'] == 0:
            logger.info("âœ“ TASK_CHANGESæ¢å¤æµ‹è¯•é€šè¿‡ï¼pendingäº‹ä»¶æˆåŠŸè¢«æ¥ç®¡å’Œå¤„ç†")
        else:
            logger.warning(f"âœ— TASK_CHANGESæ¢å¤æµ‹è¯•å¤±è´¥ã€‚æ¢å¤å‰: {pending_info['pending']}ï¼Œæ¢å¤å: {pending_info_after['pending']}")
        
        return pending_info['pending'] > 0 and pending_info_after['pending'] == 0
    
    async def test_offline_worker_recovery(self):
        """æµ‹è¯•OfflineWorkerRecoveryçš„åŠŸèƒ½"""
        logger.info("\n" + "=" * 60)
        logger.info("æµ‹è¯• OfflineWorkerRecovery çš„åŠŸèƒ½")
        logger.info("=" * 60)
        
        # å¯¼å…¥OfflineWorkerRecovery
        from jettask.core.offline_worker_recovery import OfflineWorkerRecovery
        
        # åˆ›å»ºåŒæ­¥Rediså®¢æˆ·ç«¯ï¼ˆConsumerManageréœ€è¦ï¼‰
        import redis as sync_redis
        sync_redis_client = sync_redis.StrictRedis(
            host=self.redis_config.host,
            port=self.redis_config.port,
            db=self.redis_config.db,
            password=self.redis_config.password,
            decode_responses=True
        )
        
        # åˆ›å»ºConsumerManager
        consumer_manager = ConsumerManager(
            redis_client=sync_redis_client,
            strategy=ConsumerStrategy.HEARTBEAT,
            config={
                'redis_prefix': self.prefix,
                'queues': ['OFFLINE_TEST_QUEUE'],
                'worker_prefix': 'TEST_WORKER'
            }
        )
        
        # åˆ›å»ºOfflineWorkerRecovery
        recovery = OfflineWorkerRecovery(
            async_redis_client=self.redis_client,
            redis_prefix=self.prefix,
            worker_prefix='TEST_WORKER',
            consumer_manager=consumer_manager
        )
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        queue_name = 'OFFLINE_TEST_QUEUE'
        stream_key = f"{self.prefix}:QUEUE:{queue_name}"
        consumer_group = f"{self.prefix}_pg_consumer1"
        
        # æ¸…ç†æ—§æ•°æ®
        logger.info("\n1. æ¸…ç†æ—§æ•°æ®...")
        try:
            await self.redis_client.delete(stream_key)
        except:
            pass
        
        # å‘é€æµ‹è¯•æ¶ˆæ¯
        logger.info("\n2. å‘é€æµ‹è¯•æ¶ˆæ¯...")
        for i in range(3):
            task_data = {
                'name': f'offline_test_{i}',
                'queue': queue_name,
                'test_id': f'offline_{i}'
            }
            await self.redis_client.xadd(
                stream_key,
                {'data': dumps_str(task_data)}
            )
        
        # åˆ›å»ºæ¶ˆè´¹è€…ç»„
        try:
            await self.redis_client.xgroup_create(
                stream_key, consumer_group, id='0', mkstream=True
            )
        except:
            pass
        
        # æ¨¡æ‹Ÿç¦»çº¿workerè¯»å–æ¶ˆæ¯
        logger.info("\n3. æ¨¡æ‹Ÿç¦»çº¿workerè¯»å–æ¶ˆæ¯...")
        offline_worker = "offline_worker_1"
        messages = await self.redis_client.xreadgroup(
            consumer_group,
            offline_worker,
            {stream_key: '>'},
            count=3,
            block=1000
        )
        
        if messages:
            logger.info(f"  - ç¦»çº¿workerè¯»å–äº† {len(messages[0][1])} æ¡æ¶ˆæ¯")
        
        # æ¨¡æ‹Ÿworkerç¦»çº¿ï¼ˆåˆ é™¤å…¶æ³¨å†Œä¿¡æ¯ï¼‰
        worker_key = f"{self.prefix}:TEST_WORKER:{offline_worker}"
        await self.redis_client.delete(worker_key)
        logger.info(f"  - æ¨¡æ‹Ÿworkerç¦»çº¿: åˆ é™¤ {worker_key}")
        
        # å®šä¹‰å¤„ç†å›è°ƒ
        processed_messages = []
        async def process_callback(msg_id, msg_data, queue, consumer_id):
            processed_messages.append({
                'msg_id': msg_id,
                'queue': queue,
                'consumer_id': consumer_id
            })
            logger.info(f"  - å¤„ç†æ¢å¤çš„æ¶ˆæ¯: {msg_id} from {consumer_id}")
        
        # æ‰§è¡Œæ¢å¤
        logger.info("\n4. æ‰§è¡Œç¦»çº¿workeræ¢å¤...")
        current_consumer = "active_worker_1"
        recovered_count = await recovery.recover_offline_workers(
            queue=queue_name,
            current_consumer_name=current_consumer,
            process_message_callback=process_callback
        )
        
        logger.info(f"  - æ¢å¤äº† {recovered_count} æ¡æ¶ˆæ¯")
        
        # æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n5. æ£€æŸ¥æ¢å¤åçš„pendingæ¶ˆæ¯...")
        pending_info = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"  - å‰©ä½™pendingæ¶ˆæ¯æ•°: {pending_info['pending']}")
        
        # éªŒè¯ç»“æœ
        logger.info("\n" + "=" * 60)
        if recovered_count > 0 and len(processed_messages) == recovered_count:
            logger.info("âœ“ OfflineWorkerRecoveryæµ‹è¯•é€šè¿‡ï¼æˆåŠŸæ¢å¤ç¦»çº¿workerçš„æ¶ˆæ¯")
        else:
            logger.warning(f"âœ— OfflineWorkerRecoveryæµ‹è¯•å¤±è´¥ã€‚æ¢å¤æ•°é‡: {recovered_count}ï¼Œå¤„ç†æ•°é‡: {len(processed_messages)}")
        
        # æ¸…ç†
        consumer_manager.cleanup()
        sync_redis_client.close()
        
        return recovered_count > 0


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    from dotenv import load_dotenv
    load_dotenv()
    
    tester = RecoveryTester()
    await tester.setup()
    
    try:
        # æµ‹è¯•1ï¼šé˜Ÿåˆ—æ¶ˆæ¯æ¢å¤
        queue_test = await tester.test_queue_recovery()
        await asyncio.sleep(1)
        
        # æµ‹è¯•2ï¼šTASK_CHANGESæ¢å¤
        changes_test = await tester.test_task_changes_recovery()
        await asyncio.sleep(1)
        
        # æµ‹è¯•3ï¼šOfflineWorkerRecovery
        offline_test = await tester.test_offline_worker_recovery()
        
        # æ€»ç»“
        logger.info("\n" + "=" * 60)
        logger.info("æµ‹è¯•æ€»ç»“")
        logger.info("=" * 60)
        logger.info(f"é˜Ÿåˆ—æ¶ˆæ¯æ¢å¤æµ‹è¯•: {'âœ“ é€šè¿‡' if queue_test else 'âœ— å¤±è´¥'}")
        logger.info(f"TASK_CHANGESæ¢å¤æµ‹è¯•: {'âœ“ é€šè¿‡' if changes_test else 'âœ— å¤±è´¥'}")
        logger.info(f"OfflineWorkerRecoveryæµ‹è¯•: {'âœ“ é€šè¿‡' if offline_test else 'âœ— å¤±è´¥'}")
        
        if queue_test and changes_test and offline_test:
            logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¢å¤æœºåˆ¶å·¥ä½œæ­£å¸¸")
        else:
            logger.warning("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥æ¢å¤æœºåˆ¶")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å‡ºé”™: {e}", exc_info=True)
    finally:
        await tester.cleanup()


if __name__ == '__main__':
    asyncio.run(main())