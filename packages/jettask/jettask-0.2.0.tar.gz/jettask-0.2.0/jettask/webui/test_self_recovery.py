#!/usr/bin/env python
"""æµ‹è¯•å•ä¸ªworkeré‡å¯åè‡ªåŠ¨æ¢å¤pendingæ¶ˆæ¯çš„æœºåˆ¶"""

import asyncio
import json
import logging
import os
import sys
import time
from typing import Dict, List, Optional
from datetime import datetime, timezone

import redis.asyncio as redis
from redis.asyncio import Redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text

from jettask.webui.config import PostgreSQLConfig, RedisConfig
from jettask.utils.serializer import dumps_str

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class SelfRecoveryTester:
    """æµ‹è¯•å•ä¸ªworkerè‡ªæˆ‘æ¢å¤æœºåˆ¶"""
    
    def __init__(self):
        self.pg_config = PostgreSQLConfig(
            host=os.getenv('JETTASK_PG_HOST', 'localhost'),
            port=int(os.getenv('JETTASK_PG_PORT', '5432')),
            database=os.getenv('JETTASK_PG_DB', 'jettask'),
            user=os.getenv('JETTASK_PG_USER', 'jettask'),
            password=os.getenv('JETTASK_PG_PASSWORD', '123456'),
        )
        
        self.redis_config = RedisConfig(
            host=os.getenv('REDIS_HOST', 'localhost'),
            port=int(os.getenv('REDIS_PORT', '6379')),
            db=int(os.getenv('REDIS_DB', '0')),
            password=os.getenv('REDIS_PASSWORD'),
        )
        
        self.prefix = "jettask"
        self.redis_client: Optional[Redis] = None
        
    async def setup(self):
        """åˆå§‹åŒ–è¿æ¥"""
        self.redis_client = await redis.Redis(
            host=self.redis_config.host,
            port=self.redis_config.port,
            db=self.redis_config.db,
            password=self.redis_config.password,
            decode_responses=False
        )
        
    async def cleanup(self):
        """æ¸…ç†è¿æ¥"""
        if self.redis_client:
            await self.redis_client.aclose()
    
    async def test_queue_self_recovery(self):
        """æµ‹è¯•é˜Ÿåˆ—æ¶ˆæ¯çš„è‡ªæˆ‘æ¢å¤æœºåˆ¶"""
        logger.info("=" * 60)
        logger.info("æµ‹è¯•å•ä¸ªworkeré‡å¯åè‡ªåŠ¨æ¢å¤è‡ªå·±çš„pendingæ¶ˆæ¯")
        logger.info("=" * 60)
        
        queue_name = 'SELF_RECOVERY_QUEUE'
        stream_key = f"{self.prefix}:QUEUE:{queue_name}"
        consumer_group = f"{self.prefix}_pg_consumer1"
        consumer_name = "self_recovery_worker"
        
        # 1. æ¸…ç†æ—§æ•°æ®
        logger.info("\n1. æ¸…ç†æ—§æ•°æ®...")
        try:
            await self.redis_client.delete(stream_key)
        except:
            pass
        
        # 2. å‘é€æµ‹è¯•æ¶ˆæ¯
        logger.info("\n2. å‘é€æµ‹è¯•æ¶ˆæ¯åˆ°é˜Ÿåˆ—...")
        message_ids = []
        for i in range(5):
            task_data = {
                'name': f'self_recovery_task_{i}',
                'queue': queue_name,
                'priority': 1,
                'trigger_time': time.time(),
                'test_id': f'self_recovery_{i}'
            }
            
            msg_id = await self.redis_client.xadd(
                stream_key,
                {'data': dumps_str(task_data)}
            )
            message_ids.append(msg_id)
            logger.info(f"  - å‘é€æ¶ˆæ¯ {i}: {msg_id}")
        
        # 3. åˆ›å»ºæ¶ˆè´¹è€…ç»„
        logger.info("\n3. åˆ›å»ºæ¶ˆè´¹è€…ç»„...")
        try:
            await self.redis_client.xgroup_create(
                stream_key, consumer_group, id='0', mkstream=True
            )
            logger.info(f"  - åˆ›å»ºæ¶ˆè´¹è€…ç»„: {consumer_group}")
        except:
            pass
        
        # 4. æ¨¡æ‹Ÿworkerç¬¬ä¸€æ¬¡è¯»å–æ¶ˆæ¯ï¼ˆè¯»å–æ–°æ¶ˆæ¯ï¼‰
        logger.info(f"\n4. æ¨¡æ‹Ÿ {consumer_name} ç¬¬ä¸€æ¬¡è¯»å–æ–°æ¶ˆæ¯ä½†ä¸ACKï¼ˆæ¨¡æ‹Ÿçªç„¶æŒ‚æ‰ï¼‰...")
        
        messages = await self.redis_client.xreadgroup(
            consumer_group,
            consumer_name,
            {stream_key: '>'},  # '>' è¡¨ç¤ºè¯»å–æ–°æ¶ˆæ¯
            count=5,
            block=1000
        )
        
        first_read_ids = []
        if messages:
            logger.info(f"  - {consumer_name} è¯»å–äº† {len(messages[0][1])} æ¡æ–°æ¶ˆæ¯")
            for msg_id, data in messages[0][1]:
                first_read_ids.append(msg_id)
                logger.info(f"    - æ¶ˆæ¯ID: {msg_id}")
        
        # 5. æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n5. æ£€æŸ¥pendingæ¶ˆæ¯çŠ¶æ€...")
        pending_info = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"  - æ€»pendingæ¶ˆæ¯æ•°: {pending_info['pending']}")
        
        if pending_info['pending'] > 0:
            detailed = await self.redis_client.xpending_range(
                stream_key, consumer_group,
                min='-', max='+', count=10
            )
            for msg in detailed:
                logger.info(f"    - æ¶ˆæ¯ {msg['message_id']}: consumer={msg['consumer']}, times_delivered={msg['times_delivered']}")
        
        # 6. æ¨¡æ‹ŸåŒä¸€ä¸ªworkeré‡å¯åè¯»å–pendingæ¶ˆæ¯ï¼ˆä¸ä½¿ç”¨XCLAIMï¼‰
        logger.info(f"\n6. æ¨¡æ‹Ÿ {consumer_name} é‡å¯åè¯»å–è‡ªå·±çš„pendingæ¶ˆæ¯...")
        logger.info("  - ä½¿ç”¨ '0' ä½œä¸ºèµ·å§‹IDï¼Œè¿™ä¼šè®©consumerè¯»å–è‡ªå·±çš„pendingæ¶ˆæ¯")
        
        # å…ˆè¯»å–pendingæ¶ˆæ¯ï¼ˆä½¿ç”¨0ä½œä¸ºèµ·å§‹IDï¼‰
        pending_messages = await self.redis_client.xreadgroup(
            consumer_group,
            consumer_name,  # åŒä¸€ä¸ªconsumer name
            {stream_key: '0'},  # '0' æˆ– '0-0' è¡¨ç¤ºä»å¤´è¯»å–pendingæ¶ˆæ¯
            count=10,
            block=0  # ä¸é˜»å¡
        )
        
        recovered_ids = []
        if pending_messages:
            logger.info(f"  - {consumer_name} æˆåŠŸè¯»å–åˆ° {len(pending_messages[0][1])} æ¡è‡ªå·±çš„pendingæ¶ˆæ¯")
            for msg_id, data in pending_messages[0][1]:
                recovered_ids.append(msg_id)
                logger.info(f"    - æ¢å¤æ¶ˆæ¯ID: {msg_id}")
                
            # ACKè¿™äº›æ¶ˆæ¯
            if recovered_ids:
                await self.redis_client.xack(stream_key, consumer_group, *recovered_ids)
                logger.info(f"  - {consumer_name} ACKäº† {len(recovered_ids)} æ¡æ¢å¤çš„æ¶ˆæ¯")
        else:
            logger.info(f"  - {consumer_name} æ²¡æœ‰è¯»å–åˆ°pendingæ¶ˆæ¯")
        
        # 7. å†æ¬¡æ£€æŸ¥pendingæ¶ˆæ¯
        logger.info("\n7. å†æ¬¡æ£€æŸ¥pendingæ¶ˆæ¯çŠ¶æ€...")
        pending_info_after = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"  - æ€»pendingæ¶ˆæ¯æ•°: {pending_info_after['pending']}")
        
        # 8. éªŒè¯ç»“æœ
        logger.info("\n" + "=" * 60)
        if pending_info['pending'] > 0 and pending_info_after['pending'] == 0:
            logger.info("âœ“ å•ä¸ªworkerè‡ªæˆ‘æ¢å¤æµ‹è¯•é€šè¿‡ï¼")
            logger.info("  workeré‡å¯åæˆåŠŸè¯»å–å¹¶å¤„ç†äº†è‡ªå·±çš„pendingæ¶ˆæ¯ï¼Œæ— éœ€XCLAIM")
            return True
        else:
            logger.warning(f"âœ— å•ä¸ªworkerè‡ªæˆ‘æ¢å¤æµ‹è¯•å¤±è´¥ã€‚")
            logger.warning(f"  æ¢å¤å‰pending: {pending_info['pending']}ï¼Œæ¢å¤åpending: {pending_info_after['pending']}")
            return False
    
    async def test_task_changes_self_recovery(self):
        """æµ‹è¯•TASK_CHANGESçš„è‡ªæˆ‘æ¢å¤æœºåˆ¶"""
        logger.info("\n" + "=" * 60)
        logger.info("æµ‹è¯•TASK_CHANGESå•ä¸ªworkerè‡ªæˆ‘æ¢å¤æœºåˆ¶")
        logger.info("=" * 60)
        
        change_stream_key = f"{self.prefix}:TASK_CHANGES"
        consumer_group = f"{self.prefix}_changes_consumer"
        consumer_name = "changes_self_recovery_worker"
        
        # 1. æ¸…ç†æ—§æ•°æ®
        logger.info("\n1. æ¸…ç†æ—§æ•°æ®...")
        try:
            await self.redis_client.delete(change_stream_key)
        except:
            pass
        
        # 2. å‘é€ä»»åŠ¡å˜æ›´äº‹ä»¶
        logger.info("\n2. å‘é€ä»»åŠ¡å˜æ›´äº‹ä»¶...")
        event_ids = []
        for i in range(5):
            event_data = {
                'event_id': f'self_recovery_change_{i}',
                'event_type': 'task_updated',
                'timestamp': str(time.time())
            }
            
            msg_id = await self.redis_client.xadd(
                change_stream_key,
                event_data
            )
            event_ids.append(msg_id)
            logger.info(f"  - å‘é€äº‹ä»¶ {i}: {msg_id}")
        
        # 3. åˆ›å»ºæ¶ˆè´¹è€…ç»„
        logger.info("\n3. åˆ›å»ºæ¶ˆè´¹è€…ç»„...")
        try:
            await self.redis_client.xgroup_create(
                change_stream_key, consumer_group, id='0', mkstream=True
            )
            logger.info(f"  - åˆ›å»ºæ¶ˆè´¹è€…ç»„: {consumer_group}")
        except:
            pass
        
        # 4. æ¨¡æ‹Ÿworkerç¬¬ä¸€æ¬¡è¯»å–æ¶ˆæ¯ä½†ä¸ACK
        logger.info(f"\n4. æ¨¡æ‹Ÿ {consumer_name} ç¬¬ä¸€æ¬¡è¯»å–äº‹ä»¶ä½†ä¸ACK...")
        
        messages = await self.redis_client.xreadgroup(
            consumer_group,
            consumer_name,
            {change_stream_key: '>'},
            count=5,
            block=1000
        )
        
        if messages:
            logger.info(f"  - {consumer_name} è¯»å–äº† {len(messages[0][1])} æ¡äº‹ä»¶")
        
        # 5. æ£€æŸ¥pendingäº‹ä»¶
        logger.info("\n5. æ£€æŸ¥pendingäº‹ä»¶...")
        pending_info = await self.redis_client.xpending(change_stream_key, consumer_group)
        logger.info(f"  - æ€»pendingäº‹ä»¶æ•°: {pending_info['pending']}")
        
        # 6. æ¨¡æ‹ŸåŒä¸€ä¸ªworkeré‡å¯åæ¢å¤
        logger.info(f"\n6. æ¨¡æ‹Ÿ {consumer_name} é‡å¯åè¯»å–è‡ªå·±çš„pendingäº‹ä»¶...")
        
        # è¯»å–pendingæ¶ˆæ¯
        pending_messages = await self.redis_client.xreadgroup(
            consumer_group,
            consumer_name,
            {change_stream_key: '0'},  # ä»å¤´è¯»å–pending
            count=10,
            block=0
        )
        
        recovered_ids = []
        if pending_messages:
            logger.info(f"  - {consumer_name} æˆåŠŸæ¢å¤ {len(pending_messages[0][1])} æ¡pendingäº‹ä»¶")
            for msg_id, data in pending_messages[0][1]:
                recovered_ids.append(msg_id)
                
            # ACKæ¶ˆæ¯
            if recovered_ids:
                await self.redis_client.xack(change_stream_key, consumer_group, *recovered_ids)
                logger.info(f"  - {consumer_name} ACKäº† {len(recovered_ids)} æ¡æ¢å¤çš„äº‹ä»¶")
        
        # 7. å†æ¬¡æ£€æŸ¥pendingäº‹ä»¶
        logger.info("\n7. å†æ¬¡æ£€æŸ¥pendingäº‹ä»¶...")
        pending_info_after = await self.redis_client.xpending(change_stream_key, consumer_group)
        logger.info(f"  - æ€»pendingäº‹ä»¶æ•°: {pending_info_after['pending']}")
        
        # 8. éªŒè¯ç»“æœ
        logger.info("\n" + "=" * 60)
        if pending_info['pending'] > 0 and pending_info_after['pending'] == 0:
            logger.info("âœ“ TASK_CHANGESè‡ªæˆ‘æ¢å¤æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            logger.warning(f"âœ— TASK_CHANGESè‡ªæˆ‘æ¢å¤æµ‹è¯•å¤±è´¥")
            return False
    
    async def demonstrate_pg_consumer_recovery_flow(self):
        """æ¼”ç¤ºpg_consumerå®é™…çš„æ¢å¤æµç¨‹"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤ºpg_consumerçš„å®é™…æ¢å¤æµç¨‹")
        logger.info("=" * 60)
        
        queue_name = 'PG_CONSUMER_DEMO'
        stream_key = f"{self.prefix}:QUEUE:{queue_name}"
        consumer_group = f"{self.prefix}_pg_consumer1"
        consumer_name = "pg_consumer_worker_1"
        
        # æ¸…ç†
        try:
            await self.redis_client.delete(stream_key)
        except:
            pass
        
        # å‘é€æ¶ˆæ¯
        logger.info("\n1. å‘é€5æ¡æ¶ˆæ¯...")
        for i in range(5):
            await self.redis_client.xadd(
                stream_key,
                {'data': dumps_str({'name': f'demo_task_{i}'})}
            )
        
        # åˆ›å»ºæ¶ˆè´¹è€…ç»„
        try:
            await self.redis_client.xgroup_create(
                stream_key, consumer_group, id='0', mkstream=True
            )
        except:
            pass
        
        # æ¨¡æ‹Ÿpg_consumerçš„æ¶ˆè´¹é€»è¾‘
        logger.info(f"\n2. {consumer_name} å¼€å§‹æ¶ˆè´¹...")
        
        check_backlog = True
        lastid = "0-0"
        processed_count = 0
        
        while True:
            # è¿™æ˜¯pg_consumerçš„å®é™…é€»è¾‘
            myid = lastid if check_backlog else ">"
            
            logger.info(f"  - è¯»å–æ¶ˆæ¯ï¼Œmyid={myid}, check_backlog={check_backlog}")
            
            messages = await self.redis_client.xreadgroup(
                consumer_group,
                consumer_name,
                {stream_key: myid},
                count=2,  # æ‰¹é‡è¯»å–
                block=0 if check_backlog else 1000
            )
            
            if not messages or len(messages[0][1]) == 0:
                if check_backlog:
                    logger.info("  - æ²¡æœ‰æ›´å¤šbacklogæ¶ˆæ¯ï¼Œåˆ‡æ¢åˆ°è¯»å–æ–°æ¶ˆæ¯æ¨¡å¼")
                    check_backlog = False
                    continue
                else:
                    logger.info("  - æ²¡æœ‰æ–°æ¶ˆæ¯")
                    break
            
            # å¤„ç†æ¶ˆæ¯
            msg_count = len(messages[0][1])
            logger.info(f"  - è¯»å–åˆ° {msg_count} æ¡æ¶ˆæ¯")
            
            # æ¨¡æ‹Ÿå¤„ç†å¤±è´¥ï¼ˆç¬¬ä¸€æ¬¡åªå¤„ç†2æ¡å°±"æŒ‚æ‰"ï¼‰
            if processed_count < 2:
                processed_count += msg_count
                if processed_count >= 2:
                    logger.info("  - æ¨¡æ‹ŸworkeræŒ‚æ‰ï¼Œä¸ACKå‰©ä½™æ¶ˆæ¯")
                    break
            
            # æ­£å¸¸ACK
            msg_ids = [msg[0] for msg in messages[0][1]]
            await self.redis_client.xack(stream_key, consumer_group, *msg_ids)
            logger.info(f"  - ACKäº† {len(msg_ids)} æ¡æ¶ˆæ¯")
            
            # æ›´æ–°lastid
            lastid = messages[0][1][-1][0].decode('utf-8') if isinstance(messages[0][1][-1][0], bytes) else messages[0][1][-1][0]
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šbacklog
            check_backlog = msg_count >= 2
        
        # æ£€æŸ¥pending
        pending_info = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"\n3. WorkeræŒ‚æ‰åï¼Œpendingæ¶ˆæ¯æ•°: {pending_info['pending']}")
        
        # æ¨¡æ‹Ÿé‡å¯åæ¢å¤
        logger.info(f"\n4. {consumer_name} é‡å¯ï¼Œç»§ç»­æ¶ˆè´¹...")
        
        check_backlog = True
        lastid = "0-0"  # é‡æ–°ä»0å¼€å§‹ï¼Œä¼šè¯»å–pendingæ¶ˆæ¯
        
        while True:
            myid = lastid if check_backlog else ">"
            
            logger.info(f"  - è¯»å–æ¶ˆæ¯ï¼Œmyid={myid}, check_backlog={check_backlog}")
            
            messages = await self.redis_client.xreadgroup(
                consumer_group,
                consumer_name,
                {stream_key: myid},
                count=10,
                block=0 if check_backlog else 1000
            )
            
            if not messages or len(messages[0][1]) == 0:
                if check_backlog:
                    logger.info("  - æ²¡æœ‰æ›´å¤špending/backlogæ¶ˆæ¯")
                    check_backlog = False
                    continue
                else:
                    logger.info("  - æ²¡æœ‰æ–°æ¶ˆæ¯ï¼Œå®Œæˆ")
                    break
            
            msg_count = len(messages[0][1])
            logger.info(f"  - æ¢å¤å¹¶å¤„ç† {msg_count} æ¡æ¶ˆæ¯")
            
            # ACKæ¶ˆæ¯
            msg_ids = [msg[0] for msg in messages[0][1]]
            await self.redis_client.xack(stream_key, consumer_group, *msg_ids)
            logger.info(f"  - ACKäº† {len(msg_ids)} æ¡æ¶ˆæ¯")
            
            # æ›´æ–°lastid
            lastid = messages[0][1][-1][0].decode('utf-8') if isinstance(messages[0][1][-1][0], bytes) else messages[0][1][-1][0]
            check_backlog = msg_count >= 10
        
        # æœ€ç»ˆæ£€æŸ¥
        pending_info_after = await self.redis_client.xpending(stream_key, consumer_group)
        logger.info(f"\n5. æ¢å¤å®Œæˆåï¼Œpendingæ¶ˆæ¯æ•°: {pending_info_after['pending']}")
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ“ æ¼”ç¤ºå®Œæˆï¼špg_consumerä½¿ç”¨check_backlogæœºåˆ¶è‡ªåŠ¨æ¢å¤pendingæ¶ˆæ¯")
        logger.info("  å…³é”®ç‚¹ï¼šé‡å¯åè®¾ç½®lastid='0-0'å’Œcheck_backlog=True")
        logger.info("  è¿™æ ·ä¼šå…ˆå¤„ç†æ‰€æœ‰pendingå’Œæœªè¯»æ¶ˆæ¯ï¼Œç„¶åå†è¯»å–æ–°æ¶ˆæ¯")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    from dotenv import load_dotenv
    load_dotenv()
    
    tester = SelfRecoveryTester()
    await tester.setup()
    
    try:
        # æµ‹è¯•1ï¼šé˜Ÿåˆ—æ¶ˆæ¯è‡ªæˆ‘æ¢å¤
        queue_test = await tester.test_queue_self_recovery()
        await asyncio.sleep(1)
        
        # æµ‹è¯•2ï¼šTASK_CHANGESè‡ªæˆ‘æ¢å¤
        changes_test = await tester.test_task_changes_self_recovery()
        await asyncio.sleep(1)
        
        # æ¼”ç¤ºpg_consumerçš„å®é™…æ¢å¤æµç¨‹
        await tester.demonstrate_pg_consumer_recovery_flow()
        
        # æ€»ç»“
        logger.info("\n" + "=" * 60)
        logger.info("æµ‹è¯•æ€»ç»“")
        logger.info("=" * 60)
        logger.info(f"é˜Ÿåˆ—æ¶ˆæ¯è‡ªæˆ‘æ¢å¤æµ‹è¯•: {'âœ“ é€šè¿‡' if queue_test else 'âœ— å¤±è´¥'}")
        logger.info(f"TASK_CHANGESè‡ªæˆ‘æ¢å¤æµ‹è¯•: {'âœ“ é€šè¿‡' if changes_test else 'âœ— å¤±è´¥'}")
        
        if queue_test and changes_test:
            logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            logger.info("\nå…³é”®å‘ç°ï¼š")
            logger.info("1. å•ä¸ªworkeré‡å¯åï¼Œä½¿ç”¨ç›¸åŒçš„consumer_nameè¯»å–")
            logger.info("2. è®¾ç½®èµ·å§‹IDä¸º'0'æˆ–'0-0'ï¼Œå¯ä»¥è¯»å–è‡ªå·±çš„pendingæ¶ˆæ¯")
            logger.info("3. ä¸éœ€è¦XCLAIMï¼Œç›´æ¥xreadgroupå³å¯æ¢å¤")
            logger.info("4. pg_consumerçš„check_backlogæœºåˆ¶å®Œç¾æ”¯æŒè¿™ç§æ¢å¤æ–¹å¼")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å‡ºé”™: {e}", exc_info=True)
    finally:
        await tester.cleanup()


if __name__ == '__main__':
    asyncio.run(main())