import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from requests.adapters import HTTPAdapter, Retry
import GEOparse

def download_geo_family(gse_id: str, dest_dir: str = "Downloads", max_depth: int = 3, delay: float = 0.5):

    """
    Downloads all available files for a given GEO Series (GSE) ID from NCBI GEO FTP.

    Parameters:
    - gse_id: str — GEO Series ID (e.g., "GSE12345")
    - dest_dir: str — Destination base directory (default: "Downloads")
    - max_depth: int — Maximum recursion depth for subfolders (default: 3)
    - delay: float — Delay (in seconds) between requests (default: 0.5)

    Args:
        gse_id (str): GEO accession ID to download, e.g., "GSE12345".

    Returns:
        all the files.

    Example:
        >>> dl = geofetchr.download_geo_family("gse_id")
        >>> print(dl)

    """

    number = int(gse_id.replace("GSE", ""))
    digits = len(gse_id) - 6  # dynamic digit slice to form prefix
    prefix = f"GSE{str(number)[:digits]}nnn"
    base_url = f"https://ftp.ncbi.nlm.nih.gov/geo/series/{prefix}/{gse_id}/"
    dest_path = os.path.join(dest_dir, gse_id)
    os.makedirs(dest_path, exist_ok=True)

    session = requests.Session()
    retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retries))

    visited_urls = set()

    def download_all_files(url: str, local_path: str, depth: int):
        if depth > max_depth or url in visited_urls or not url.startswith(base_url):
            return
        visited_urls.add(url)

        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()
        except Exception as e:
            print(f"[ERROR] Cannot access {url} — {e}")
            return

        soup = BeautifulSoup(response.text, "html.parser")
        for link in soup.find_all("a"):
            href = link.get("href")
            if not href or href in ("../", "./"):
                continue

            full_url = urljoin(url, href)
            local_file_path = os.path.join(local_path, href)

            if href.endswith("/"):
                os.makedirs(local_file_path, exist_ok=True)
                time.sleep(delay)
                download_all_files(full_url, local_file_path, depth + 1)
            else:
                if os.path.exists(local_file_path) and os.path.getsize(local_file_path) > 0:
                    print(f"[SKIP] {href} already exists")
                    continue

                print(f"[DOWNLOADING] {href}")
                try:
                    with session.get(full_url, stream=True, timeout=60) as r:
                        r.raise_for_status()
                        total_size = int(r.headers.get("content-length", 0))
                        with open(local_file_path, "wb") as f, tqdm(
                            total=total_size,
                            unit="B",
                            unit_scale=True,
                            desc=href,
                            leave=False
                        ) as bar:
                            for chunk in r.iter_content(chunk_size=8192):
                                if chunk:
                                    f.write(chunk)
                                    bar.update(len(chunk))
                    print(f"[SAVED] {href}")
                except Exception as e:
                    print(f"[ERROR] Failed to download {href} — {e}")

                time.sleep(delay)

    print(f"[START] Downloading files for {gse_id}")
    download_all_files(base_url, dest_path, depth=0)
    print(f"[DONE] Completed downloading for {gse_id}")
    