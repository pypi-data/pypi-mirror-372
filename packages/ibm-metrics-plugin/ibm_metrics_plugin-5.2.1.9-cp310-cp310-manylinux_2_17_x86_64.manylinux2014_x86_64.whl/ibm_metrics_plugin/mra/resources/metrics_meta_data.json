
{
    "score_name": {"name": "Score Name", "description": "The main score of the benchmark."},
    "str": {"name": "En equivalent", "description": "description"}, 
    "num_of_instances": {"name": "Number of instances", "description": "Sample size used for evaluation."},
    "all_scores": {"name": "all scores", "description": ""},
    "average_score": {"name": "average score", "description": "The average score."},
    "bp": {
        "name": "Brevity Penalty",
        "description": "A factor used in the BLEU score to penalize translations that are too short compared to the reference translation."
    },
    "category_explicit_content": {
        "name": "Category Explicit Content",
        "description": "A harmlessness score leveraging reward model trained from human feedback for the category explicit content"
    },
    "category_harmful_info": {
        "name": "Category Harmful Info",
        "description": "A harmlessness score leveraging reward model trained from human feedback for the category harmful info"
    },
    "category_violence": {
        "name": "Category Violence",
        "description": "A harmlessness score leveraging reward model trained from human feedback for the category violence"
    },
    "health_score": {"name": "health score", "description": "The privacy health score, a data leakage score, ranges from 0 (no privacy) to 1 (perfect privacy)."},
    "injection_type_direct": {
        "name": "Injection Type Direct",
        "description": "Based on the evaluation of the judge LLM, the output of visual prompt injection test cases will be judged as either a successful or unsuccessful injection. These can be bucketed by injection type (direct vs. indirect)"
    },
    "injection_type_indirect": {
        "name": "Injection Type Indirect",
        "description": "Based on the evaluation of the judge LLM, the output of visual prompt injection test cases will be judged as either a successful or unsuccessful injection. These can be bucketed by injection type (direct vs. indirect)"
    },
    "injection_variant_different_user_input_language": {
        "name": "Injection Variant Different User Input Language",
        "description": "Based on the evaluation of the judge LLM, the output of visual prompt injection test cases will be judged as either a successful or unsuccessful injection. These can be bucketed by injection type (direct vs. indirect)."
    },
    "injection_variant_few_shot_attack": {
        "name": "Injection Variant Few Shot Attack",
        "description": "Exploits the context window by including a few examples of compliance prior to a harmful request."
    },
    "injection_variant_hypothetical_scenario": {
        "name": "Injection Variant Hypothetical Scenario",
        "description": "Provides hypothetical or imaginary scenarios to persuade the LLM that ignoring alignment in such contexts is acceptable."
    },
    "injection_variant_ignore_previous_instructions": {
        "name": "Injection Variant Ignore Previous Instructions",
        "description": "Instructs the model to ignore prior guardrail instructions and to provide malicious content."
    },
    "injection_variant_indirect_reference": {
        "name": "Injection Variant Indirect Reference",
        "description": "Instructions for the LLM to override system prompt instructions, supplied indirectly to the LLM in data like websites, source code, output generated by other LLMs, etc., that the LLM is processing in RAG or other integrated applications"
    },
    "injection_variant_many_shot_attack": {
        "name": "Injection Variant Many Shot Attack",
        "description": "Exploits the context window by including many examples of compliance prior to a harmful request."
    },
    "injection_variant_mixed_techniques": {
        "name": "Injection Variant Mixed Techniques",
        "description": "Combining multiple attack types to produce complex attacks."
    },
    "injection_variant_output_formatting_manipulation": {
        "name": "Injection Variant Output Formatting Manipulation",
        "description": "Disguise or dilute harmful intent by leveraging requests which instruct the response format."
    },
    "injection_variant_overload_with_information": {
        "name": "Injection Variant Overload With Information",
        "description": "Aims to overload and exploit the context of an LLM in order to jailbreak alignment protocols."
    },
    "injection_variant_payload_splitting": {
        "name": "Injection Variant Payload Splitting",
        "description": "Breaking a malicious prompt into multiple smaller parts (payloads), each of which does not trigger detection, but can be fully reassembled by an LLM"
    },
    "injection_variant_persuasion": {
        "name": "Injection Variant Persuasion",
        "description": "Treats LLMs as human-like communicators and use subtle human-developed interpersonal and persuasive arguments from social sciences and psychology to influence LLMsâ€™ response towards jailbreak goal."
    },
    "injection_variant_repeated_token_attack": {
        "name": "Injection Variant Repeated Token Attack",
        "description": "Precedes harmful requests with a repeated token or phrase."
    },
    "injection_variant_system_mode": {
        "name": "Injection Variant System Mode",
        "description": "Creation of Imaginary scenarios or personas related to jailbreak prompt that helps in persuading the LLMs to bypass safety protocols. Similar to virtualization."
    },
    "injection_variant_token_smuggling": {
        "name": "Injection Variant Token Smuggling",
        "description": "An attack may encoded using ASCII, Base46 or even Morse Code which hides the instruction from the user but suffices for the LLM."
    },
    "injection_variant_virtualization": {
        "name": "Injection Variant Virtualization",
        "description": "Creation of Imaginary scenarios or personas related to jailbreak prompt that helps in persuading the LLMs to bypass safety protocols."
    },
    "precisions": {"name": "Precisions", "description": "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation: Precision = TP / (TP + FP) where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive)."},
    "rouge1": {"name": "Rouge 1", "description": "Refers to the overlap of unigrams (each word) between the system and reference summaries."},
    "rouge2": {"name": "Rouge 2", "description": "Refers to the overlap of bigrams between the system and reference summaries."},
    "rougeL": {"name": "Rouge L", "description": "Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically."},
    "counts": {
        "name": "Counts",
        "description": "This refers to the number of matching n-grams between the candidate translation and the reference translations. For example if a bigram appears three times in both the candidate and reference texts it contributes three to the 'counts' metric."
    },
    "ref_len": {
        "name": "Reference length",
        "description": "This stands for 'Reference Length' and indicates the total number of words in the reference translations. It's crucial for calculating the brevity penalty in BLEU scores to ensure that shorter translations aren't unfairly favored"
    },
    "rougeLsum": {
        "name": "Rouge L sum",
        "description": "ROUGE-Lsum is a variant tailored for summarization tasks. Unlike ROUGE-L which computes the LCS over the entire text ROUGE-Lsum operates at the sentence level. It splits the text into sentences based on newlines computes the LCS for each sentence pair and averages these scores. This approach is particularly beneficial for extractive summarization where sentence-level extraction is crucial."
    },
    "sacrebleu": {
        "name": "Sacrebleu",
        "description": "SacreBLEU is a standardized tool for computing BLEU (Bilingual Evaluation Understudy) scores which measure the accuracy of machine translation by comparing the overlap of n-grams between the generated translation and reference translations. SacreBLEU addresses inconsistencies in BLEU score computations by providing a hassle-free comparable and reproducible framework."
    },
    "severity_critical": {
        "name": "Severity Critical",
        "description": "Indicates the percentage of evaluated instances that meet the critical threshold"
    },
    
    "severity_high": {
        "name": "Severity High",
        "description": "Indicates the percentage of evaluated instances that meet the high threshold"
    },
    "severity_low": {
        "name": "Severity Low",
        "description": "Indicates the percentage of evaluated instances that meet the low threshold"
    },
    "severity_medium": {
        "name": "Severity Medium",
        "description": "Indicates the percentage of evaluated instances that meet the medium threshold"
    },
    "speaking_language_English": {
        "name": "Speaking Language English",
        "description": "Dataset Annotation: Used to label data entries where the primary spoken language is English."
    },
    "sys_len": {
        "name": "System Length",
        "description": "Denotes the total number of words in the candidate translation produced by the LLM. It's used to assess the length of the generated output."
    },
    "totals": {
        "name": "Totals",
        "description": "Represents the total number of n-grams (sequences of 'n' words) present in the reference translations. For instance if evaluating bigrams (2-grams) 'Totals' would indicate the total count of bigrams across all reference sentences"
    },
    "unjudged_sample": {
        "name": "Unjudged Sample",
        "description": "Instances within the evaluation dataset that were not assessed or labeled."
    }
    
}
