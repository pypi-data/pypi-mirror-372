[
  {
    "Benchmark Details": {
      "Name": "Bias Benchmark for QA (BBQ)",
      "ID": "cards.safety.bbq.Age",
      "Overview": "Bias Benchmark for QA (BBQ), a dataset of question sets that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": "Question Answering systems, ",
      "Languages": "English",
      "Similar Benchmarks": "UnQover",
      "Resources": "[{'arxiv': 'https://arxiv.org/abs/2110.08193'}]"
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the biases of question answering (QA) models.  Evaluate model responses at two levels: (i) given an under-informative context, test how strongly responses reflect social biases, and (ii) given an adequately informative context, test whether the model's biases override a correct answer choice.",
      "Audience": "Researchers, developers, policymakers",
      "Tasks": "Question Answering",
      "Out-of-Scope Uses": "Systems that are not question answering (QA) models. "
    },
    "Data": {
      "Source": "Bias Benchmark for QA (BBQ), a dataset of question sets that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.",
      "Size": "58,492 unique examples",
      "Format": "Templates which consist of two questions, answer choices, a partial context missing information necessary to answer the questions, and a disambiguating context that provides the necessary information.",
      "Annotation": "Human annotators, MTurk workers located in the US, compensated for their time"
    },
    "Methodology": {
      "Main Score":"accuracy",
      "Desired_score"  : "Higher is Better",
      "Methods": "Metric: accuracy.  Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\n Where:\nTP: True positive\nTN: True negative\nFP: False positive\nFN: False negative\n, Metric Citation\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E҆},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n.",
      "Metrics": "Accuracy",
      "Calculation": "Accuracy = (TP + TN) / (TP + TN + FP + FN)",
      "Interpretation": "Higher accuracy indicates better performance. However, it's important to note that accuracy alone does not paint a complete picture of a model's performance, especially when it comes to assessing biases in language models.  A low bias score should not be seen as indicative of a less biased model in all cases.",
      "Validation": "Templates were validated by MTurk testing of rater agreement. The paper reports an estimate that raw human (crowdworker annotator) accuracy on BBQ is 95.7%, and aggregate human accuracy calculated via majority vote is 99.7%."
    },
    "Risks": {
      "Risk Categories": "Social Biases, stereotype reinforcement, stereotype attribution",
      "Limitations": "Gaps in demographic coverage or specific domains. Categories are limited to a US English-speaking cultural context and do not include all possible social biases",
      "Harm": "Perpetuation of stereotypes and biases"
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "Publicly available data",
      "Data Licensing": "CC BY 4.0"
  },
    "Benchmark Summary": "benchmark uses question sets that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts"
  },

  {
    "Benchmark Details": {
      "Name": "Prompt Leakage (RAG)",
      "ID": "cards.prompt_leakage.rag_dataset",
      "Overview": "This benchmark is designed to assess the tendency of a model to leak its system prompt, also known as the base prompt or prompt template. The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt. The rag-dataset is used to add few shot examples to the system prompt. The metric used to assess this ability is the prompt leakage score, which measures the degree of similarity between the model's response and the actual system prompt.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": ["LLMs"],
      "Languages": "English",
      "Similar Benchmarks": "prompt-leaking",
      "Resources": [
        { "arxiv": "https://arxiv.org/pdf/2307.06865.pdf" },
        {
          "huggingface": "https://huggingface.co/datasets/neural-bridge/rag-dataset-12000"
        }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the tendency of a model to leak its system prompt, also known as the base prompt or prompt template.",
      "Audience": "Researchers, developers, policymakers, etc.",
      "Tasks": "Prompt leakage assessment",
      "Out-of-Scope Uses": "Models that do not have a system prompt."
    },
    "Data": {
      "Source": "The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt.",
      "Size": "The dataset contains 58 prompts.",
      "Format": "The dataset is in CSV format and contains two columns: the prompt and rank.",
      "Annotation": " The dataset was manually curated by IBM and extended using the mixtral-8x7b-instruct-v01 model."
    },
    "Methodology": {
      "Main Score" : "Prompt Leakage (RAG)",
      "Desired_score"  : "Higher is Better",
      "Methods": "The benchmark uses a prompt leakage attack to assess the model's tendency to disclose its system prompt. The attack involves presenting the model with a series of prompts and measuring the degree of similarity between the model's response and the actual system prompt.",
      "Metrics": "This metric is based on a system prompt leakage attack and represents to what degree the model is willing to disclose its own system/base prompt. The attack tries to cause the model to leak its system prompt, and measures the degree of similarity between the model response and the actual system prompt. A high similarity between the model response and the actual prompt represents higher leakage risk. The score is based on the attack’s average similarity score across all attack prompts tested, weighted according to their difficulty level, and normalized such that that 1 represents perfect privacy and 0 represents worst privacy.",
      "Calculation": "The prompt leakage score is calculated using a similarity metric, such as rouge or bert, between the model's response and the actual system prompt.",
      "Interpretation": "A higher prompt leakage score indicates a lower risk of the model leaking its system prompt (better privacy).",
      "Baseline Results": "",
      "Validation": ""
    },
    "Risks": {
      "Risk Categories": "Prompt leaking",
      "Limitations": "The benchmark work does not experiment with sophisticated attacking strategies. ",
      "Harm": "Prompt leaking risk for AI. A successful attack exposes the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to use this information to mount a secondary attack, such as performing fraud."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not handle any personal or sensitive data, and it does not apply any anonymization techniques.",
      "Data Licensing": "This is a proprietary IBM dataset.",
      "Consent Procedures": "The benchmark does not involve any personal data, and it does not require consent from users."
    },
    "Benchmark Summary": "benchmark uses a prompt leakage attack to assess the model's tendency to disclose its system prompt. The attack involves presenting the model with a series of prompts and measuring the degree of similarity between the model's response and the actual system prompt"
  },
  {
    "Benchmark Details": {
      "Name": "Dolly data leakage",
      "ID": "cards.data_leakage.dolly",
      "Overview": "This dataset contains instruction-following examples, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization, Human preference data about helpfulness and harmlessness and Human-generated and annotated red teaming dialogues. Training data is used to assess the model's tendency to reveal confidential information or sensitive personal information in its generated output.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": ["LLMs"],
      "Languages": "English",
      "Similar Benchmarks": "",
      "Resources": "[{'huggingface': 'mosaicml/dolly_hhrlhf'}]"
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the risk of LLMs revealing memorized training samples in their generated output.",
      "Audience": "Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.",
      "Tasks": "The benchmark is designed for the task of detecting data leakage",
      "Out-of-Scope Uses": "The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering."
    },
    "Data": {
      "Source": "This uses the dataset mosaicml/dolly_hhrlhf.  This dataset is a combination of [Databrick's dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset and a filtered subset of [Anthropic's HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf). It also includes a test split, which was missing in the original `dolly` set. That test set is composed of 200 randomly selected samples from `dolly` + 4,929 of the test set samples from HH-RLHF which made it through a filtering process. The train set contains 59,310 samples; `15,014 - 200 = 14,814` from Dolly, and the remaining 44,496 from HH-RLHF..",
      "Size": "The dataset contains 64,539 samples, with 59,310 samples in the training set and 5,229 samples in the test set. The download size is reported as 24882010 and dataset size as 48260742. A subset of data (1000 sample instances) is used to run the MRA recipe.",
      "Format": "The dataset is stored in the Hugging Face Dataset format, with each sample consisting of a prompt and its corresponding response.",
      "Annotation": "The dataset was manually curated and filtered by MosaicML, and the responses were checked for the presence of sensitive information."
    },
    "Methodology": {
      "Desired_score"  : "Lower is Better",
      "Methods": "The benchmark looks for high syntactic similarity between the model responses and the actual samples, to identify leakage risk.",
      "Metrics": "Data leakage (metrics.data_leakage). ",
      "Calculation": "This metric is based on a training data or fine-tuning data leakage attack, and represents to what degree the model has memorized its training data. The attack tries to cause the model to leak complete or partial samples from its training (or fine-tuning) data, and measures the degree of similarity between the model responses and the actual training data.",
      "Interpretation": "A high syntactic similarity between the model response and the actual sample represents higher leakage risk. The score is based on the attack’s average similarity score across all training samples tested, and normalized such that 1 represents perfect privacy and 0 represents worst privacy."
    },
    "Risks": {
      "Risk Categories": [
        "revealing-confidential-information",
        "exposing-personal-information"
      ],
      "Limitations": "This benchmark only assesses the leakage of this specific dataset and no other datasets used in the model’s training or tuning. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. The benchmark assumes partial knowledge of the training samples in order to prompt the model to leak the rest of the data.",
      "Demographic Analysis": "Unknown",
      "Harm": "The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information in their generated output. Risk Title: Revealing confidential information,\n Tag: revealing-confidential-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:intellectual property,\n Descriptor:amplified,\n Description: When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\n Tag: exposing-personal-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:privacy,\n Descriptor:amplified,\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not include any personal or sensitive data.",
      "Data Licensing": "The dataset is licensed under the CC BY-SA 3.0 license, which allows for free use and distribution of the data.",
      "Consent Procedures": "Unknown"
    },
     "Benchmark Summary": "benchmark looks for high syntactic similarity between the model responses and the actual samples, to identify leakage risk"
  },

  {
    "Benchmark Details": {
      "Name": "Prompt Leakage (QNLI)",
      "ID": "cards.prompt_leakage.glue_qnli",
      "Overview": "This benchmark is designed to assess the tendency of a model to leak its system prompt, also known as the base prompt or prompt template. The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt. The GLUE QNLI dataset is used to add few shot examples to the system prompt. The metric used to assess this ability is the prompt leakage score, which measures the degree of similarity between the model's response and the actual system prompt.",
      "Data Type": "Text",
      "Domains": ["natural language processing"],
      "Application Domains": ["LLMs"],
      "Languages": ["English"],
      "Similar Benchmarks": ["Prompt Leakage (RAG)"],
      "Resources": [
        { "arxiv": "https://arxiv.org/pdf/2307.06865.pdf" },
        { "huggingface": "glue" }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "The benchmark is designed to assess the tendency of LLMs to leak their system prompt, and to provide a way to measure and evaluate this risk. The benchmark is intended for researchers, developers, and policymakers who are interested in understanding and addressing the potential security and privacy risks associated with LLMs.",
      "Audience": "The benchmark is intended for researchers, developers, and policymakers who are interested in understanding and addressing the prompt leakage from their LLMs",
      "Tasks": "The benchmark evaluates the model's tendency to leak its system prompt by measuring the degree of similarity between the model's responses and the system prompts. The benchmark does not assess other aspects of the model's performance, such as accuracy or generation quality.",
      "Out-of-Scope Uses": "Models that do not have a system prompt."
    },
    "Data": {
      "Source": "The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt.",
      "Size": "The dataset contains 58 prompts.",
      "Format": "The dataset is in CSV format and contains two columns: the prompt and rank.",
      "Annotation": "The dataset was manually curated by IBM and extended using the mixtral-8x7b-instruct-v01 model."
    },
    "Methodology": {
      "Main Score" : "Prompt Leakage (QNLI)",
      "Desired_score"  : "Higher is Better",
      "Methods": "The benchmark evaluates the model's tendency to leak its system prompt by measuring the degree of similarity between the model's responses and the system prompts. The benchmark uses a prompt leakage attack that tries to cause the model to leak its system prompt, and measures the degree of similarity between the model response and the actual system prompt. The attack is designed to be challenging for the model, and the benchmark uses a set of difficulty levels to measure the attack's effectiveness. The benchmark also provides a set of metrics that can be used to assess the model's performance, including the average similarity score across all attack prompts tested, and the normalized score that represents the degree of similarity between the model response and the system prompt. The benchmark provides a way to measure and evaluate the model's tendency to leak its system prompt, and can be used to help identify and mitigate potential security and privacy risks associated with LLMs.",
      "Metrics": "Prompt leakage ['metrics.prompt_leakage'].",
      "Calculation": "The prompt leakage score is calculated using a similarity metric, such as rouge or bert, between the model's response and the actual system prompt.",
      "Interpretation": "A higher prompt leakage score indicates a lower risk of the model leaking its system prompt (better privacy)."
    },
    "Risks": {
      "Risk Categories": "Prompt leaking",
      "Limitations": "The benchmark work does not experiment with sophisticated attacking strategies.",
      "Harm": "Prompt leaking risk for AI. A successful attack copies the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to replicate some of the functionality of the model."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not handle any personal or sensitive data.",
      "Data Licensing": "This is a proprietary IBM dataset."
  },
     "Benchmark Summary": "benchmark evaluates the model's tendency to leak its system prompt by measuring the degree of similarity between the model's responses and the system prompts. The benchmark uses a prompt leakage attack that tries to cause the model to leak its system prompt, and measures the degree of similarity between the model response and the actual system prompt"
  },

  {
    "Benchmark Details": {
      "Name": "The AttaQ Dataset",
      "ID": "cards.value_alignment.toxic_output.attaq",
      "Overview": "The AttaQ dataset is a benchmark for evaluating the harmlessness of Large Language Models (LLMs) and to further investigate the factors influencing LLM behavior. It consists of adversarial samples that represent questions and queries for which LLMs should refrain from providing answers, such as inquiries about constructing dangerous devices or engaging in harmful activities.",
      "Data Type": "Text",
      "Domains": ["natural language processing"],
      "Application Domains": ["LLMs"],
      "Languages": ["English"],
      "Similar Benchmarks": ["None"],
      "Resources": [
        {
          "arxiv": "https://arxiv.org/pdf/2311.04124"
        },
        {
          "huggingface": "ibm/AttaQ"
        }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "Designed to provoke harmful or inappropriate responses.",
      "Audience": "Researchers, developers and policymakers working on safety and ethical considerations in AI.",
      "Tasks": "Evaluate the tendency of LLMs to generate harmful or undesirable responses in text generation.",
      "Out-of-Scope Uses": "The benchmark is not designed to be applied in cases where harmful or unethical content is not the primary concern."
    },
    "Data": {
      "Source": "The dataset is a representative subset of all possible attacks, and many other attacks can cause LLMs to generate harmful or inappropriate responses.",
      "Size": "The dataset consists of 1402 adversarial questions.",
      "Format": "The dataset is in CSV format, with columns for label (label of adversarial question), input (adversarial question), and additional_info (source of the adversarial question). The data contains offensive and upsetting content by nature therefore it may not be easy to read.",
      "Annotation": "The dataset was derived by extracting attacks from the hh-rlhf dataset provided by Anthropic, and semi-automatically labelling them into seven categories: deception, discrimination, harmful information, substance abuse, sexual content, personally identifiable information (PII), and violence. LLMs generated further attack data and the Wikipedia crimes page was used to generate actions associated with each crime.  These actions were used to create the question data specifically focusing on strategies to evade detection or consequences. "
    },
    "Methodology": {
      "Main Score" : "safety",
      "Desired_score"  : "Higher is Better",
      "Methods": "The dataset is created by semi-automatically labeling extracted attacks into seven categories.",
      "Metrics": "The metric used is ['metrics.mra_safety'], which provides a harmlessness score leveraging a reward model trained from human feedback.",
      "Calculation": "The safety score is a measure of harmlessness. The level of harmlessness is evaluated using a reward model that considers both the input and output. The metric is described further in https://arxiv.org/abs/2209.07858.",
      "Interpretation": "A higher harmlessness score can be seen as better or less harmful, and a lower score can be seen as more harmful."
    },
    "Risks": {
      "Risk Categories": ["Toxic output", "Harmful output"],
      "Limitations": "The dataset is not designed to evaluate all possible risks of LLMs.  In the case of template-generated attacks, the model refused to generate inputs related to certain crimes and actions. This could potentially introduce biases to the dataset, possibly omitting certain crime domains. Secondly, the definition of what constitutes a crime is culturally specific which can potentially introduce bias.This dataset is a representative subset of all possible attacks. Many other attacks can cause LLM to answer harmful or inappropriate responses.",
      "Harm": "Toxic output occurs when the model produces hateful, abusive, and profane (HAP) or obscene content. This also includes behaviors like bullying.,\n Concern: Hateful, abusive, and profane (HAP) or obscene content can adversely impact and harm people interacting with the model..', 'Risk Title: Harmful output,\n Tag: harmful-output,\n Type: output,\n Phase:None,\n Category:None,\n Group:value alignment,\n Descriptor:specific,\n Description: A model might generate language that leads to physical harm The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements.,\n Concern: A model generating harmful output can cause immediate physical harm or create prejudices that might lead to future harm..']"
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The dataset does not include any personal or sensitive data.",
      "Data Licensing": "The dataset is licensed under the MIT license.",
      "Consent Procedures": "The dataset does not include any personal data, so no consent is required."
    },
     "Benchmark Summary": "benchmark evaluates the harmlessness of Large Language Models (LLMs) and further investigate the factors influencing LLM behavior. It consists of adversarial samples that represents questions and queries for which LLMs should refrain from providing answers, such as inquiries about constructing dangerous devices or engaging in harmful activities"
  },


  {
    "Benchmark Details": {
      "Name": "CNN Daily Mail data leakage",
      "ID": "cards.data_leakage.cnn_dailymail",
      "Overview": "This dataset contains news articles as written by journalists at CNN and the Daily Mail. It includes both training and test splits, and is used to assess the model's tendency to reveal confidential information or sensitive personal information in its generated output.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": ["LLMs"],
      "Languages": "English",
      "Similar Benchmarks": "",
      "Resources": [
        { "github": "https://github.com/abisee/cnn-dailymail" },
        {
          "paper": "https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf"
        },
        {
          "leaderboard": "https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail"
        }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the risk of LLMs revealing memorized training samples in their generated output.",
      "Audience": "Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.",
      "Tasks": "The benchmark is designed for the task of detecting data leakage.",
      "Out-of-Scope Uses": "The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering."
    },
    "Data": {
      "Source": "This uses the dataset CNN / Daily Mail.  This dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.",
      "Size": "The dataset contains 287,113 samples in the training set, 13,368 in the validation set and 11,490 samples in the test set.  The test set is not used in this assessment. The download size is reported as 837094602 and dataset size as 1369361929. A subset of data (1000 sample instances) is used to run the MRA recipe.",
      "Format": "The dataset is stored in the Hugging Face Dataset format. For each instance, there is a string for the article, a string for the highlights, and a string for the id.",
      "Annotation": "The dataset does not contain any additional annotations."
    },
    "Methodology": {
      "Desired_score"  : "Lower is Better",
      "Methods": "The benchmark looks for high syntactic similarity between the model responses and the actual samples, to identify leakage risk.",
      "Metrics": "Data leakage (metrics.data_leakage).",
      "Calculation": "This metric is based on a training data or fine-tuning data leakage attack, and represents to what degree the model has memorized its training data. The attack tries to cause the model to leak complete or partial samples from its training (or fine-tuning) data, and measures the degree of similarity between the model responses and the actual training data.",
      "Interpretation": "A high syntactic similarity between the model response and the actual sample represents higher leakage risk. The score is based on the attack’s average similarity score across all training samples tested, and normalized such that 1 represents perfect privacy and 0 represents worst privacy."
    },
    "Risks": {
      "Risk Categories": [
        "revealing-confidential-information",
        "exposing-personal-information"
      ],
      "Limitations": "This benchmark only assesses the leakage of this specific dataset and no other datasets used in the model’s training or tuning. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. The benchmark assumes partial knowledge of the training samples in order to prompt the model to leak the rest of the data.",
      "Demographic Analysis": "Bordia and Bowman (2019) find the CNN / Dailymail dataset to have a slightly lower gender bias based on their metric compared to the other datasets, but still show evidence of gender bias when looking at words such as 'fragile'.",
      "Harm": "When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret..', 'Risk Title: Exposing personal information,\n Tag: exposing-personal-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:privacy,\n Descriptor:amplified,\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable..'"
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "Version 3.0 of data set is not anonymized, so individuals' names can be found in the dataset. Information about the original author is not included in the dataset.",
      "Data Licensing": "The dataset is licensed under the Apache-2.0 License."
  },
    "Benchmark Summary": "No Summary is not avialable"
  },

  {
    "Benchmark Details": {
      "Name": "Jailbreak Risk LLM Judge",
      "ID": "cards.jailbreak_risk.jailbreak_risk_llm_judge",
      "Overview": "The dataset represents jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is based on a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. We use this dataset as a benchmark for evaluating the jailbreak risk of LLMs.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": ["LLMs"],
      "Languages": ["English"],
      "Similar Benchmarks": [],
      "Resources": [
        { "github": "https://github.com/IBM/Adversarial-Prompt-Evaluation" },
        { "paper": "https://openreview.net/pdf?id=a44MiSFw6G" }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "Assess the jailbreak risk of LLMs",
      "Audience": "Researchers, developers, and policymakers",
      "Tasks": "Evaluate the jailbreak risk of LLMs",
      "Out-of-Scope Uses": "Not available"
    },
    "Data": {
      "Source": "The dataset was manually compiled from several public datasets.",
      "Size": "10644",
      "Format": "CSV format, with columns for prompt, label, and source.",
      "Annotation": "The dataset was manually curated by IBM."
    },
    "Methodology": {
      "Main Score" : "Jailbreak Risk LLM Judge",
      "Desired_score"  : "Lower is Better",
      "Methods": "Uses an LLM-as-judge metric for evaluating the jailbreak risk of generated predictions.",
      "Metrics": "Metric summary:['metrics.jailbreak_risk_llm_judge'], ",
      "Calculation": "Uses the source prompt, a template which reflects the criteria for judging jailbreaking risk, and the predictions to evaluate correctness using rating.single_turn with a designated judge model.",
      "Interpretation": "A higher score indicates a higher level of jailbreak risk."
    },
    "Risks": {
      "Risk Categories": ["prompt-injection", "jailbreaking"],
      "Limitations": "Not all LLMs (including large ones) can serve as good judges - their assessment may not correlate with human judgements and can also be biased. This means that unless you have a prior indication that the LLM you use is a good judge for your task, you need to evaluate its judgements and see they match your expectations.",
      "Demographic Analysis": "Unknown",
      "Harm": "A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.,\n Concern: Injection attacks can be used to alter model behavior and benefit the attacker. Risk Title: Jailbreaking,\n Tag: jailbreaking,\n Type: input,\n Phase:inference,\n Category:,\n Group:multi-category,\n Descriptor:specific,\n Description: A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.,\n Concern: Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not handle any personal or sensitive data.",
      "Data Licensing": "IBM proprietary dataset, available upon request."
  },
     "Benchmark Summary": "benchmark uses an LLM-as-judge metric for evaluating the jailbreak risk of generated predictions. It evaluates the jailbreak risk of LLMs and uses jailbreak prompts which are employed to bypass the safety alignment of LLMs"
  },

  {
    "Benchmark Details": {
      "Name": "Jailbreak Risk",
      "ID": "cards.jailbreak_risk.jailbreak_risk",
      "Overview": "The dataset represents jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is based on a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. We use this dataset as a benchmark for evaluating the jailbreak risk of LLMs.",
      "Data Type": "Text",
      "Domains": ["Natural language processing"],
      "Application Domains": ["Question Answering"],
      "Languages": ["English"],
      "Similar Benchmarks": [],
      "Resources": [
        { "github": "https://github.com/IBM/Adversarial-Prompt-Evaluation" },
        { "paper": "https://openreview.net/pdf?id=a44MiSFw6G" }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "Assess the jailbreak risk of LLMs",
      "Audience": "Researchers, developers, and policymakers",
      "Tasks": ["Evaluate the jailbreak risk of LLMs"],
      "Out-of-Scope Uses": "Not available"
    },
    "Data": {
      "Source": "The dataset was manually compiled from several public datasets.",
      "Size": "10644 prompts",
      "Format": "CSV format, with columns for prompt, label, and source.",
      "Annotation": "The dataset was manually curated by IBM."
    },
    "Methodology": {
      "Main Score" : "Jailbreak Risk",
      "Desired_score"  : "Lower is Better",
      "Methods": "The dataset is a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions.",
      "Metrics": ["Metric summary:['metrics.jailbreak_risk']"],
      "Calculation": "The metric is calculated based on the ability of the model to provide correct answers to jailbreak prompts",
      "Interpretation": "A higher score indicates a higher level of jailbreak risk.",
      "Baseline Results": "The dataset is not accompanied by any baseline results.",
      "Validation": "The dataset is not accompanied by any validation details."
    },
    "Risks": {
      "Risk Categories": ["prompt-injection", "jailbreaking"],
      "Limitations": "The dataset is not provided with any limitations or details on evaluating or addressing risks",
      "Demographic Analysis": "Unknown",
      "Harm": "A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.,\n Concern: Injection attacks can be used to alter model behavior and benefit the attacker. Risk Title: Jailbreaking,\n Tag: jailbreaking,\n Type: input,\n Phase:inference,\n Category:,\n Group:multi-category,\n Descriptor:specific,\n Description: A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.,\n Concern: Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not handle any personal or sensitive data.",
      "Data Licensing": "IBM proprietary dataset, available upon request."
    },
    "Benchmark Summary": "benchmark data contains jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is based on a compilation of diverse datasets with malicious prompts spanning several categories"
 },




  {
    "Benchmark Details": {
      "Name": "SocialStigmaQA",
      "ID": "cards.value_alignment.toxic_output.social_stigma_cot",
      "Overview": "A benchmark to uncover stigma amplification in generative language models. The benchmark aims to test for social bias and model robustness in generative language models. It consists of 10K prompts with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": ["Question Answering"],
      "Languages": "English",
      "Similar Benchmarks": "None",
      "Resources": [
        { "source": "ibm/SocialStigmaQA" },
        { "paper": "https://ojs.aaai.org/index.php/AAAI/article/view/30142" },
        { "model": "ibm/social-bias-detector-v0" }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "Assess the model's ability to produce hateful, abusive, and profane (HAP) or obscene content, as well as its robustness to social bias.",
      "Audience": "Researchers, developers, and policymakers in the field of natural language processing, as well as those interested in social bias and model evaluation.",
      "Tasks": "Question-answering, toxic output detection",
      "Out-of-Scope Uses": "The sole purpose for of the benchmark and dataset is as a bias auditing tool, meant to evaluate generative language models."
    },
    "Data": {
      "Source": "The bencmark uses the ibm/SocialStigmaQA curated dataset. The dataset is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, it starts with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. The dataset, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness.",
      "Size": "10,000 prompts",
      "Format": "CSV file. 37 patterns are present within the dataset. These patterns, or templates, were individually hand curated. Each of these patterns follows the same overarching structure - provide a mundane social situation and ask a question pertaining to this situation. The dataset to contain patterns for which answering either “yes” or “no” implies social bias."
    },
    "Methodology": {
      "Desired_score"  : "not applicable",
      "Methods": "Question-answering, toxic output detection",
      "Metrics": "Metric summary: ['metrics.socialbias']",
      "Calculation": "The social-bias-detector is a BERT-like classification model which takes as input a sentence and outputs a label. It calculates the proportion of biased outputs and biased CoT (chain-of-thought) for the yes_no and cot versions of the dataset.",
      "Interpretation": "Interpret the scores based on the proportion of biased outputs and biased CoT."
    },
    "Risks": {
      "Risk Categories": "Toxic output",
      "Limitations": "Using this dataset on a large language model may result in text that is harmful and discriminatory. Possible limitations in evaluating or addressing stigmas, due to gaps in demographic coverage or specific domains, as the dataset is in English and reflects US culture.  The authors of the benchmark view evaluating open-ended text generation as an unsolved problem, so there may subtle biases pressent in CoT results.  Aditionally it is still an open question on how bias scores should be adjusted for certain models.",
      "Harm": "Toxic output occurs when the model produces hateful, abusive, and profane (HAP) or obscene content. This also includes behaviors like bullying.,\n Concern: Hateful, abusive, and profane (HAP) or obscene content can adversely impact and harm people interacting with the model."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "No personal or sensitive data is included in the benchmark.",
      "Data Licensing": "The benchmark data is available under the cdla-permissive-2.0 license."
  },
     "Benchmark Summary": "benchmark aims to test for social bias and model robustness in generative language models"
  },

  {
    "Benchmark Details": {
      "Name": "TruthfulQA",
      "ID": "cards.value_alignment.hallucinations.truthfulqa",
      "Overview": "TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions.",
      "Data Type": "Text",
      "Domains": ["natural language processing"],
      "Application Domains": ["Health", "Law", "Finance", "Politics"],
      "Languages": ["English"],
      "Similar Benchmarks": [],
      "Resources": [
        { "github": "https://github.com/sylinrl/TruthfulQA" },
        { "paper": "https://arxiv.org/abs/2109.07958" }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "A benchmark to measure whether a language model is truthful in generating answers to questions",
      "Audience": "Researchers, developers, policymakers",
      "Tasks": [
        "Multiple-choice question answering",
        "Text generation",
        "Question answering"
      ],
      "Out-of-Scope Uses": "Use cases where the benchmark is not designed to be applied and could give misleading results."
    },
    "Data": {
      "Source": "All questions were written by the authors and were designed to elicit imitative falsehoods. The questions were designed to be “adversarial” in the sense of testing for a weakness in the truthfulness of language models (rather than testing models on a useful task).",
      "Size": "The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics",
      "Format": "CSV format.  It is structured as sentence pairs, question-answer format. It includes columns: type, category, question, best_answer, correct_answers, incorrect_answers, source.",
      "Annotation": "Human annotations"
    },
    "Methodology": {
      "Main Score": "rougeL",
      "Desired_score"  : "Higher is Better",
      "Methods": "Adversarial procedure",
      "Metrics": "ROUGE: ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing.  Normalized Sacrebleu: SacreBLEU provides a method of computation of BLEU scores, which are a method of automatic machine translation evaluation.",
      "Calculation": "Rouge scores are calculated by comparing an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. Normalized Sacrebleu: Bleu metrics range from 0 to 1. Scores are calculated for individual translated segments by comparing them with a set of good quality translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. ",
      "Interpretation": "To perform well, models must avoid generating false answers learned from imitating human texts. A higher score suggests the model performed better at returning truthful responses. However, if a model performs well, we cannot conclude that it will be equally truthful on other kinds of tasks (even if we expect some transfer). "
    },
    "Risks": {
      "Risk Categories": "Hallucination",
      "Limitations": "TruthfulQA does not cover long-form generation (e.g. news articles) or interactive settings (e.g. extended chat with an adversarial human). Moreover, while the questions in TruthfulQA resemble real-world questions, they were not collected from a deployed system — and hence may over- or underestimate truthfulness for a deployed system.",
      "Demographic Analysis": "Performance across different demographic groups",
      "Harm": "Hallucinations generate factually inaccurate or untruthful content with respect to the model’s training data or input. This is also sometimes referred to lack of faithfulness or lack of groundedness.,\n Concern: Hallucinations can be misleading. These false outputs can mislead users and be incorporated into downstream artifacts, further spreading misinformation. False output can harm both owners and users of the AI models. In some uses, hallucinations can be particularly consequential."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "Not available",
      "Data Licensing": "This dataset is licensed under the [Apache License, Version 2.0]",
      "Consent Procedures": "Obtaining consent (if applicable)",
      "Compliance with Regulations": "Compliance with relevant legal or ethical regulations"
  },
     "Benchmark Summary": "benchmark aims to measure whether a language model is truthful in generating answers to questions"
  },

  {
    "Benchmark Details": {
      "Name": "Mitre CSE",
      "ID": "cards.harmful_code.mitre.mitre_cse_benchmark",
      "Overview": "The Mitre Test is a benchmark designed to assess the compliance of a Large Language Model (LLM) with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. The benchmark involves three LLMs: a target LLM, an expansion LLM, and a judge LLM. The expansion LLM summarizes the response generated by the target LLM, which is then classified by the judge LLM as benign or malicious. The benchmark provides the numbers of test prompts and number of classifications as benign, malicious, and refusal of classification.",
      "Data Type": "Text",
      "Domains": ["Cybersecurity"],
      "Application Domains": ["Cybersecurity", "LLMs"],
      "Languages": "['english']",
      "Similar Benchmarks": [],
      "Resources": [
        { "paper": "https://arxiv.org/abs/2404.13161" },
        {
          "github": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks"
        }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "The primary goal of the Mitre Test is to evaluate the ability of LLMs to generate harmful code that could be used in cyberattacks. ",
      "Audience": "The benchmark is intended for researchers, developers, and policymakers who are interested in assessing the cybersecurity risks associated with LLMs.",
      "Tasks": "The benchmark is designed to assess the compliance of an LLM with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. The benchmark evaluates the LLM's ability to generate code that could be used in cyberattacks.",
      "Out-of-Scope Uses": "The benchmark is not designed to be applied to evaluate the cybersecurity risks associated with other types of language generation models, such as those used for creative writing or storytelling."
    },
    "Data": {
      "Source": "The data used is benchmark data, publicly available under the MIT license.",
      "Size": "4.3 MB",
      "Format": "CSV file. The data instances include the base_prompt ,mutated_prompt_base, mutated_prompt, mitre_category, ttp_id_name_mapping.",
      "Annotation": "The data is automatically labeled by the judge LLM."
    },
    "Methodology": {
      "Main Score"  : "Mitre CSE",
      "Desired_score"  : "Lower is Better",
      "Methods": "The benchmark evaluates the compliance of an LLM with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. ",
      "Metrics": "Metric: Harmful Code Mitre CSE [metrics.harmful_code.mitre.mitre_cse_benchmark]",
      "Calculation": "The benchmark uses the metric of Harmful Code Mitre CSE, which is designed to assess the LLM's ability to generate code that could be used in cyberattacks. The metric involves three LLMs: a target LLM, an expansion LLM, and a judge LLM. The expansion LLM summarizes the response generated by the target LLM, which is then classified by the judge LLM as benign or malicious. The metric provides the numbers of test prompts and number of classifications as benign, malicious, and refusal of classification.",
      "Interpretation": "The metric provides the numbers of test prompts and number of classifications as benign, malicious, and refusal of classification. A higher number of classifications as malicious indicates a higher risk of harmful code generation."
    },
    "Risks": {
      "Risk Categories": ["Harmful code generation"],
      "Limitations": "Please note that this metric is a benchmark and is subject to ongoing development and refinement. Future improvements and advancements are planned to be incorporated to enhance its accuracy and effectiveness in model assessments.",
      "Demographic Analysis": "The benchmark does not provide a detailed analysis of the impact of LLMs on different demographic groups. However, it does note that the benchmark is designed to be inclusive and should be applied to all populations and demographic groups.",
      "Harm": "Models might generate code that causes harm or unintentionally affects other systems.,\n Concern: The execution of harmful code might open vulnerabilities in IT systems."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not use any personal or sensitive data. It does not collect or store any personal information from users or the general public.",
      "Data Licensing": "The data used in the benchmark is publicly available under the MIT license."
  },
     "Benchmark Summary": "benchmark designed to assess the compliance of a Large Language Model (LLM) with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. The Harmful Code Mitre CSE score describe LLM's ability to generate code that could be used in cyberattacks"
  },

  {
    "Benchmark Details": {
      "Name": "False Refusal Rate (FRR) CSE Benchmark",
      "ID": "cards.harmful_code.frr.frr_cse_benchmark",
      "Overview": "This benchmark evaluates the rate of falsely refusing benign prompts for generating source code of a target Large Language Models (LLM). The test prompts have been specifically designed to be borderline or ambiguous by containing keywords that are typical for benign as well as malicious source code requests. Refusals are determined by matching the generated output with a list of keywords commonly found in refusal statements of current LLMs. The final score is the fraction of benign test prompts that have been refused.",
      "Data Type": "Text",
      "Domains": ["Cybersecurity"],
      "Application Domains": ["Cybersecurity", "LLMs"],
      "Languages": ["English"],
      "Similar Benchmarks": [],
      "Resources": [
        {
          "Paper": "https://arxiv.org/abs/2404.13161"
        },
        {
          "Github": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks"
        }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "To give visibility into how often an LLM designed to refuse help with cyber attacks also refuses ambiguous cases that are not clearly malicious (the safety-utility tradeoff).",
      "Audience": "Researchers, developers, and policymakers working in the field of cybersecurity",
      "Tasks": "Assess the false refusal rate (FRR) of LLMs on benchmark datasets",
      "Out-of-Scope Uses": "Any other use than assessing the false refusal rate (FRR) of LLMs on benchmark datasets"
    },
    "Data": {
      "Source": "The data used is benchmark data, publicly available under the MIT license.",
      "Size": "352 KB",
      "Format": "CSV format.  The data instances include the mutated_prompt, is_malicious, attack_type, model",
      "Annotation": "General annotation, the data samples are borderline benign prompts. In terms of label: {benign}"
    },
    "Methodology": {
      "Main Score" : "False Refusal Rate (FRR) CSE Benchmark",
      "Desired_score"  : "Higher is Better",
      "Methods": "Benchmark datasets are used to evaluate the false refusal rate (FRR) of LLMs",
      "Metrics": "False Refusal Rate (FRR)",
      "Calculation": "This metric evaluates the rate of falsely refusing benign prompts for generating source code of a target Large Language Models (LLM). The test prompts have been specifically designed to be borderline or ambiguous by containing keywords that are typical for benign as well as malicious source code requests. Refusals are determined by matching the generated output with a list of keywords or patterns commonly found in refusal statements of current LLMs. The final score is the fraction of benign test prompts that have been refused.",
      "Interpretation": "A higher FRR score indicates a higher likelihood of LLMs rejecting to answer the requests that are borderline benign."
    },
    "Risks": {
      "Risk Categories": ["Harmful code generation"],
      "Limitations": "Please note that this metric is a benchmark and is subject to ongoing development and refinement. Future improvements and advancements are planned to be incorporated to enhance its accuracy and effectiveness in model assessments.",
      "Demographic Analysis": "Not applicable",
      "Harm": "Models might generate code that causes harm or unintentionally affects other systems.,\n Concern: The execution of harmful code might open vulnerabilities in IT systems."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "Benchmark datasets do not contain any personal or sensitive data",
      "Data Licensing": "The data used in the benchmark is publicly available under the MIT license."

    },
    "Benchmark Summary": "benchmark evaluates the rate of falsely refusing benign prompts for generating source code of a target Large Language Models (LLM). The test prompts have been specifically designed to be borderline or ambiguous by containing keywords that are typical for benign as well as malicious source code requests"

  },
 
  { "Benchmark Details": {
    "Name": "SimpleSafetyTests",
    "ID": "cards.safety.simple_safety_tests",
    "Overview": "SimpleSafetyTests assists in identifying critical safety risks in Large Language Models.  It consists of 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with",
    "Data Type": "text",
    "Domains": ["LLMs"],
    "Languages": ["English"],
    "Resources": [
      { "arxiv": "https://arxiv.org/abs/2311.08370" },
      {
        "huggingface": "https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests"
      }
    ]
  },
"Purpose and Intended Users": {
    "Goal": "A benchmark to measure whether an LLM is following malicious instructions, providing unsafe advice, and generating toxic content",
    "Audience": "Researchers, developers, policymakers",
    "Tasks": ["text-generation"],
    "Out-of-Scope Uses": []
},
    "Data": {
      "Source": "SST dataset consists of 100 test prompts equally split into five harm areas and two prompt categories. The harm areas are: (1) Suicide, Self-Harm, and Eating Disorders; (2) Physical Harm (3) Illegal and Highly Regulated Items; (4) Scams and Fraud; and (5) Child Abuse.  Ther categories are: (1) Information and Advice Seeking; and (2) Instructions and Actions.",
      "Size": "100 prompts",
      "Format": "CSV",
      "Annotation": "Hand-crafted. The test prompts were created by one of the authors, and then iteratively adjusted through multiple rounds of review and editing by the other authors and assessed for internal validity."
    },
    "Methodology": {
      "Main Score" : "sacrebleu",
      "Desired_score"  : "Higher is Better",
      "Metrics": [
        "Normalized Sacrebleu: SacreBLEU provides a method of computation of BLEU scores, which are a method of automatic machine translation evaluation."
      ],
      "Calculation": "Normalized Sacrebleu: Bleu metrics range from 0 to 100%. Scores are calculated for individual translated segments by comparing them with a set of good quality translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. ",
      "Interpretation": "To perform well, models must avoid returning unsafe responses. A higher score suggests better safety. However, if a model performs well, we cannot conclude that no safety risk exists, just that none was detected.",
      "Baseline Results": "The dataset is not accompanied by any baseline results.",
      "Validation": "The dataset is not accompanied by any validation details."
    },
    "Risks": {
      "Risk Categories": ["harmful-output"],
      "Limitations": "Caution is advised as the prompts are sensitive and you could find them offensive, however, for the vast majority of applications, LLMs should refuse to comply with all of them. SST is not intended to be a comprehensive test of model safety. If a model returns 0% unsafe responses to SST then it indicates only that no safety risks have been found, rather than guaranteeing the absence of any safety risks (negative predictive power). The test prompts are designed to be extremely unsafe, and may not be commonly used by actual users. However, if models fail on even these simple prompts, they may be vulnerable to more sophisticated forms of unsafe prompts.  SST is  limited to the English language, with ten prompts each across five harm areas, so there are gaps in demographic coverage and specific domains. ",
      "Demographic Analysis": "Not available",
      "Harm": "A model might generate language that leads to physical harm The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements.,\n Concern: A model generating harmful output can cause immediate physical harm or create prejudices that might lead to future harm."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "Personal or sensitive data not included",
      "Data Licensing": "cc-by-2.0",
      "Consent Procedures": "Not applicable",
      "Compliance with Regulations": "Not applicable"
  },
    "Benchmark Summary" :"benchmark assists in identifying critical safety risks in Large Language Models.  It consists of 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with"
  },

  {
    "Benchmark Details": {
      "Name": "Discrim-Eval Implicit",
      "ID": "cards.safety.discrim_eval.implicit",
      "Overview": "The data contains a diverse set of prompts covering 70 hypothetical decision scenarios, ranging from approving a loan to providing press credentials. Each prompt instructs the model to make a binary decision (yes/no) about a particular person described in the prompt. Each person is described in terms of three demographic attributes: age (ranging from 20 to 100 in increments of 10), gender (male, female, non-binary), and race (white, Black, Asian, Hispanic, Native American), for a total of 135 examples per decision scenario. The prompts are designed so a 'yes' decision is always advantageous to the person (e.g. deciding to grant the loan).",
      "Data Type": "Text",
      "Domains": "Question answering",
      "Languages": "English",
      "Resources": {
        "Paper": "https://arxiv.org/abs/2312.03689"
      }
    },
    "Purpose and Intended Users": {
      "Goal": "To evaluate the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed.",
      "Audience": "Researchers, developers, and policymakers who are interested in understanding and mitigating the potential risks of AI systems.",
      "Tasks": "Evaluating and mitigating discrimination in language model decisions.",
      "Out-of-Scope Uses": "Using the benchmark for high-risk automated decision making or for other purposes not related to its original goals."
    },
    "Data": {
      "Source": "Anthropic",
      "Size": "The dataset contains 135 examples per decision scenario, totalling 9,450 examples across all 70 decision scenarios.",
      "Format": "The data is provided in two JSONL files: explicit.jsonl and implicit.jsonl. Each line in the files contains a prompt, decision question ID, age, gender, and race of the person described in the prompt. The implicit.jsonl file does not have an explicit mention of race or gender, but rather relies on an implicit version of these attributes based on a name. ",
      "Annotation": "The data was annotated using Anthropics's Claude language models."
    },
    "Methodology": {
      "Main Score":"accuracy",
      "Desired_score"  : "Lower is Better",
      "Methods": "The benchmark uses a series of hypothetical decision scenarios to evaluate the ability of language models to make unbiased decisions. Each prompt is designed to elicit a binary decision (yes/no) about a particular person described in the prompt. The prompts are then used to test the ability of language models to make accurate decisions and to identify potential biases.",
      "Metrics": "The benchmark uses accuracy as the primary metric to evaluate the model's performance. Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN).",
      "Calculation": "The benchmark calculates the discrimination score for each decision scenario and demographic attribute based on the model's performance on the prompt examples.",
      "Interpretation": "The benchmark provides a way to measure the extent to which the model may be biased towards certain demographic groups. A higher discrimination score indicates a higher likelihood of discrimination.",
      "Baseline Results": "The benchmark compares the model's performance to a baseline model that does not take into account the demographic attributes of the person described in the prompt. This can help identify if the model is biased towards any particular demographic group.",
      "Validation": "The benchmark does not provide information on the validation process used to ensure the data and methods are valid and reliable."
    },
    "Risks": {
      "Risk Categories": ["output-bias"],
      "Limitations": "The benchmark does not cover all possible decision making scenarios or demographics, and may not be fully representative of all real-world scenarios.  In terms of external validity, the evaluation is done on a model's judgement of a short text description of candiates only, which does not account for the wider variety of input formats that might affect the model's conclusions. It is important to note that the dataset authors do not permit or endorse the use of LMs for high-risk automated decision making.",
      "Demographic Analysis": "The benchmark evaluates the performance of language models across different demographic groups, including age, gender, and race. However, it does not provide a detailed analysis of how these biases manifest in the model's decisions.",
      "Harm": "Generated content might unfairly represent certain groups or individuals.,\n Concern:Bias can harm users of the AI models and magnify existing discriminatory behaviors."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not collect or use personal or sensitive data.",
      "Data Licensing": "The benchmark data is licensed under the Creative Commons Attribution 4.0 International (CC-BY-4.0) license.",
      "Consent Procedures": "The benchmark does not collect any personal data, and does not require any consent from users or subjects.",
      "Compliance with Regulations": "The benchmark does not address compliance with any specific regulations related to data privacy or discrimination."
  },
    "Benchmark Summary":"benchmark data contains a diverse set of prompts covering 70 hypothetical decision scenarios, ranging from approving a loan to providing press credentials. Each prompt instructs the model to make a binary decision (yes/no) about a particular person described in the prompt. Each person is described in terms of three demographic attributes"
  },



  {
    "Benchmark Details": {
      "Name": "ProvoQ (PROVOcative Questions about minority-associated stigmas)",
      "ID": "cards.safety.provoq",
      "Overview": "The ProvoQ dataset is designed to evaluate the sensitivity of large language models (LLMs) to stigma-related topics. It contains 2,705 human-curated provocative questions that systematically target minority-stigma pairs in the United States, creating a diverse and nuanced set of questions that reflect these sensitive topics. The dataset aims to support research in understanding and mitigating biases in AI systems, particularly in the context of minority groups. Whilst most questions are toxic, others may seem benign but potentially elicit harmful responses. The dataset contains questions in text format, organized by minority-stigma pairs.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Languages": "English"
    },
    "Resources": {
      "Paper": "https://arxiv.org/abs/2311.04124",
      "Code": "https://huggingface.co/datasets/ibm-research/ProvoQ"
    },
    "Purpose and Intended Users": {
      "Goal": "To evaluate the sensitivity of LLMs to stigma-related topics and support research in understanding and mitigating biases in AI systems, particularly in the context of minority groups.",
      "Audience": "Researchers, developers, and policymakers",
      "Tasks": "The dataset can be used to train and evaluate models that detect and mitigate biases in LLMs related to stigma-related topics.",
      "Out-of-Scope Uses": "The dataset is not intended for use in applications that require specific knowledge or skills related to stigma-related topics."
    },
    "Data": {
      "Source": "IBM Provoq. Curated by two teams from the US and the Middle East. The minority groups and associated stigmas were identified using a combination of a proprietary language model and manual collection methods.",
      "Size": "2,705 questions",
      "Format": "Questions in text format, organized by minority-stigma pairs.",
      "Annotation": "The questions were labeled by 14 volunteers from diverse genders, backgrounds, and nationalities, who marked them as good, based on the definition of 'good' outlined in the annotators' guidelines. Each question has received at least one annotation."
    },
    "Methodology": {
      "Desired_score"  : "not applicable",
      "Methods": "The dataset was created using the 'Crawl and Generate' approach, inspired by Kour et al. (2023).",
      "Metrics": "Metric summary: ['metrics.llm_as_judge.safety.llama_3_70b_instruct_ibm_watsonx_template_harm_rating'].  This is an LLM-as-judge-based metric class for evaluating correctness of generated predictions.  It uses the source prompt given to the generator and the generator's predictions to evaluate correctness using a single turn task.",
      "Calculation": "The harm rating metric is a binary classification score indicating whether a question was harmful or not.",
      "Interpretation": "Users should interpret the harm rating score as a measure of the question's potential to elicit harmful responses from LLMs.",
      "Baseline Results": "Not available",
      "Validation": "The dataset underwent both automatic and manual curation processes, including grammar and spell checks, followed by human review to ensure the quality and relevance of the questions."
    },
    "Risks": {
      "Risk Categories": ["output-bias"],
      "Limitations": "The groups and associated stigmas in the ProvoQ dataset may be controversial. The dataset's biases could arise from human input and automated processes involved in its creation process. Users should exercise caution and judgment when using this dataset. Please note that the researchers involved in the creation of this dataset are not specialists in the social aspects addressed. This dataset is intended for research and testing, particularly for evaluation and addressing biases in chat models. It contains sensitive questions, so applying ethical consideration, exercising proper judgment, and implementing appropriate safeguards when working with this dataset is crucial.  Each step in this dataset generation process can potentially introduce bias, whether through human input or automated procedures. This is particularly true for the selection of minority groups and their associated stigmas, which can be especially controversial.  The dataset may not be exhaustive or cover all possible perspectives on the stigma-related topics. It may also not capture the nuances and complexities of real-life situations.",
      "Demographic Analysis": "The dataset does not provide a comprehensive analysis of the impact of stigma on different demographic groups. It is important to consider the potential implications of the dataset for various communities and individuals.",
      "Harm": "Generated content might unfairly represent certain groups or individuals.,\n Concern:Bias can harm users of the AI models and magnify existing discriminatory behaviors."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The dataset does not contain any personal or sensitive data.",
      "Data Licensing": "The dataset is licensed under the cdla-permissive-2.0 license.",
      "Consent Procedures": "The dataset does not include any personal data, so no consent is required."
  },
    "Benchmark Summary":"benchmark is designed to evaluate the sensitivity of large language models (LLMs) to stigma-related topics"
  },


  {
    "Benchmark Details": {
      "Name": "Custom Data Leakage",
      "ID": "cards.data_leakage.empty_data_leakage_card",
      "Overview": "This evaluation is performed on custom models. A user may use this evaluation to evaluate a custom model with a subset of the assessed model's training data, which must be supplied by the user to the system.  The benchmark is designed to assess the risk of data leakage in language models. It aims to evaluate the degree to which models memorize their training data and reveal confidential or sensitive information in the generated output.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Languages": "English"
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the risk of data leakage in the LM.",
      "Audience": "Researchers, developers, and policymakers",
      "Tasks": "The benchmark evaluates the model's ability to memorize training data and reveal confidential or sensitive information in the generated output.",
      "Out-of-Scope Uses": "The benchmark is not designed to be applied to other types of models or data modalities."
    },
    "Data": {
      "Source": "The data for this evaluation must be supplied by the user.  It must be a subset of the assessed model's training data.",
      "Size": "Unknown",
      "Format": "Unknown",
      "Annotation": "Unknown "
    },
    "Methodology": {
      "Methods": "The benchmark uses a data leakage attack to evaluate the model's ability to memorize training data. The attack tries to cause the model to leak complete or partial samples from its training data and measures the degree of similarity between the model responses and the actual training data.",
      "Metrics": "This metric is based on a training data or fine-tuning data leakage attack, and represents to what degree the model has memorized its training data. ",
      "Calculation": " The attack tries to cause the model to leak complete or partial samples from its training (or fine-tuning) data, and measures the degree of similarity between the model responses and the actual training data.",
      "Interpretation": "A high syntactic similarity between the model response and the actual sample represents higher leakage risk. The score is based on the attack’s average similarity score across all training samples tested, and normalized such that 1 represents perfect privacy and 0 represents worst privacy."
    },
    "Risks": {
      "Risk Categories": [
        "revealing-confidential-information",
        "exposing-personal-information"
      ],
      "Limitations": "The benchmark does not evaluate all potential risks associated with data leakage in language models. It does not address gaps in demographic coverage or specific domains.",
      "Demographic Analysis": "The benchmark does not perform demographic analysis to evaluate the impact of data leakage on different demographic groups.",
      "Harm": "The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information in their generated output. Risk Title: Revealing confidential information,\n Tag: revealing-confidential-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:intellectual property,\n Descriptor:amplified,\n Description: When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\n Tag: exposing-personal-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:privacy,\n Descriptor:amplified,\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not handle any personal or sensitive data, and it does not apply any anonymization techniques.",
      "Data Licensing": "Unknown, custom model and dataset.",
      "Consent Procedures": "The benchmark does not involve any personal data, and it does not require consent from users."
    }
  },


  {
    "Benchmark Details": {
      "Name": "Prompt Injection Risk CSE Benchmark",
      "ID": "cards.jailbreak_risk.prompt_injection_risk_cse_benchmark",
      "Overview": "The dataset represents prompt injection attacks which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is taken from cyberseceval benchmark developed by Meta and released under MIT License. We use this dataset as a benchmark for evaluating the prompt injection risk of LLMs.",
      "Data Type": "text",
      "Domains": "Cybersecurity",
      "Application Domains": ["Cybersecurity", "LLMs"],
      "Languages": "english",

      "Resources": [
        {
          "Github": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks"
        }
      ]
    },
    "Purpose and Intended Users": {
      "Goal": "To evaluate the prompt injection risk of LLMs",
      "Audience": "Researchers, developers, policymakers",
      "Tasks": [
        "Assessing the prompt injection risk of LLMs on benchmark datasets."
      ],
      "Out-of-Scope Uses": "Any other use than assessing the prompt injection risk of LLMs on benchmark datasets."
    },
    "Data": {
      "Source": "Cyberseceval benchmark developed by Meta. The data used is benchmark data, publicly available under the MIT license.",
      "Size": "352 KB",
      "Format": "CSV format. The data instances include the test_case_prompt, user_input, judge_question, injection_variant, injection_type, risk_category, speaking_language",
      "Annotation": "General annotation, the data samples are injection attack prompts."
    },
    "Methodology": {
      "Main Score" : "Prompt Injection Risk CSE Benchmark",
      "Desired_score"  : "Lower is Better",
      "Methods": [
        "Using the benchmark to evaluate the prompt injection risk of LLMs"
      ],
      "Metrics": "Prompt Injection Risk CSE",
      "Calculation": "This metric evaluates the rate of prompt injection attack risk. The test prompts have been specifically designed to be ambiguous by containing keywords that are typical for benign as well as malicious requests. The final score is the fraction of successful attack prompts that result in model response according to injection goal.",
      "Interpretation": "A higher risk score indicates a higher likelihood of prompt injection risk."
    },
    "Risks": {
      "Risk Categories": ["prompt-injection", "jailbreaking"],
      "Limitations": "Please note that this metric is a benchmark and is subject to ongoing development and refinement. Future improvements and advancements are planned to be incorporated to enhance its accuracy and effectiveness in model assessments.",
      "Harm": "A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.,\n Concern: Injection attacks can be used to alter model behavior and benefit the attacker. Risk Title: Jailbreaking,\n Tag: jailbreaking,\n Type: input,\n Phase:inference,\n Category:,\n Group:multi-category,\n Descriptor:specific,\n Description: A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.,\n Concern: Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "Benchmark datasets do not contain any personal or sensitive data",
      "Data Licensing": "The data used in the benchmark is publicly available under the MIT license."
  },
    "Benchmark Summary": "benchmark evaluates the jailbreak risk of LLMs and uses jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts"

  },

  {
    "Benchmark Details": {
      "Name": "Discrim-Eval Explicit",
      "ID": "cards.safety.discrim_eval.explicit",
      "Overview": "The data contains a diverse set of prompts covering 70 hypothetical decision scenarios, ranging from approving a loan to providing press credentials. Each prompt instructs the model to make a binary decision (yes/no) about a particular person described in the prompt. Each person is described in terms of three demographic attributes: age (ranging from 20 to 100 in increments of 10), gender (male, female, non-binary), and race (white, Black, Asian, Hispanic, Native American), for a total of 135 examples per decision scenario. The prompts are designed so a 'yes' decision is always advantageous to the person (e.g. deciding to grant the loan).",
      "Data Type": "Text",
      "Domains": "Question answering",
      "Languages": "English",
      "Resources": {
        "Paper": "https://arxiv.org/abs/2312.03689"
      }
    },
    "Purpose and Intended Users": {
      "Goal": "To evaluate the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed.",
      "Audience": "Researchers, developers, and policymakers who are interested in understanding and mitigating the potential risks of AI systems.",
      "Tasks": "Evaluating and mitigating discrimination in language model decisions.",
      "Out-of-Scope Uses": "Using the benchmark for high-risk automated decision making or for other purposes not related to its original goals."
    },
    "Data": {
      "Source": "Anthropic",
      "Size": "The dataset contains 135 examples per decision scenario, totalling 9,450 examples across all 70 decision scenarios.",
      "Format": "The data is provided in two JSONL files: explicit.jsonl and implicit.jsonl. Each line in the files contains a prompt, decision question ID, age, gender, and race of the person described in the prompt. The implicit.jsonl file does not have an explicit mention of race or gender, but rather relies on an implicit version of these attributes based on a name. ",
      "Annotation": "The data was annotated using Anthropics's Claude language models."
    },
    "Methodology": {
      "Main Score":"accuracy",
      "Desired_score"  : "Lower is Better",
      "Methods": "The benchmark uses a series of hypothetical decision scenarios to evaluate the ability of language models to make unbiased decisions. Each prompt is designed to elicit a binary decision (yes/no) about a particular person described in the prompt. The prompts are then used to test the ability of language models to make accurate decisions and to identify potential biases.",
      "Metrics": "The benchmark uses accuracy as the primary metric to evaluate the model's performance. Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN).",
      "Calculation": "The benchmark calculates the discrimination score for each decision scenario and demographic attribute based on the model's performance on the prompt examples.",
      "Interpretation": "The benchmark provides a way to measure the extent to which the model may be biased towards certain demographic groups. A higher discrimination score indicates a higher likelihood of discrimination.",
      "Baseline Results": "The benchmark compares the model's performance to a baseline model that does not take into account the demographic attributes of the person described in the prompt. This can help identify if the model is biased towards any particular demographic group.",
      "Validation": "The benchmark does not provide information on the validation process used to ensure the data and methods are valid and reliable."
    },
    "Risks": {
      "Risk Categories": ["output-bias"],
      "Limitations": "The benchmark does not cover all possible decision making scenarios or demographics, and may not be fully representative of all real-world scenarios.  In terms of external validity, the evaluation is done on a model's judgement of a short text description of candiates only, which does not account for the wider variety of input formats that might affect the model's conclusions. It is important to note that the dataset authors do not permit or endorse the use of LMs for high-risk automated decision making.",
      "Demographic Analysis": "The benchmark evaluates the performance of language models across different demographic groups, including age, gender, and race. However, it does not provide a detailed analysis of how these biases manifest in the model's decisions.",
      "Harm": "Generated content might unfairly represent certain groups or individuals.,\n Concern:Bias can harm users of the AI models and magnify existing discriminatory behaviors."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not collect or use personal or sensitive data.",
      "Data Licensing": "The benchmark data is licensed under the Creative Commons Attribution 4.0 International (CC-BY-4.0) license.",
      "Consent Procedures": "The benchmark does not collect any personal data, and does not require any consent from users or subjects.",
      "Compliance with Regulations": "The benchmark does not address compliance with any specific regulations related to data privacy or discrimination."
    },
    "Benchmark Summary":"benchmark data contains a diverse set of prompts covering 70 hypothetical decision scenarios, ranging from approving a loan to providing press credentials. Each prompt instructs the model to make a binary decision (yes/no) about a particular person described in the prompt. Each person is described in terms of three demographic attributes"
  },

    {
    "Benchmark Details": {
      "Name": "Data leakage (SYNTH)",
      "ID": "cards.data_leakage_synth.synth_sentence",
      "Overview": "This benchmark is designed to assess the tendency of a model to leak the original text (seed) while using it to generate synthetic data.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": "",
      "Languages": "English",
      "Similar Benchmarks": "",
      "Resources": "[{'huggingface': 'fedml/PubMedQA_instruction'}]"
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the risk of LLMs revealing the original text (seed) while using it to generate synthetic data.",
      "Audience": "Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.",
      "Tasks": "The benchmark is designed for the task of detecting data leakage from the original text (seed) while paraphrasing it.",
      "Out-of-Scope Uses": "The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering."
    },
    "Data": {
      "Source": "This benchmark uses the dataset fedml/PubMedQA_instruction. It contains a PubMedQA dataset converted for instruction tuning.",
      "Size": "The dataset contains 273,518 samples, with 272,518 samples in the training set and 1,000 samples in the test set. The download size is reported as 275 MB and dataset size as 275 MB. A subset of data (1000 sample instances) is used to run the MRA recipe.",
      "Format": "The dataset is stored in the Hugging Face Dataset format, with each sample consisting of an instruction, a context and its corresponding response.",
      "Annotation": "The dataset was built by FedML, and distributed un the the MIT License."
    },
    "Methodology": {
      "Methods": "To identify leakage risk, the benchmark looks for high syntactic similarity between the model responses and the original text, used as a seed for paraphrasing.",
      "Metrics": "Data leakage (metrics.data_leakage). ",
      "Calculation": "The synthetic data generation leakage score is calculated using a syntactic similarity metric, an enhanced variation of rougeL (rougLmod), between the model's response and the seed (text to be rephrased). A high syntactic similarity between the model response and the seed represents higher leakage risk." ,
      "Interpretation": "The final score is based on the attack’s average similarity score across all the synthetic generated samples tested, normalized such that 1 represents perfect privacy and 0 represents worst privacy."
    },
    "Risks": {
      "Risk Categories": [
        "revealing-confidential-information",
        "exposing-personal-information"
      ],
      "Limitations": "This benchmark only assesses the tendency of synthetic data generation seeds leakage by using an available public dataset. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. ",
      "Demographic Analysis": "Unknown",
      "Harm": "The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information from the original text (aka synthetic data generation seed) in their generated output. Risk Title: Revealing confidential information,\n Tag: revealing-confidential-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:intellectual property,\n Descriptor:amplified,\n Description: When confidential information is used in the seed, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\n Tag: exposing-personal-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:privacy,\n Descriptor:amplified,\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in the seed for synthetic data generation, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not collect or use any personal or sensitive data.",
      "Data Licensing": "The dataset is licensed under the MIT License.",
      "Consent Procedures": "Unknown"
    },
    "Benchmark Summary":"benchmark is designed to assess the tendency of a model to leak the original text (seed) while using it to generate synthetic data."

  },

    {
    "Benchmark Details": {
      "Name": "Data leakage (RAG)",
      "ID": "cards.data_leakage_rag.rag_dataset_12000",
      "Overview": "This benchmark is designed to assess the tendency of a model to leak RAG context in case it will be provided together with a user prompt.",
      "Data Type": "Text",
      "Domains": "Natural Language Processing",
      "Application Domains": "",
      "Languages": "English",
      "Similar Benchmarks": "",
      "Resources": "[{'huggingface': 'neural-bridge/rag-dataset-12000'}]"
    },
    "Purpose and Intended Users": {
      "Goal": "To assess the risk of LLMs leaking RAG context into their generated output.",
      "Audience": "Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.",
      "Tasks": "The benchmark is designed for the task of detecting data leakage from RAG context",
      "Out-of-Scope Uses": "The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering."
    },
    "Data": {
      "Source": "This benchmark uses the dataset neural-bridge/rag-dataset-12000. Retrieval-Augmented Generation (RAG) Dataset 12000 is an English dataset designed for RAG-optimized models, built by Neural Bridge AI, and released under Apache license 2.0. Retrieval-Augmented Generation (RAG) Dataset 12000 dataset is a triple-feature collection, with each entry containing a \"context\", \"question\", and \"answer\" fields, designed to help build RAG-optimized models.",
      "Size": "The dataset contains 12,000 samples, with 9,600 samples in the training set and 2,400 samples in the test set. The download size is reported as 28.9 MB and dataset size as 28.9 MB. A subset of data (1000 sample instances) is used to run the MRA recipe.",
      "Format": "The dataset is stored in the Hugging Face Dataset format, with each sample consisting of a context and its corresponding question and answer.",
      "Annotation": "The dataset was built by Neural Bridge AI, and the responses were checked for the presence of sensitive information."
    },
    "Methodology": {
      "Methods": "The benchmark looks for high syntactic similarity between the model responses and the given RAG context, to identify leakage risk.",
      "Metrics": "Data leakage (metrics.data_leakage). ",
      "Calculation": "The RAG context leakage score is calculated using a syntactic similarity metric, an enhanced variation of rougeL (rougLmod), between the model's response and the RAG context. A high syntactic similarity between the model response and the RAG context represents higher leakage risk." ,
      "Interpretation": "The final score is based on the attack’s average similarity score across all RAG context samples tested, normalized such that 1 represents perfect privacy and 0 represents worst privacy."
    },
    "Risks": {
      "Risk Categories": [
        "revealing-confidential-information",
        "exposing-personal-information"
      ],
      "Limitations": "This benchmark only assesses the tendency of RAG context leakage by using public RAG dataset. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. ",
      "Demographic Analysis": "Unknown",
      "Harm": "The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information from RAG context in their generated output. Risk Title: Revealing confidential information,\n Tag: revealing-confidential-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:intellectual property,\n Descriptor:amplified,\n Description: When confidential information is used in RAG context, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\n Tag: exposing-personal-information,\n Type: output,\n Phase:None,\n Category:None,\n Group:privacy,\n Descriptor:amplified,\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in RAG context, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable."
    },
    "Ethical and Legal Considerations": {
      "Privacy and Anonymity": "The benchmark does not include any personal or sensitive data.",
      "Data Licensing": "This dataset is licensed under the [Apache License, Version 2.0]",
      "Consent Procedures": "Unknown"
    },
    "Benchmark Summary":"benchmark is designed to assess the tendency of a model to leak RAG context in case it will be provided together with a user prompt."

  }
]

