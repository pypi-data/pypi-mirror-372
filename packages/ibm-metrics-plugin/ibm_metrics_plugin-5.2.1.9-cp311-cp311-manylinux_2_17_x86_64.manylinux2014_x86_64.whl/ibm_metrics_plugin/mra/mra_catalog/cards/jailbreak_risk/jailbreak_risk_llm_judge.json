{
    "__type__": "task_card",
    "__description__": "The collection of datasets represent jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. These datasets are based on a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. We use these datasets as a benchmark for evaluating the jailbreak risk of LLMs.",
    "__tags__": {
        "urls": {
            "arxiv": "https://github.com/IBM/Adversarial-Prompt-Evaluation"
        },
        "languages": [
            "english"
        ],
        "risks": [
            "prompt-injection",
            "jailbreaking"
        ],
        "authors": "zaid.hameed@ibm.com",
        "version": "1.0.0"
    },
    "loader": {
        "__type__": "multiple_source_loader",
        "sources": [
            {
                "__type__": "load_csv",
                "files": {
                    "test": "data/prompt_injection/aart/aart.csv"
                },
                "data_classification_policy": [
                    "public"
                ]
            },
            {
                "__type__": "load_csv",
                "files": {
                    "test": "data/prompt_injection/attaq/attaq.csv"
                },
                "data_classification_policy": [
                    "public"
                ]
            },
            {
                "__type__": "load_csv",
                "files": {
                    "test": "data/prompt_injection/gandalf_ignore_instructions/gandalf_ignore_instructions.csv"
                },
                "data_classification_policy": [
                    "public"
                ]
            },
            {
                "__type__": "load_csv",
                "files": {
                    "test": "data/prompt_injection/harmful_behaviours/harmful_behaviours.csv"
                },
                "data_classification_policy": [
                    "public"
                ]
            },
            {
                "__type__": "load_csv",
                "files": {
                    "test": "data/prompt_injection/sap/sap.csv"
                },
                "data_classification_policy": [
                    "public"
                ]
            },
            {
                "__type__": "load_csv",
                "files": {
                    "test": "data/prompt_injection/ibm_generated_samples/ibm_generated_samples.csv"
                },
                "data_classification_policy": [
                    "proprietary"
                ]
            },
            {
                "__type__": "load_from_dictionary",
                "data": {
                    "train": [
                        {
                            "dummy": "dummy"
                        }
                    ]
                }
            },
            {
                "__type__": "load_from_dictionary",
                "data": {
                    "validation": [
                        {
                            "dummy": "dummy"
                        }
                    ]
                }
            }
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "delete_splits",
            "splits": [
                "validation",
                "train"
            ]
        },
        {
            "__type__": "rename",
            "field_to_field": {
                "prompt": "question"
            }
        },
        {
            "__type__": "set",
            "fields": {
                "answer": " "
            }
        }
    ],
    "task": {
        "__type__": "task",
        "inputs": {
            "question": "str"
        },
        "outputs": {
            "answer": "str"
        },
        "prediction_type": "str",
        "metrics": [
            "metrics.jailbreak_risk_llm_judge"
        ]
    },
    "templates": {
        "__type__": "templates_list",
        "items": [
            {
                "__type__": "input_output_template",
                "input_format": "{question}",
                "output_format": "{answer}",
                "postprocessors": [
                    "processors.lower_case"
                ]
            }
        ]
    }
}
