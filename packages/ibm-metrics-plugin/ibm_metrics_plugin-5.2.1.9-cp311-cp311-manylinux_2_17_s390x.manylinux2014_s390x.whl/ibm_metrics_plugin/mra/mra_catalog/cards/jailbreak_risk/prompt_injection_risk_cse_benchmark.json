{
    "__type__": "task_card",
    "__description__": "The dataset represents prompt injection attacks which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is taken from cyberseceval benchmark developed by Meta and released under MIT License. We use this dataset as a benchmark for evaluating the prompt injection risk of LLMs.",
    "__tags__": {
        "urls": {
            "code": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks"
        },
        "languages": [
            "english"
        ],
        "risks": [
            "prompt-injection",
            "jailbreaking"
        ],
        "authors": "zaid.hameed@ibm.com",
        "version": "1.0.0"
    },
    "loader": {
        "__type__": "load_csv",
        "files": {
            "test": "data/prompt_injection/cyberseceval/prompt_injection.csv"
        },
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "rename",
            "field_to_field": {
                "test_case_prompt": "target_instruction",
                "user_input": "user_question"
            }
        },
        {
            "__type__": "set",
            "fields": {
                "answer": " "
            }
        }
    ],
    "task": {
        "__type__": "task",
        "inputs": {
            "target_instruction": "str",
            "user_question": "str",
            "judge_question": "str",
            "injection_variant": "str",
            "injection_type": "str",
            "risk_category": "str",
            "speaking_language": "str"
        },
        "outputs": {
            "answer": "str"
        },
        "prediction_type": "str",
        "metrics": [
            "metrics.prompt_injection_risk_cse_benchmark"
        ]
    },
    "templates": {
        "__type__": "templates_list",
        "items": [
            {
                "__type__": "input_output_template",
                "instruction": "{target_instruction}",
                "input_format": "{user_question}",
                "output_format": "{answer}",
                "postprocessors": [
                    "processors.lower_case"
                ]
            }
        ]
    }
}
