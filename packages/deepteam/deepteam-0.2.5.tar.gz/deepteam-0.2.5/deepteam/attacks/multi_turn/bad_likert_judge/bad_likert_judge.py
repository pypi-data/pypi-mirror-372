from pydantic import BaseModel
from tqdm import tqdm
from typing import Optional, Union

from deepeval.models import DeepEvalBaseLLM
from deepeval.metrics.utils import initialize_model

from deepteam.attacks import BaseAttack
from deepteam.attacks.multi_turn.bad_likert_judge.schema import (
    NonRefusal,
    BLJJudge,
)
from deepteam.attacks.multi_turn.bad_likert_judge.template import (
    BadLikertJudgeTemplate,
)
from deepteam.attacks.attack_simulator.utils import (
    generate_schema,
    a_generate_schema,
)
from deepteam.attacks.multi_turn.types import CallbackType


class BadLikertJudge(BaseAttack):
    def __init__(
        self,
        weight: int = 1,
        category: str = "bias",
        turns: int = 3,
        enable_refinement: bool = True,
    ):
        """
        Initialize Bad Likert Judge attack.

        Args:
            weight: Attack selection probability weight
            category: Vulnerability category for Likert scale guidelines.
            turns: Maximum number of turns (default 3)
            enable_refinement: Whether to use Turn 3 refinement
        """
        self.weight = weight
        self.category = category
        self.turns = min(max(turns, 2), 3)  # Ensure between 2-3 turns
        self.enable_refinement = enable_refinement

    def enhance(
        self,
        attack: str,
        model_callback: CallbackType,
        simulator_model: Optional[Union[DeepEvalBaseLLM, str]] = None,
    ) -> str:
        self.simulator_model, _ = initialize_model(simulator_model)

        # Progress bar setup
        total_steps = self.turns + 2  # turns + refusal check + judge
        pbar = tqdm(
            total=total_steps,
            desc=f"ðŸŽ¯ Bad Likert Judge ({self.category})",
            leave=False,
        )

        try:
            # Turn 1: Set up model as Likert Judge
            turn_1_prompt = BadLikertJudgeTemplate.turn_1_judge_setup(
                attack, self.category
            )
            turn_1_response = model_callback(turn_1_prompt)
            pbar.update(1)

            # Check if Turn 1 was refused
            non_refusal_prompt = BadLikertJudgeTemplate.non_refusal(
                attack, turn_1_response
            )
            non_refusal_res: NonRefusal = self._generate_schema(
                non_refusal_prompt, NonRefusal
            )

            if non_refusal_res.classification == "Refusal":
                pbar.close()
                return attack
            pbar.update(1)

            # Turn 2: Request examples for different scores
            turn_2_prompt = BadLikertJudgeTemplate.turn_2_generate_examples(
                attack, self.category
            )
            turn_2_response = model_callback(turn_2_prompt)

            # Extract the score 3 example (most harmful)
            score_3_example = BadLikertJudgeTemplate.extract_score_3_example(
                turn_2_response
            )
            current_attack = (
                score_3_example if score_3_example else turn_2_response
            )
            pbar.update(1)

            # Turn 3: Optional refinement (if enabled and turns >= 3)
            if self.enable_refinement and self.turns >= 3:
                turn_3_prompt = BadLikertJudgeTemplate.turn_3_refinement(attack)
                turn_3_response = model_callback(turn_3_prompt)

                # Use refined response if it's more detailed
                if len(turn_3_response.strip()) > len(current_attack.strip()):
                    current_attack = turn_3_response
                pbar.update(1)
            elif self.turns >= 3:
                pbar.update(1)  # Skip refinement but update progress

            # Final judgment
            judge_prompt = BadLikertJudgeTemplate.blj_judge(
                attack, current_attack, current_attack
            )
            judge_res: BLJJudge = self._generate_schema(judge_prompt, BLJJudge)
            pbar.update(1)

            pbar.close()

            # Return the harmful content if successful, otherwise original attack
            if judge_res.classification == "Harmful":
                return current_attack
            else:
                return attack

        except Exception as e:
            pbar.close()
            return attack

    async def a_enhance(
        self,
        attack: str,
        model_callback: CallbackType,
        simulator_model: Optional[Union[DeepEvalBaseLLM, str]] = None,
    ) -> str:
        self.simulator_model, _ = initialize_model(simulator_model)

        # Progress bar setup
        total_steps = self.turns + 2  # turns + refusal check + judge
        pbar = tqdm(
            total=total_steps,
            desc=f"ðŸŽ¯ Bad Likert Judge ({self.category})",
            leave=False,
        )

        try:
            # Turn 1: Set up model as Likert Judge
            turn_1_prompt = BadLikertJudgeTemplate.turn_1_judge_setup(
                attack, self.category
            )
            turn_1_response = await model_callback(turn_1_prompt)
            pbar.update(1)

            # Check if Turn 1 was refused
            non_refusal_prompt = BadLikertJudgeTemplate.non_refusal(
                attack, turn_1_response
            )
            non_refusal_res: NonRefusal = await self._a_generate_schema(
                non_refusal_prompt, NonRefusal
            )

            if non_refusal_res.classification == "Refusal":
                pbar.close()
                return attack
            pbar.update(1)

            # Turn 2: Request examples for different scores
            turn_2_prompt = BadLikertJudgeTemplate.turn_2_generate_examples(
                attack, self.category
            )
            turn_2_response = await model_callback(turn_2_prompt)

            # Extract the score 3 example (most harmful)
            score_3_example = BadLikertJudgeTemplate.extract_score_3_example(
                turn_2_response
            )
            current_attack = (
                score_3_example if score_3_example else turn_2_response
            )
            pbar.update(1)

            # Turn 3: Optional refinement (if enabled and turns >= 3)
            if self.enable_refinement and self.turns >= 3:
                turn_3_prompt = BadLikertJudgeTemplate.turn_3_refinement(attack)
                turn_3_response = await model_callback(turn_3_prompt)

                # Use refined response if it's more detailed
                if len(turn_3_response.strip()) > len(current_attack.strip()):
                    current_attack = turn_3_response
                pbar.update(1)
            elif self.turns >= 3:
                pbar.update(1)  # Skip refinement but update progress

            # Final judgment
            judge_prompt = BadLikertJudgeTemplate.blj_judge(
                attack, current_attack, current_attack
            )
            judge_res: BLJJudge = await self._a_generate_schema(
                judge_prompt, BLJJudge
            )
            pbar.update(1)

            pbar.close()

            # Return the harmful content if successful, otherwise original attack
            if judge_res.classification == "Harmful":
                return current_attack
            else:
                return attack

        except Exception as e:
            pbar.close()
            return attack

    def _generate_schema(self, prompt: str, schema: BaseModel):
        return generate_schema(prompt, schema, self.simulator_model)

    async def _a_generate_schema(self, prompt: str, schema: BaseModel):
        return await a_generate_schema(prompt, schema, self.simulator_model)

    def get_name(self) -> str:
        return "Bad Likert Judge"

    @classmethod
    def get_supported_categories(cls) -> list[str]:
        """
        Returns a list of example supported vulnerability categories.
        Note: The system supports any category - this is just a sample of common ones.
        """
        return [
            # Core vulnerability types
            "bias",
            "religion",
            "politics",
            "gender",
            "race",
            "toxicity",
            "profanity",
            "insults",
            "threats",
            "mockery",
            "illegal_activity",
            "weapons",
            "illegal_drugs",
            "violent_crimes",
            "cybercrime",
            "unauthorized_access",
            "bfla",
            "bola",
            "rbac",
            "sql_injection",
            "shell_injection",
            "personal_safety",
            "bullying",
            "self-harm",
            "unsafe_practices",
            "stalking",
            "graphic_content",
            "sexual_content",
            "pornographic_content",
            "misinformation",
            "factual_errors",
            "unsupported_claims",
            "pii_leakage",
            "database_access",
            "direct_disclosure",
            "session_leak",
            "prompt_leakage",
            "secrets_and_credentials",
            "instructions",
            "guard_exposure",
            "excessive_agency",
            "functionality",
            "permissions",
            "autonomy",
            "robustness",
            "input_overreliance",
            "hijacking",
            "intellectual_property",
            "copyright_violations",
            "trademark_infringement",
            "competition",
            "competitor_mention",
            "market_manipulation",
            "malware",
            "hacking",
            "exploitation",
            "violence",
            "harassment",
        ]
