{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670c99d6",
   "metadata": {},
   "source": [
    "# Load toy dataset\n",
    "For this tutorial we will use the breast cancer dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X = load_breast_cancer(as_frame=True).frame\n",
    "X_train, X_test = train_test_split(\n",
    "    X, test_size=0.3, stratify=X[\"target\"], random_state=seed\n",
    ")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c27ef4",
   "metadata": {},
   "source": [
    "# Generate synthetic data\n",
    "Simply import a generator, train it using the fit method, and sample a synthetic dataset using the generate method.\n",
    "\n",
    "For tabular data, the fit method requires the column names of discrete features as well. Discrete data typically needs to be handled differently than numerical data.\n",
    "\n",
    "All preprocessing and postprocessing is performed under the hood: no need to scale, encode, or handle missingness. \n",
    "\n",
    "The output synthetic dataframe contains the same columns of the same numerical precision and data types as the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5072ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthyverse.generators import ARFGenerator\n",
    "\n",
    "generator = ARFGenerator(random_state=seed)\n",
    "generator.fit(X_train, discrete_features=[\"target\"])\n",
    "\n",
    "syn = generator.generate((len(X)))\n",
    "syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06e2cc",
   "metadata": {},
   "source": [
    "# Evaluate synthetic data\n",
    "The evaluation module contains the MetricEvaluator object, which can be used to evaluate the quality of synthetic data.\n",
    "\n",
    "Simply pass your desired metrics (see the docs for all included metrics) and some additional info on your dataset. \n",
    "\n",
    "Then calculate the metrics using the evaluate method. Pass both the original data that was used to train the generator and an independent holdout/test set of real data. Many metrics require an independent holdout set to evaluate generalization capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc502e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthyverse.evaluation import MetricEvaluator\n",
    "\n",
    "evaluator = MetricEvaluator(\n",
    "    metrics=[\"mle\", \"dcr\", \"similarity\"],\n",
    "    discrete_features=[\"target\"],\n",
    "    random_state=seed,\n",
    "    target_column=\"target\",\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate(X_train, X_test, syn)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249cb70",
   "metadata": {},
   "source": [
    "# Evaluate synthetic data: continued\n",
    "We allow fine control over metric parameters by passing them as dictionaries. See the docs for all metric parameters.\n",
    "\n",
    "Also, you can calculate various configurations of the same metric by adding a dash to the name {metric_name}-{configuration} and then using different parameters as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87850ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MetricEvaluator(\n",
    "    metrics={\n",
    "        \"mle-trts\": {\"train_set\": \"real\"},\n",
    "        \"mle-tstr\": {\"train_set\": \"synthetic\"},\n",
    "        \"dcr\": {\"estimates\": [\"mean\", 0.01, 0.05]},\n",
    "    },\n",
    "    discrete_features=[\"target\"],\n",
    "    random_state=seed,\n",
    "    target_column=\"target\",\n",
    ")\n",
    "results = evaluator.evaluate(X_train, X_test, syn)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0379da",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "To simplify the pipeline of synthetic data generation and evaluation, we also include a benchmarking module. \n",
    "\n",
    "This module abstracts a pipeline of synthetic data generation and evaluation in a few lines of code. \n",
    "\n",
    "Input the name of the generator, its parameters, the number of random train-test splits to fit the generator to, number of random initializations to fit the generator to, the number of synthetic sets to sample and evaluate for each fitted generator, the metrics to evaluate, and the size of the test set.\n",
    "\n",
    "Then run the benchmark on a dataframe (including some information on which columns are discrete, and which is the target column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19158272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthyverse.benchmark import TabularBenchmark\n",
    "\n",
    "benchmark = TabularBenchmark(\n",
    "    generator_name=\"arf\",\n",
    "    generator_params={\"random_state\": seed, \"num_trees\": 20},\n",
    "    n_random_splits=1,\n",
    "    n_inits=1,\n",
    "    n_generated_datasets=1,\n",
    "    metrics=[\"classifier_test\", \"mle\", \"dcr\"],\n",
    "    test_size=0.3,\n",
    ")\n",
    "results = benchmark.run(X, target_column=\"target\", discrete_columns=[\"target\"])\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
