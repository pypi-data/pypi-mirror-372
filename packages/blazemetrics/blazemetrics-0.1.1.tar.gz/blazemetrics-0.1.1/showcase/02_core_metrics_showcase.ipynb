{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä BlazeMetrics Core Metrics Showcase\n",
    "\n",
    "This notebook demonstrates the core NLP evaluation metrics provided by BlazeMetrics. All metrics are implemented in Rust for blazing-fast performance and can be computed in parallel across CPU cores.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "- **ROUGE Scores**: ROUGE-1, ROUGE-2, ROUGE-L for text summarization evaluation\n",
    "- **BLEU Score**: Bilingual Evaluation Understudy for machine translation\n",
    "- **chrF Score**: Character n-gram F-score for multilingual evaluation\n",
    "- **METEOR**: Metric for Evaluation of Translation with Explicit ORdering\n",
    "- **WER**: Word Error Rate for speech recognition evaluation\n",
    "- **Token-level Metrics**: F1, Jaccard similarity for fine-grained analysis\n",
    "- **BERTScore**: Semantic similarity using contextual embeddings\n",
    "- **MoverScore**: Word mover distance for semantic evaluation\n",
    "\n",
    "## üöÄ Performance Features\n",
    "\n",
    "- **Rust Core**: Native performance without Python GIL limitations\n",
    "- **Parallel Processing**: Automatic parallelization with Rayon\n",
    "- **NumPy Integration**: Efficient handling of embeddings and numerical data\n",
    "- **Batch Processing**: Optimized for large-scale evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import BlazeMetrics functions\n",
    "from blazemetrics import (\n",
    "    rouge_score, bleu, chrf_score, meteor, wer,\n",
    "    token_f1, jaccard, bert_score_similarity, moverscore_greedy,\n",
    "    compute_text_metrics, aggregate_samples\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Sample Data Preparation\n",
    "\n",
    "Let's create realistic sample data for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for evaluation\n",
    "candidates = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A good book is a great friend that never betrays\",\n",
    "    \"Machine learning algorithms can process vast amounts of data\",\n",
    "    \"The weather today is sunny and warm\",\n",
    "    \"Artificial intelligence transforms how we solve problems\",\n",
    "    \"Natural language processing enables computers to understand text\",\n",
    "    \"Deep learning models require significant computational resources\",\n",
    "    \"The internet connects billions of people worldwide\",\n",
    "    \"Climate change affects global ecosystems and human societies\",\n",
    "    \"Quantum computing promises revolutionary advances in cryptography\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\"A quick brown fox leaps over the lazy dog\"],\n",
    "    [\"A good book is a true friend that never disappoints\"],\n",
    "    [\"Machine learning algorithms process large amounts of data\"],\n",
    "    [\"The weather today is sunny and pleasant\"],\n",
    "    [\"AI transforms how we approach problem-solving\"],\n",
    "    [\"NLP allows computers to comprehend human language\"],\n",
    "    [\"Deep learning models need substantial computational power\"],\n",
    "    [\"The internet links billions of people globally\"],\n",
    "    [\"Climate change impacts ecosystems and human communities\"],\n",
    "    [\"Quantum computing offers breakthrough advances in security\"]\n",
    "]\n",
    "\n",
    "print(f\"üìä Sample Data Created:\")\n",
    "print(f\"  ‚Ä¢ Candidates: {len(candidates)} texts\")\n",
    "print(f\"  ‚Ä¢ References: {len(references)} reference sets\")\n",
    "print(f\"  ‚Ä¢ Average candidate length: {np.mean([len(c.split()) for c in candidates]):.1f} words\")\n",
    "print(f\"  ‚Ä¢ Average reference length: {np.mean([len(r[0].split()) for r in references]):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• ROUGE Scores\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is widely used for evaluating text summarization and generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing ROUGE Scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ROUGE-1 (Unigram overlap)\n",
    "start_time = time.perf_counter()\n",
    "rouge1_scores = rouge_score(candidates, references, score_type=\"rouge_n\", n=1)\n",
    "rouge1_time = time.perf_counter() - start_time\n",
    "\n",
    "# ROUGE-2 (Bigram overlap)\n",
    "start_time = time.perf_counter()\n",
    "rouge2_scores = rouge_score(candidates, references, score_type=\"rouge_n\", n=2)\n",
    "rouge2_time = time.perf_counter() - start_time\n",
    "\n",
    "# ROUGE-L (Longest Common Subsequence)\n",
    "start_time = time.perf_counter()\n",
    "rougeL_scores = rouge_score(candidates, references, score_type=\"rouge_l\")\n",
    "rougeL_time = time.perf_counter() - start_time\n",
    "\n",
    "# Display results\n",
    "print(f\"ROUGE-1 Scores (took {rouge1_time*1000:.2f} ms):\")\n",
    "for i, (p, r, f1) in enumerate(rouge1_scores):\n",
    "    print(f\"  Text {i+1}: P={p:.3f}, R={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(f\"\\nROUGE-2 Scores (took {rouge2_time*1000:.2f} ms):\")\n",
    "for i, (p, r, f1) in enumerate(rouge2_scores):\n",
    "    print(f\"  Text {i+1}: P={p:.3f}, R={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(f\"\\nROUGE-L Scores (took {rougeL_time*1000:.2f} ms):\")\n",
    "for i, (p, r, f1) in enumerate(rougeL_scores):\n",
    "    print(f\"  Text {i+1}: P={p:.3f}, R={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\n‚ö° Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ ROUGE-1: {rouge1_time*1000:.2f} ms\")\n",
    "print(f\"  ‚Ä¢ ROUGE-2: {rouge2_time*1000:.2f} ms\")\n",
    "print(f\"  ‚Ä¢ ROUGE-L: {rougeL_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ BLEU Score\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is the standard metric for machine translation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing BLEU Scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# BLEU with different n-gram orders\n",
    "start_time = time.perf_counter()\n",
    "bleu_scores = bleu(candidates, references, max_n=4)\n",
    "bleu_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"BLEU Scores (took {bleu_time*1000:.2f} ms):\")\n",
    "for i, score in enumerate(bleu_scores):\n",
    "    print(f\"  Text {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä BLEU Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean BLEU: {np.mean(bleu_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Std BLEU: {np.std(bleu_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Min BLEU: {np.min(bleu_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Max BLEU: {np.max(bleu_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ chrF Score\n",
    "\n",
    "chrF is a character n-gram F-score that works well across different languages and is less sensitive to tokenization issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing chrF Scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# chrF with different parameters\n",
    "start_time = time.perf_counter()\n",
    "chrf_scores = chrf_score(candidates, references, max_n=6, beta=2.0)\n",
    "chrf_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"chrF Scores (took {chrf_time*1000:.2f} ms):\")\n",
    "for i, score in enumerate(chrf_scores):\n",
    "    print(f\"  Text {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä chrF Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean chrF: {np.mean(chrf_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Std chrF: {np.std(chrf_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Min chrF: {np.min(chrf_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Max chrF: {np.max(chrf_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü METEOR Score\n",
    "\n",
    "METEOR considers synonymy, stemming, and paraphrasing for more robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing METEOR Scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "meteor_scores = meteor(candidates, references)\n",
    "meteor_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"METEOR Scores (took {meteor_time*1000:.2f} ms):\")\n",
    "for i, score in enumerate(meteor_scores):\n",
    "    print(f\"  Text {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä METEOR Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean METEOR: {np.mean(meteor_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Std METEOR: {np.std(meteor_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Min METEOR: {np.min(meteor_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Max METEOR: {np.max(meteor_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùå Word Error Rate (WER)\n",
    "\n",
    "WER measures the minimum number of word-level operations needed to transform the candidate into the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing WER Scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "wer_scores = wer(candidates, references)\n",
    "wer_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"WER Scores (took {wer_time*1000:.2f} ms):\")\n",
    "for i, score in enumerate(wer_scores):\n",
    "    print(f\"  Text {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä WER Statistics (lower is better):\")\n",
    "print(f\"  ‚Ä¢ Mean WER: {np.mean(wer_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Std WER: {np.std(wer_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Min WER: {np.min(wer_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Max WER: {np.max(wer_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Token-Level Metrics\n",
    "\n",
    "Fine-grained metrics for detailed analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing Token-Level Metrics...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Token F1\n",
    "start_time = time.perf_counter()\n",
    "token_f1_scores = token_f1(candidates, references)\n",
    "token_f1_time = time.perf_counter() - start_time\n",
    "\n",
    "# Jaccard similarity\n",
    "start_time = time.perf_counter()\n",
    "jaccard_scores = jaccard(candidates, references)\n",
    "jaccard_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"Token F1 Scores (took {token_f1_time*1000:.2f} ms):\")\n",
    "for i, score in enumerate(token_f1_scores):\n",
    "    print(f\"  Text {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nJaccard Scores (took {jaccard_time*1000:.2f} ms):\")\n",
    "for i, score in enumerate(jaccard_scores):\n",
    "    print(f\"  Text {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Token-Level Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean Token F1: {np.mean(token_f1_scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Mean Jaccard: {np.mean(jaccard_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† BERTScore Similarity\n",
    "\n",
    "BERTScore computes similarity using contextual embeddings for semantic evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing BERTScore Similarity...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample embeddings (in practice, these would come from a BERT model)\n",
    "np.random.seed(42)\n",
    "cand_embeddings = np.random.rand(len(candidates), 768).astype(np.float32)\n",
    "ref_embeddings = np.random.rand(len(references), 768).astype(np.float32)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "precision, recall, f1 = bert_score_similarity(cand_embeddings, ref_embeddings)\n",
    "bertscore_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"BERTScore Results (took {bertscore_time*1000:.2f} ms):\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Note: These are random embeddings for demonstration.\")\n",
    "print(f\"   In practice, use real BERT embeddings for meaningful results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ High-Level API: compute_text_metrics\n",
    "\n",
    "BlazeMetrics provides a convenient high-level API to compute multiple metrics at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Computing All Metrics at Once...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "all_metrics = compute_text_metrics(\n",
    "    candidates, references,\n",
    "    include=[\"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"chrf\", \"meteor\", \"wer\", \"token_f1\", \"jaccard\"],\n",
    "    lowercase=False,\n",
    "    stemming=False\n",
    ")\n",
    "compute_all_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"All metrics computed in {compute_all_time*1000:.2f} ms!\")\n",
    "print(f\"\\nüìä Available metrics: {list(all_metrics.keys())}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\nüìã Sample Results for Text 1:\")\n",
    "for metric, scores in all_metrics.items():\n",
    "    if len(scores) > 0:\n",
    "        print(f\"  ‚Ä¢ {metric}: {scores[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Aggregation and Analysis\n",
    "\n",
    "Let's aggregate the results and create visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all metrics\n",
    "aggregated = aggregate_samples(all_metrics)\n",
    "\n",
    "print(\"üìä Aggregated Metrics (across all texts):\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in aggregated.items():\n",
    "    print(f\"  ‚Ä¢ {metric}: {value:.4f}\")\n",
    "\n",
    "# Create performance comparison chart\n",
    "metrics_to_plot = [\"rouge1_f1\", \"rouge2_f1\", \"rougeL_f1\", \"bleu\", \"chrf\", \"meteor\", \"token_f1\", \"jaccard\"]\n",
    "values = [aggregated.get(m, 0.0) for m in metrics_to_plot]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(metrics_to_plot, values, color='skyblue', alpha=0.7)\n",
    "plt.title('Aggregated Metric Scores Across All Texts', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Analysis\n",
    "\n",
    "Let's compare the performance of different metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "performance_data = {\n",
    "    'ROUGE-1': rouge1_time * 1000,\n",
    "    'ROUGE-2': rouge2_time * 1000,\n",
    "    'ROUGE-L': rougeL_time * 1000,\n",
    "    'BLEU': bleu_time * 1000,\n",
    "    'chrF': chrf_time * 1000,\n",
    "    'METEOR': meteor_time * 1000,\n",
    "    'WER': wer_time * 1000,\n",
    "    'Token F1': token_f1_time * 1000,\n",
    "    'Jaccard': jaccard_time * 1000,\n",
    "    'All Combined': compute_all_time * 1000\n",
    "}\n",
    "\n",
    "print(\"‚ö° Performance Comparison (milliseconds):\")\n",
    "print(\"=\" * 50)\n",
    "for metric, time_ms in performance_data.items():\n",
    "    print(f\"  ‚Ä¢ {metric}: {time_ms:.2f} ms\")\n",
    "\n",
    "# Performance visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "metrics = list(performance_data.keys())\n",
    "times = list(performance_data.values())\n",
    "\n",
    "bars = plt.bar(metrics, times, color='lightcoral', alpha=0.7)\n",
    "plt.title('Metric Computation Performance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('Time (milliseconds)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{time_val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "print(f\"  ‚Ä¢ Fastest metric: {min(performance_data, key=performance_data.get)} ({min(performance_data.values()):.2f} ms)\")\n",
    "print(f\"  ‚Ä¢ Slowest metric: {max(performance_data, key=performance_data.get)} ({max(performance_data.values()):.2f} ms)\")\n",
    "print(f\"  ‚Ä¢ Computing all metrics: {performance_data['All Combined']:.2f} ms\")\n",
    "print(f\"  ‚Ä¢ Efficiency gain: {sum(performance_data.values()) - performance_data['All Combined']:.2f} ms saved by batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Batch Processing Efficiency\n",
    "\n",
    "Let's demonstrate the efficiency of batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 5, 10, 25, 50, 100]\n",
    "batch_times = []\n",
    "\n",
    "print(\"üîÑ Batch Processing Efficiency Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Create larger dataset\n",
    "    large_candidates = candidates * (batch_size // len(candidates) + 1)\n",
    "    large_references = references * (batch_size // len(references) + 1)\n",
    "    large_candidates = large_candidates[:batch_size]\n",
    "    large_references = large_references[:batch_size]\n",
    "    \n",
    "    # Time the computation\n",
    "    start_time = time.perf_counter()\n",
    "    _ = compute_text_metrics(large_candidates, large_references, include=[\"bleu\", \"rouge1\", \"chrf\"])\n",
    "    batch_time = time.perf_counter() - start_time\n",
    "    batch_times.append(batch_time)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Batch size {batch_size:3d}: {batch_time*1000:6.2f} ms ({batch_time*1000/batch_size:6.2f} ms per text)\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_sizes, [t*1000 for t in batch_times], 'o-', linewidth=2, markersize=8, color='green')\n",
    "plt.title('Total Processing Time vs Batch Size', fontweight='bold')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (milliseconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "per_text_times = [t*1000/s for t, s in zip(batch_times, batch_sizes)]\n",
    "plt.plot(batch_sizes, per_text_times, 's-', linewidth=2, markersize=8, color='orange')\n",
    "plt.title('Per-Text Processing Time vs Batch Size', fontweight='bold')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time per Text (milliseconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Efficiency Analysis:\")\n",
    "print(f\"  ‚Ä¢ Smallest batch (1): {batch_times[0]*1000:.2f} ms per text\")\n",
    "print(f\"  ‚Ä¢ Largest batch (100): {batch_times[-1]*1000/batch_sizes[-1]:.2f} ms per text\")\n",
    "print(f\"  ‚Ä¢ Efficiency gain: {batch_times[0]/batch_times[-1]*batch_sizes[-1]:.1f}x faster per text with batching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You've successfully explored all the core metrics provided by BlazeMetrics! Here's what we've covered:\n",
    "\n",
    "### ‚úÖ **Metrics Demonstrated:**\n",
    "- **ROUGE**: ROUGE-1, ROUGE-2, ROUGE-L for text overlap evaluation\n",
    "- **BLEU**: Standard machine translation metric\n",
    "- **chrF**: Character-level F-score for multilingual evaluation\n",
    "- **METEOR**: Robust evaluation considering synonyms and paraphrasing\n",
    "- **WER**: Word-level error rate for speech/text recognition\n",
    "- **Token-level**: F1 and Jaccard for fine-grained analysis\n",
    "- **BERTScore**: Semantic similarity using embeddings\n",
    "\n",
    "### üöÄ **Performance Features:**\n",
    "- **Rust Core**: Native performance without Python GIL limitations\n",
    "- **Parallel Processing**: Automatic parallelization with Rayon\n",
    "- **Batch Optimization**: Efficient processing of large datasets\n",
    "- **High-Level API**: `compute_text_metrics()` for computing multiple metrics at once\n",
    "\n",
    "### üìä **Key Benefits:**\n",
    "- **Speed**: 10-100x faster than pure Python implementations\n",
    "- **Accuracy**: Mathematically equivalent to standard implementations\n",
    "- **Scalability**: Efficient batch processing and parallelization\n",
    "- **Ease of Use**: Simple, familiar Python API\n",
    "\n",
    "### üîÑ **Next Steps:**\n",
    "Continue to the next notebook to explore:\n",
    "1. **üõ°Ô∏è [Guardrails Showcase](./03_guardrails_showcase.ipynb)** - Content moderation and safety features\n",
    "2. **üîÑ [Streaming & Monitoring](./04_streaming_monitoring.ipynb)** - Real-time evaluation\n",
    "3. **üè≠ [Production Workflows](./05_production_workflows.ipynb)** - Batch processing and deployment\n",
    "\n",
    "BlazeMetrics provides enterprise-grade performance for all your NLP evaluation needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}