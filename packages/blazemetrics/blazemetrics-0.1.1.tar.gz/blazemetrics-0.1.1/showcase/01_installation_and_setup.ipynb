{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ BlazeMetrics Installation & Setup\n",
    "\n",
    "This notebook demonstrates how to install and set up the BlazeMetrics package, a high-performance NLP evaluation metrics library powered by Rust.\n",
    "\n",
    "## What is BlazeMetrics?\n",
    "\n",
    "**BlazeMetrics** is a Python library designed to be the fastest implementation of standard NLP evaluation metrics, powered by a highly optimized Rust core. It leverages Rust's performance, memory safety, and true parallelism to offer significant speedups over pure Python implementations.\n",
    "\n",
    "### Key Features:\n",
    "- üî• **Blazing Fast**: Core logic written in Rust, compiled to native code\n",
    "- üöÄ **Parallelized**: Uses Rayon for true parallelism across CPU cores\n",
    "- üêç **Python Native**: Clean, intuitive API familiar to Python developers\n",
    "- üìä **Comprehensive Metrics**: ROUGE, BLEU, chrF, METEOR, WER, BERTScore, and more\n",
    "- üõ°Ô∏è **LLM Guardrails**: Ultra-fast content moderation and safety features\n",
    "- üìà **Production Ready**: Monitoring, exporters, and batch processing capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "BlazeMetrics can be installed directly from PyPI. When a prebuilt wheel is available for your OS/Python version, no compilation or Rust toolchain is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blazemetrics in c:\\users\\cgaur\\anaconda3\\envs\\plug_mcp\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\cgaur\\anaconda3\\envs\\plug_mcp\\lib\\site-packages (from blazemetrics) (2.3.2)\n"
     ]
    }
   ],
   "source": [
    "# Install BlazeMetrics from PyPI\n",
    "!pip install blazemetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Verify Installation\n",
    "\n",
    "Let's verify that BlazeMetrics is properly installed and accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'blazemetrics' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mblazemetrics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOK BlazeMetrics version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mblazemetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOK Package location: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblazemetrics.\u001b[34m__file__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mAttributeError\u001b[39m: module 'blazemetrics' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "# Check if BlazeMetrics is installed\n",
    "try:\n",
    "    import blazemetrics\n",
    "    print(f\"OK BlazeMetrics version: {blazemetrics.__version__}\")\n",
    "    print(f\"OK Package location: {blazemetrics.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Please install BlazeMetrics first: pip install blazemetrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Core Functions\n",
    "\n",
    "Let's import all the main functions and classes available in BlazeMetrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core metrics functions\n",
    "from blazemetrics import (\n",
    "    # Core metrics\n",
    "    rouge_score,\n",
    "    bleu,\n",
    "    chrf_score,\n",
    "    meteor,\n",
    "    wer,\n",
    "    token_f1,\n",
    "    jaccard,\n",
    "    bert_score_similarity,\n",
    "    moverscore_greedy,\n",
    "    \n",
    "    # High-level APIs\n",
    "    compute_text_metrics,\n",
    "    aggregate_samples,\n",
    "    \n",
    "    # Guardrails\n",
    "    Guardrails,\n",
    "    guardrails_check,\n",
    "    \n",
    "    # Streaming and monitoring\n",
    "    monitor_tokens_sync,\n",
    "    monitor_tokens_async,\n",
    "    map_large_texts,\n",
    "    enforce_stream_sync,\n",
    "    \n",
    "    # Parallelism controls\n",
    "    set_parallel,\n",
    "    get_parallel,\n",
    "    set_parallel_threshold,\n",
    "    get_parallel_threshold\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All BlazeMetrics functions imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Parallelism Configuration\n",
    "\n",
    "BlazeMetrics uses Rayon for parallelization. You can control this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current parallelism settings\n",
    "print(f\"Current parallelism enabled: {get_parallel()}\")\n",
    "print(f\"Current parallel threshold: {get_parallel_threshold()}\")\n",
    "\n",
    "# Configure parallelism for your use case\n",
    "# For small batches (streaming), you might want to disable parallelism\n",
    "set_parallel_threshold(1000)  # Only parallelize batches >= 1000\n",
    "print(f\"\\nUpdated parallel threshold: {get_parallel_threshold()}\")\n",
    "\n",
    "# For large batch processing, enable full parallelism\n",
    "set_parallel(True)\n",
    "print(f\"Parallelism enabled: {get_parallel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç System Information\n",
    "\n",
    "Let's check your system configuration to understand the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "\n",
    "print(\"üñ•Ô∏è  System Information:\")\n",
    "print(f\"  ‚Ä¢ Python version: {sys.version}\")\n",
    "print(f\"  ‚Ä¢ Platform: {platform.platform()}\")\n",
    "print(f\"  ‚Ä¢ Architecture: {platform.architecture()}\")\n",
    "print(f\"  ‚Ä¢ CPU cores: {os.cpu_count()}\")\n",
    "print(f\"  ‚Ä¢ BlazeMetrics parallelism: {get_parallel()}\")\n",
    "print(f\"  ‚Ä¢ Parallel threshold: {get_parallel_threshold()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Next Steps\n",
    "\n",
    "Now that BlazeMetrics is installed and configured, you can explore:\n",
    "\n",
    "1. **üìä [Core Metrics Showcase](./02_core_metrics_showcase.ipynb)** - Learn about ROUGE, BLEU, chrF, and other metrics\n",
    "2. **üõ°Ô∏è [Guardrails Showcase](./03_guardrails_showcase.ipynb)** - Explore content moderation and safety features\n",
    "3. **üîÑ [Streaming & Monitoring](./04_streaming_monitoring.ipynb)** - See real-time monitoring in action\n",
    "4. **üè≠ [Production Workflows](./05_production_workflows.ipynb)** - Learn about batch processing and production use cases\n",
    "5. **‚ö° [Performance Benchmarking](./06_performance_benchmarking.ipynb)** - Compare with other packages\n",
    "\n",
    "## üéØ Quick Test\n",
    "\n",
    "Let's run a quick test to ensure everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick functionality test\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [[\"the cat was on the mat\"]]\n",
    "\n",
    "try:\n",
    "    # Test ROUGE\n",
    "    rouge_scores = rouge_score(candidates, references, score_type=\"rouge_n\", n=1)\n",
    "    print(f\"‚úÖ ROUGE-1 test passed: {rouge_scores[0]}\")\n",
    "    \n",
    "    # Test BLEU\n",
    "    bleu_scores = bleu(candidates, references)\n",
    "    print(f\"‚úÖ BLEU test passed: {bleu_scores[0]:.4f}\")\n",
    "    \n",
    "    # Test Guardrails\n",
    "    gr = Guardrails(blocklist=[\"bomb\"], redact_pii=True)\n",
    "    result = gr.check([\"My email is test@example.com\"])\n",
    "    print(f\"‚úÖ Guardrails test passed: {result['redacted'][0]}\")\n",
    "    \n",
    "    print(\"\\nüéâ All tests passed! BlazeMetrics is ready to use.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    print(\"Please check your installation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plug_mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
