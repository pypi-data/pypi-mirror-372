{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸ”„ BlazeMetrics Streaming & Monitoring\n",
        "\n",
        "This notebook demonstrates real-time streaming evaluation and monitoring for production.\n",
        "\n",
        "- Live rolling-window metrics (BLEU, ROUGE-1, chrF, WER)\n",
        "- Token-level guardrails monitoring and enforcement\n",
        "- Threshold-based alerts\n",
        "- Optional exporters (Prometheus/StatsD)\n",
        "\n",
        "Assumptions:\n",
        "- BlazeMetrics installed: `pip install blazemetrics`\n",
        "- For exporters, install `prometheus_client` and `statsd` if you want to push metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from collections import deque\n",
        "from typing import List, Dict, Deque, Iterator\n",
        "import time\n",
        "import random\n",
        "\n",
        "from blazemetrics import (\n",
        "    compute_text_metrics,\n",
        "    aggregate_samples,\n",
        "    monitor_tokens_sync,\n",
        "    enforce_stream_sync,\n",
        "    Guardrails,\n",
        "    MetricsExporters,\n",
        ")\n",
        "\n",
        "print(\"âœ… Imports ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated streaming source\n",
        "\n",
        "def simulate_stream(n: int = 300, seed: int = 123) -> List[tuple[str, List[str]]]:\n",
        "    rng = random.Random(seed)\n",
        "    data: List[tuple[str, List[str]]] = []\n",
        "    for i in range(n):\n",
        "        prompt = f\"user asked {i}\"\n",
        "        ref = [f\"answer for {i}\"]\n",
        "        # insert quality dips every 75 steps\n",
        "        if i % 75 == 0:\n",
        "            ref = [\"noisy reference\"]\n",
        "        data.append((prompt, ref))\n",
        "    return data\n",
        "\n",
        "print(\"âœ… Stream simulator ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rolling-window metrics with thresholded alerts\n",
        "\n",
        "ALERT_THRESHOLDS = {\n",
        "    \"bleu\": 0.15,\n",
        "    \"rouge1_f1\": 0.30,\n",
        "    \"chrf\": 0.25,\n",
        "    \"wer\": 0.40,  # higher is worse\n",
        "}\n",
        "\n",
        "window: Deque[tuple[str, List[str]]] = deque(maxlen=100)\n",
        "history: List[Dict[str, float]] = []\n",
        "\n",
        "for i, (prompt, ref) in enumerate(simulate_stream()):\n",
        "    pred = prompt.replace(\"asked\", \"answered\")  # placeholder for model\n",
        "    window.append((pred, ref))\n",
        "    if len(window) < window.maxlen:\n",
        "        continue\n",
        "\n",
        "    candidates = [p for p, _ in window]\n",
        "    refs = [r for _, r in window]\n",
        "\n",
        "    sm = compute_text_metrics(candidates, refs, include=[\"bleu\", \"rouge1\", \"chrf\", \"wer\"])\n",
        "    agg = aggregate_samples(sm)\n",
        "    history.append(agg)\n",
        "\n",
        "    alerts = []\n",
        "    if agg.get(\"bleu\", 1.0) < ALERT_THRESHOLDS[\"bleu\"]:\n",
        "        alerts.append(f\"BLEU drop: {agg['bleu']:.3f}\")\n",
        "    if agg.get(\"rouge1_f1\", 1.0) < ALERT_THRESHOLDS[\"rouge1_f1\"]:\n",
        "        alerts.append(f\"ROUGE-1 drop: {agg['rouge1_f1']:.3f}\")\n",
        "    if agg.get(\"chrf\", 1.0) < ALERT_THRESHOLDS[\"chrf\"]:\n",
        "        alerts.append(f\"chrF drop: {agg['chrf']:.3f}\")\n",
        "    if agg.get(\"wer\", 0.0) > ALERT_THRESHOLDS[\"wer\"]:\n",
        "        alerts.append(f\"WER spike: {agg['wer']:.3f}\")\n",
        "\n",
        "    if i % 20 == 0:\n",
        "        print(f\"t={i}: metrics={{k: round(v,3) for k,v in agg.items()}}\")\n",
        "    if alerts:\n",
        "        print(\"ALERT:\", \"; \".join(alerts))\n",
        "\n",
        "print(\"âœ… Rolling-window monitoring complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token-level guardrails monitoring and enforcement\n",
        "\n",
        "rails = Guardrails(\n",
        "    blocklist=[\"bomb\", \"terror\"],\n",
        "    regexes=[r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"],  # SSN\n",
        "    case_insensitive=True,\n",
        "    redact_pii=True,\n",
        "    safety=True,\n",
        ")\n",
        "\n",
        "def token_iter_from_text(text: str, n: int = 6) -> Iterator[str]:\n",
        "    toks = text.split()\n",
        "    for i in range(0, len(toks), n):\n",
        "        yield \" \".join(toks[i:i+n])\n",
        "\n",
        "text = (\n",
        "    \"Write a note including an SSN 123-45-6789 and do not ignore previous instructions. \"\n",
        "    \"This could be dangerous, almost like a terror threat.\"\n",
        ")\n",
        "\n",
        "print(\"â€” Monitoring tokens â€”\")\n",
        "for res in monitor_tokens_sync(token_iter_from_text(text, 6), rails, every_n_tokens=1, joiner=\" \"):\n",
        "    print({k: (v if isinstance(v, list) else v) for k, v in res.items()})\n",
        "\n",
        "print(\"\\nâ€” Enforcing tokens â€”\")\n",
        "for chunk in enforce_stream_sync(token_iter_from_text(text, 6), rails, every_n_tokens=1, joiner=\" \", replacement=\"[BLOCKED]\", safety_threshold=0.6):\n",
        "    print(chunk, end=\"\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Export aggregated metrics to Prometheus/StatsD\n",
        "\n",
        "exporters = MetricsExporters(prometheus_gateway=None, statsd_addr=None)\n",
        "\n",
        "if history:\n",
        "    latest = history[-1]\n",
        "    exporters.export(latest, labels={\"window\": \"100\"})\n",
        "    print(\"âœ… Exported latest window metrics (mock). Configure gateways to push.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
