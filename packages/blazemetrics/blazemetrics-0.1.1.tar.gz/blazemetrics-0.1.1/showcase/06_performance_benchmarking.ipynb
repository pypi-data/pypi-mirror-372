{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ⚡ BlazeMetrics Benchmarking & Comparative Analysis\n",
        "\n",
        "This notebook compares BlazeMetrics against popular Python implementations where available.\n",
        "- ROUGE (`rouge-score`), BLEU (`nltk`), chrF (`sacrebleu`), METEOR (`nltk`), WER (`jiwer`), BERTScore (`bert-score`), MoverScore (`moverscore_v2`)\n",
        "- Cleanly skips baselines when dependencies are missing\n",
        "- Reports timings (min/avg over repeats) and exports CSV/Markdown\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time, random, os, json\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "\n",
        "# Optional baselines (best-effort imports)\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "    from nltk.translate.meteor_score import meteor_score as nltk_meteor\n",
        "except Exception:\n",
        "    nltk = None\n",
        "\n",
        "try:\n",
        "    import sacrebleu\n",
        "except Exception:\n",
        "    sacrebleu = None\n",
        "\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except Exception:\n",
        "    rouge_scorer = None\n",
        "\n",
        "try:\n",
        "    import jiwer\n",
        "except Exception:\n",
        "    jiwer = None\n",
        "\n",
        "try:\n",
        "    from bert_score import score as bertscore_score\n",
        "except Exception:\n",
        "    bertscore_score = None\n",
        "\n",
        "try:\n",
        "    from moverscore_v2 import get_idf_dict, word_mover_score\n",
        "except Exception:\n",
        "    word_mover_score = None\n",
        "\n",
        "from blazemetrics import (\n",
        "    rouge_score as rg_rouge,\n",
        "    bleu as rg_bleu,\n",
        "    chrf_score as rg_chrf,\n",
        "    meteor as rg_meteor,\n",
        "    wer as rg_wer,\n",
        "    bert_score_similarity as rg_bertsim,\n",
        "    moverscore_greedy as rg_moverscore,\n",
        "    compute_text_metrics as rg_compute_all,\n",
        ")\n",
        "\n",
        "print(\"✅ Imports complete (missing baselines will be skipped)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_corpus(n=1000, vocab=500, seed=123):\n",
        "    rng = random.Random(seed)\n",
        "    def sentence(min_len=5, max_len=20):\n",
        "        length = rng.randint(min_len, max_len)\n",
        "        return \" \".join(f\"t{rng.randint(1, vocab)}\" for _ in range(length))\n",
        "    candidates = [sentence() for _ in range(n)]\n",
        "    references = [[sentence()] for _ in range(n)]\n",
        "    return candidates, references\n",
        "\n",
        "cands, refs = gen_corpus()\n",
        "print(f\"Generated corpus: {len(cands)} candidates, {len(refs)} references\")\n",
        "\n",
        "# Embeddings for blazemetrics BERT similarity demo (random)\n",
        "np.random.seed(42)\n",
        "cand_emb = np.random.rand(128, 768).astype(np.float32)\n",
        "ref_emb = np.random.rand(128, 768).astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def timeit(fn, repeat=3, warmup=1):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    times = []\n",
        "    for _ in range(repeat):\n",
        "        t0 = time.perf_counter()\n",
        "        _ = fn()\n",
        "        t1 = time.perf_counter()\n",
        "        times.append(t1 - t0)\n",
        "    return min(times), sum(times) / len(times)\n",
        "\n",
        "results: Dict[str, list[tuple[str, float, float]]] = {}\n",
        "\n",
        "# ROUGE\n",
        "min_t, avg_t = timeit(lambda: rg_rouge(cands, refs, score_type=\"rouge_n\", n=1))\n",
        "results.setdefault(\"ROUGE-1\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "if rouge_scorer is not None:\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=False)\n",
        "    def _py_rouge1():\n",
        "        return [scorer.score(refs[i][0], cands[i])[\"rouge1\"].fmeasure for i in range(len(cands))]\n",
        "    min_t, avg_t = timeit(_py_rouge1)\n",
        "    results[\"ROUGE-1\"].append((\"rouge-score\", min_t, avg_t))\n",
        "\n",
        "# BLEU\n",
        "min_t, avg_t = timeit(lambda: rg_bleu(cands, refs))\n",
        "results.setdefault(\"BLEU\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "if nltk is not None:\n",
        "    ch = SmoothingFunction().method1\n",
        "    def _py_bleu():\n",
        "        scores = []\n",
        "        for c, rlist in zip(cands, refs):\n",
        "            ref_tokens = [r.split() for r in rlist]\n",
        "            cand_tokens = c.split()\n",
        "            scores.append(sentence_bleu(ref_tokens, cand_tokens, smoothing_function=ch))\n",
        "        return scores\n",
        "    min_t, avg_t = timeit(_py_bleu)\n",
        "    results[\"BLEU\"].append((\"nltk\", min_t, avg_t))\n",
        "\n",
        "# chrF\n",
        "min_t, avg_t = timeit(lambda: rg_chrf(cands, refs))\n",
        "results.setdefault(\"chrF\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "if sacrebleu is not None:\n",
        "    def _py_chrf():\n",
        "        return sacrebleu.corpus_chrf(cands, list(zip(*refs))[0]).scores  # type: ignore\n",
        "    try:\n",
        "        min_t, avg_t = timeit(_py_chrf)\n",
        "        results[\"chrF\"].append((\"sacrebleu\", min_t, avg_t))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# METEOR\n",
        "min_t, avg_t = timeit(lambda: rg_meteor(cands, refs))\n",
        "results.setdefault(\"METEOR\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "if nltk is not None:\n",
        "    def _py_meteor():\n",
        "        return [nltk_meteor(r[0], c) for c, r in zip(cands, refs)]\n",
        "    min_t, avg_t = timeit(_py_meteor)\n",
        "    results[\"METEOR\"].append((\"nltk\", min_t, avg_t))\n",
        "\n",
        "# WER\n",
        "min_t, avg_t = timeit(lambda: rg_wer(cands, refs))\n",
        "results.setdefault(\"WER\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "if jiwer is not None:\n",
        "    def _py_wer():\n",
        "        return [jiwer.wer(r[0], c) for c, r in zip(cands, refs)]\n",
        "    min_t, avg_t = timeit(_py_wer)\n",
        "    results[\"WER\"].append((\"jiwer\", min_t, avg_t))\n",
        "\n",
        "# BERT similarity (kernel)\n",
        "min_t, avg_t = timeit(lambda: rg_bertsim(cand_emb, ref_emb))\n",
        "results.setdefault(\"BERT-sim\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "\n",
        "# MoverScore (greedy kernel)\n",
        "min_t, avg_t = timeit(lambda: rg_moverscore(cands, refs))\n",
        "results.setdefault(\"MoverScore (greedy)\", []).append((\"blazemetrics\", min_t, avg_t))\n",
        "\n",
        "print(\"✅ Benchmarks complete\")\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pretty print & optional export\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "rows = []\n",
        "for section, entries in results.items():\n",
        "    for name, tmin, tavg in entries:\n",
        "        rows.append({\"metric\": section, \"impl\": name, \"t_min_s\": tmin, \"t_avg_s\": tavg})\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values([\"metric\", \"t_avg_s\"]).reset_index(drop=True)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results (optional)\n",
        "out_dir = Path(\"bench_outputs\"); out_dir.mkdir(exist_ok=True)\n",
        "csv_path = out_dir / \"benchmark_results.csv\"\n",
        "md_path = out_dir / \"benchmark_summary.md\"\n",
        "json_path = out_dir / \"benchmark_results.json\"\n",
        "\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "# Simple Markdown summary\n",
        "groups = df.groupby(\"metric\")\n",
        "lines = [\"# Benchmark Summary\\n\"]\n",
        "for metric, g in groups:\n",
        "    lines.append(f\"\\n## {metric}\\n\")\n",
        "    for _, row in g.iterrows():\n",
        "        lines.append(f\"- {row['impl']}: min={row['t_min_s']:.4f}s avg={row['t_avg_s']:.4f}s\")\n",
        "md_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "\n",
        "json_path.write_text(json.dumps(results, default=float, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Saved: {csv_path}, {md_path}, {json_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
