# sparrow-python

[![image](https://img.shields.io/badge/Pypi-0.1.7-green.svg)](https://pypi.org/project/sparrow-python)
[![image](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/)
[![image](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## å¿«é€Ÿå‘½ä»¤ç´¢å¼•

### ğŸ¯ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥
```bash
# æŸ¥çœ‹è¡¨æ ¼æ•°æ®
spr table_viewer data.csv

# å›¾åƒæ‰¹é‡å¤„ç†  
spr mllm_call_images ./photos
spr download_images "å…³é”®è¯" --num_images=100

# è§†é¢‘å¤„ç†
spr video_dedup video.mp4
spr frames_to_video frames_dir

# æ–‡ä»¶æ“ä½œ
spr pack folder_name        # å‹ç¼©
spr split large_file.dat    # åˆ†å‰²å¤§æ–‡ä»¶
spr kill 8080              # æ€æ­»ç«¯å£è¿›ç¨‹

# é¡¹ç›®å·¥å…·
spr create my_project      # åˆ›å»ºé¡¹ç›®
spr clone repo_url         # å…‹éš†ä»“åº“
spr gen_key project_name   # ç”ŸæˆSSHå¯†é’¥

# æœåŠ¡å¯åŠ¨
spr start_server           # å¤šè¿›ç¨‹æœåŠ¡å™¨
spr reminder              # æé†’æœåŠ¡
```

### ğŸ“– è¯¦ç»†å‘½ä»¤è¯´æ˜
æ‰€æœ‰å‘½ä»¤éƒ½æ”¯æŒ `sp`ã€`spr`ã€`sparrow` ä¸‰ç§è°ƒç”¨æ–¹å¼ã€‚
ä½¿ç”¨ `spr <command> --help` æŸ¥çœ‹å…·ä½“å‚æ•°è¯´æ˜ã€‚

---

## TODO
- [ ] å¤šæ¨¡æ€å›¾åƒé¢„å¤„ç† è€ƒè™‘ä½¿ç”¨å¤šè¿›ç¨‹
- [ ] æ‰¾ä¸€ä¸ªå¯ä»¥ä¼˜é›…ç»˜åˆ¶æµç¨‹å›¾ã€ç¤ºæ„å›¾çš„å·¥å…·ï¼Œå¦‚pptï¼Ÿ
- [ ]  å®ç°ä¸€ä¸ªä¼˜é›…çš„TextSplitter

- [ ] promptè°ƒè¯•é¡µé¢
- [ ] ç›¸å…³é…ç½®æŒ‡å®šæ”¯æŒï¼špromptåç«¯åœ°å€ï¼›æ¨¡å‹å‚æ•°é…ç½®ï¼›
- [ ] 
- [ ] æ·»åŠ æµ‹è¯•æŒ‰é’®ï¼Œæ¨¡å‹é€‰é¡¹ï¼Œæ¨¡å‹é…ç½®
- [ ] åŸç”Ÿgitä¸‹è½½æ”¯æŒ
- [ ]
- [X] streamlit å¤šæ¨¡æ€chat input: https://github.com/streamlit/streamlit/issues/7409
- [ ] https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/chat/vllm_engine.py#L99

è¯†åˆ«ä¸‹é¢é“¾æ¥çš„æ»šåŠ¨æˆªå›¾ï¼š
https://sjh.baidu.com/site/dzfmws.cn/da721a31-476d-42ed-aad1-81c2dc3a66a3



vllm å¼‚æ­¥æ¨ç†ç¤ºä¾‹ï¼š

new å®ä¾‹(from deepwiki)  
```python
import asyncio  
from fastapi import FastAPI, Request  
from fastapi.responses import JSONResponse, StreamingResponse  
from vllm.engine.arg_utils import AsyncEngineArgs  
from vllm.engine.async_llm_engine import AsyncLLMEngine  
from vllm.sampling_params import SamplingParams  
from vllm.utils import random_uuid  
import json  
  
app = FastAPI()  
engine = None  
  
async def init_engine():  
    """åˆå§‹åŒ– vLLM å¼•æ“"""  
    global engine  
    # é…ç½®å¼•æ“å‚æ•°  
    engine_args = AsyncEngineArgs(  
        model="your-model-name",  # æ›¿æ¢ä¸ºæ‚¨çš„æ¨¡å‹  
        tensor_parallel_size=1,   # æ ¹æ®æ‚¨çš„GPUæ•°é‡è°ƒæ•´  
        dtype="auto",  
        max_model_len=2048,  
    )  
    engine = AsyncLLMEngine.from_engine_args(engine_args)  
  
@app.on_event("startup")  
async def startup_event():  
    await init_engine()  
  
@app.post("/generate")  
async def generate(request: Request):  
    """ç”Ÿæˆæ–‡æœ¬çš„ç«¯ç‚¹"""  
    request_dict = await request.json()  
    prompt = request_dict.get("prompt")  
    stream = request_dict.get("stream", False)  
      
    # åˆ›å»ºé‡‡æ ·å‚æ•°  
    sampling_params = SamplingParams(  
        temperature=request_dict.get("temperature", 0.7),  
        max_tokens=request_dict.get("max_tokens", 100),  
        top_p=request_dict.get("top_p", 1.0),  
    )  
      
    request_id = random_uuid()  
    results_generator = engine.generate(prompt, sampling_params, request_id)  
      
    if stream:  
        # æµå¼å“åº”  
        async def stream_results():  
            async for request_output in results_generator:  
                text_outputs = [output.text for output in request_output.outputs]  
                ret = {"text": text_outputs}
                yield f"data: {json.dumps(ret)}\n\n"  
          
        return StreamingResponse(stream_results(), media_type="text/plain")  
    else:  
        # éæµå¼å“åº”  
        final_output = None  
        async for request_output in results_generator:  
            final_output = request_output  
          
        text_outputs = [output.text for output in final_output.outputs]  
        return JSONResponse({"text": text_outputs})  
  
if __name__ == "__main__":  
    import uvicorn  
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

```python
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
import uvicorn
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.sampling_params import SamplingParams
import torch

# Define request data model
class RequestData(BaseModel):
    prompts: List[str]
    max_tokens: int = 2048
    temperature: float = 0.7

# Initialize FastAPI app
app = FastAPI()

# Determine device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Initialize AsyncLLMEngine
engine_args = AsyncEngineArgs(
    model="your-model-name",  # Replace with your model name
    dtype="bfloat16",
    gpu_memory_utilization=0.8,
    max_model_len=4096,
    trust_remote_code=True
)
llm_engine = AsyncLLMEngine.from_engine_args(engine_args)

# Define the inference endpoint
@app.post("/predict")
async def generate_text(data: RequestData):
    sampling_params = SamplingParams(
        max_tokens=data.max_tokens,
        temperature=data.temperature
    )
    request_id = "unique_request_id"  # Generate a unique request ID
    results_generator = llm_engine.generate(data.prompts, sampling_params, request_id)
  
    final_output = None
    async for request_output in results_generator:
        final_output = request_output
  
    assert final_output is not None
    text_outputs = [output.text for output in final_output.outputs]
    return {"responses": text_outputs}

# Run the server
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

```



## å¾…æ·»åŠ è„šæœ¬

## Install

```bash
pip install sparrow-python
# Or dev version
pip install sparrow-python[dev]
# Or
pip install -e .
# Or
pip install -e .[dev]
```

## Usage

### Multiprocessing SyncManager

Open server first:

```bash
$ spr start-server
```

The defualt port `50001`.

(Process1) productor:

```python
from sparrow.multiprocess.client import Client

client = Client(port=50001)
client.update_dict({'a': 1, 'b': 2})
```

(Process2) consumer:

```python
from sparrow.multiprocess.client import Client

client = Client(port=50001)
print(client.get_dict_data())

>> > {'a': 1, 'b': 2}
```

### å¸¸ç”¨å·¥å…·

#### æ•°æ®å¤„ç†ä¸æŸ¥çœ‹
- **è¡¨æ ¼æŸ¥çœ‹å™¨**
```bash
# åŸºæœ¬ç”¨æ³•
spr table_viewer sample_products.csv --port 8081

# æŒ‡å®šå›¾åƒåˆ—å¹¶è®¾ç½®ç«¯å£
spr table_viewer "products.xlsx" --image_columns="product_image,thumbnail" --port=9090

# æŒ‡å®šå·¥ä½œè¡¨
spr table_viewer "report.xlsx" --sheet_name="Sheet2"
```

- **æ–‡æœ¬å»é‡**
```bash
# ä½¿ç”¨ç¼–è¾‘è·ç¦»å»é‡
spr deduplicate input.txt output.txt --method=edit --threshold=0.8

# ä½¿ç”¨ROUGEç›¸ä¼¼åº¦å»é‡
spr deduplicate data.csv clean.csv --method=rouge --target_col=content
```

- **æ–‡ä»¶å‹ç¼©ä¸è§£å‹**
æ”¯æŒæ ¼å¼ï¼š"zip", "tar", "gztar", "bztar", "xztar"
```bash
# å‹ç¼©æ–‡ä»¶/æ–‡ä»¶å¤¹
spr pack pack_dir

# è§£å‹æ–‡ä»¶
spr unpack filename extract_dir
```

- **å¤§æ–‡ä»¶åˆ†å‰²ä¸åˆå¹¶**
```bash
# åˆ†å‰²å¤§æ–‡ä»¶ (é»˜è®¤1GBå—)
spr split large_file.dat

# åˆå¹¶åˆ†å‰²æ–‡ä»¶
spr merge large_file.dat
```

#### é¡¹ç›®ç®¡ç†
- **é¡¹ç›®è„šæ‰‹æ¶**
```bash
spr create awesome-project
```

- **Gitä»“åº“å…‹éš†**
```bash
# åŸºæœ¬å…‹éš†
spr clone https://github.com/user/repo.git

# æŒ‡å®šåˆ†æ”¯å’Œä¿å­˜è·¯å¾„
spr clone https://github.com/user/repo.git --branch=dev --save_path=./my_project
```

- **è‡ªåŠ¨Gitæäº¤ç›‘æ§**
```bash
spr auto_commit --interval=60
```

- **SSHå¯†é’¥ç”Ÿæˆ**
```bash
spr gen_key project_name --email=your@email.com
```

- **é…ç½®ç®¡ç†**
```bash
# åˆå§‹åŒ–é…ç½®æ–‡ä»¶
spr init_config

# æŸ¥çœ‹å½“å‰é…ç½®
spr get_config

# æŸ¥çœ‹ç‰¹å®šé…ç½®é¡¹
spr get_config mllm.model
```

#### ç³»ç»Ÿå·¥å…·
- **ç«¯å£è¿›ç¨‹ç®¡ç†**
```bash
# æ€æ­»æŒ‡å®šç«¯å£è¿›ç¨‹
spr kill 8080

# è·å–æœ¬æœºIP
spr get_ip
spr get_ip --env=outer  # è·å–å¤–ç½‘IP
```

- **Dockerç®¡ç†**
```bash
# ä¿å­˜æ‰€æœ‰Dockeré•œåƒ
spr save_docker_images

# åŠ è½½Dockeré•œåƒ
spr load_docker_images

# Docker GPUçŠ¶æ€ç›‘æ§
spr docker_gpu_stat
```

#### å¤šåª’ä½“å¤„ç†
- **è§†é¢‘å¸§å»é‡**
```bash
# åŸºæœ¬å»é‡ (é»˜è®¤phashç®—æ³•)
spr video_dedup video.mp4

# è‡ªå®šä¹‰å‚æ•°
spr video_dedup video.mp4 --method=dhash --threshold=5 --step=2 --workers=4
```

- **å›¾åƒå¸§è½¬è§†é¢‘**
```bash
# å°†å¸§ç›®å½•è½¬æ¢ä¸ºè§†é¢‘
spr frames_to_video frames_dir --fps=24

# ä¸€ç«™å¼ï¼šå»é‡+ç”Ÿæˆè§†é¢‘
spr dedup_and_create_video video.mp4 --video_fps=15
```

- **è§†é¢‘å­—å¹•å¤„ç†**
```bash
# è‡ªåŠ¨ç”Ÿæˆå­—å¹•ï¼ˆè½¬å½•+ç¿»è¯‘ï¼‰
spr subtitles video.mp4

# ç¿»è¯‘ç°æœ‰å­—å¹•
spr translate_subt subtitles.srt

# åˆå¹¶åŒè¯­å­—å¹•
spr merge_subtitles en.srt zh.srt
```

#### å›¾åƒä¸‹è½½ä¸å¤„ç†
- **æ‰¹é‡å›¾åƒä¸‹è½½**
```bash
# å•å…³é”®è¯ä¸‹è½½
spr download_images "çŒ«å’ª" --num_images=100

# å¤šå…³é”®è¯ï¼Œå¤šæœç´¢å¼•æ“
spr download_images "çŒ«å’ª,ç‹—ç‹—" --engines="bing,google,baidu" --save_dir="animals"
```

#### å¤§æ¨¡å‹ä¸AI
- **æ‰¹é‡å›¾åƒè¯†åˆ«ï¼ˆè¡¨æ ¼ï¼‰**
```bash
# åŸºæœ¬ç”¨æ³•
spr mllm_call_table images.xlsx --image_col=å›¾ç‰‡è·¯å¾„

# è‡ªå®šä¹‰æ¨¡å‹å’Œæç¤ºè¯
spr mllm_call_table data.csv \
    --model="gpt-4o-mini" \
    --text_prompt="è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡" \
    --output_file="results.csv"
```

- **æ‰¹é‡å›¾åƒè¯†åˆ«ï¼ˆæ–‡ä»¶å¤¹ï¼‰**
```bash
# å¤„ç†æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰å›¾ç‰‡
spr mllm_call_images ./photos --recursive=True

# æŒ‡å®šæ–‡ä»¶ç±»å‹å’Œæ•°é‡é™åˆ¶
spr mllm_call_images ./images \
    --extensions=".jpg,.png" \
    --max_num=50 \
    --output_file="analysis.csv"
```

#### ç½‘ç»œä¸API
- **å¼‚æ­¥HTTPè¯·æ±‚**
```bash
# POSTè¯·æ±‚
spr post "https://api.example.com" '{"key": "value"}' --concurrent=10

# GETè¯·æ±‚
spr get_url "https://api.example.com" --concurrent=5
```

- **æ–‡ä»¶ä¼ è¾“**
```bash
# P2Pæ–‡ä»¶ä¼ è¾“ (åŸºäºcroc)
spr send file.txt
spr recv  # åœ¨å¦ä¸€å°æœºå™¨ä¸Šæ¥æ”¶

# äº‘å­˜å‚¨ä¼ è¾“
spr send2 file.txt workspace_name
spr recv2 file.txt workspace_name
```

#### æ•°æ®åº“ä¸æœåŠ¡
- **å¯åŠ¨å¤šè¿›ç¨‹åŒæ­¥æœåŠ¡å™¨**
```bash
spr start_server --port=50001
```

- **Milvuså‘é‡æ•°æ®åº“**
```bash
# å¯åŠ¨MilvusæœåŠ¡
spr milvus start

# åœæ­¢MilvusæœåŠ¡
spr milvus stop
```

- **æ•°æ®å­˜å‚¨ (FlaxKV)**
```bash
# å­˜å‚¨æ–‡ä»¶åˆ°æŒ‡å®šç©ºé—´
spr set mykey /path/to/file.txt

# è·å–å­˜å‚¨çš„æ•°æ®
spr get mykey

# æŸ¥çœ‹æ‰€æœ‰å­˜å‚¨çš„é”®
spr keys

# æ¸…ç†è¿‡æœŸæ•°æ®
spr clean
```

#### å¼€å‘å·¥å…·
- **è½¯ä»¶å®‰è£…**
```bash
# å®‰è£…Node.js (é€šè¿‡NVM)
spr install_node --version=18

# å®‰è£…/å¸è½½Neovim
spr install_nvim --version=0.9.2
spr uninstall_nvim
```

- **å®šæ—¶å™¨å·¥å…·**
```bash
spr timer --dt=0.5  # 0.5ç§’é—´éš”å®šæ—¶å™¨
```

- **æ€§èƒ½æµ‹è¯•**
```bash
# æµ‹è¯•PyTorchç¯å¢ƒ
spr test_torch
```

#### é«˜çº§åŠŸèƒ½
- **æé†’æœåŠ¡**
```bash
# å¯åŠ¨Webæé†’æœåŠ¡
spr reminder --port=8000
```

### Some useful functions

> `sparrow.relp`
> Relative path, which is used to read or save files more easily.

> `sparrow.performance.MeasureTime`
> For measuring time (including gpu time)

> `sparrow.performance.get_process_memory`
> Get the memory size occupied by the process

> `sparrow.performance.get_virtual_memory`
> Get virtual machine memory information

> `sparrow.add_env_path`
> Add python environment variable (use relative file path)
