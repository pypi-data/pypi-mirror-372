import hashlib
import json
import os
import random
import re
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import requests
import retrying
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from gne import GeneralNewsExtractor
from htmldate import find_date
from loguru import logger
from urllib.parse import urljoin
from lxml import etree
from lxparse import LxParse
from pybloom_live import BloomFilter
import pickle
from urllib.parse import urlparse
from DrissionPage import WebPage


headers = {'user-agent':  str(UserAgent().random)}




def run_in_threads(func, datas, max_workers=10, rate_limit=None, retries=3, retry_delay=1,batch_size=None, batch_delay=0):
    """
    å¤šçº¿ç¨‹è¿è¡Œå‡½æ•°ï¼Œæ”¯æŒé€Ÿç‡é™åˆ¶ & è‡ªåŠ¨é‡è¯• & æ—¥å¿—è¾“å‡ºè¿›åº¦ & æ‰¹é‡æ‰§è¡Œ

    Args:
        func (callable): éœ€è¦æ‰§è¡Œçš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªå‚æ•°
        datas (iterable | list): æ•°æ®åˆ—è¡¨æˆ–å¯è¿­ä»£å¯¹è±¡ (å¦‚ Mongo cursor)
        max_workers (int): æœ€å¤§çº¿ç¨‹æ•°
        rate_limit (None | int | tuple):
            - None: ä¸é™é€Ÿ
            - int: æ¯ç§’æœ€å¤šæ‰§è¡Œå¤šå°‘ä»»åŠ¡ï¼ˆå›ºå®šé€Ÿç‡ï¼‰
            - tuple(min, max): æ¯ç§’ä»»åŠ¡æ•°èŒƒå›´ï¼Œéšæœºé€‰æ‹©é€Ÿç‡
        retries (int): æ¯ä¸ªä»»åŠ¡å¤±è´¥åé‡è¯•æ¬¡æ•°
        retry_delay (int | float): é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        batch_size (None | int): æ¯æ‰¹ä»»åŠ¡çš„æ•°é‡ï¼ˆNone è¡¨ç¤ºä¸åˆ†æ‰¹ï¼‰
        batch_delay (int | float): æ¯æ‰¹ä¹‹é—´çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        list: æŒ‰è¾“å…¥é¡ºåºå¯¹åº”çš„ç»“æœåˆ—è¡¨ï¼Œå¤±è´¥æ—¶è¿”å› Exception
    """
    datas = list(datas)
    total = len(datas)
    results = [None] * total
    lock = threading.Lock()
    last_time = [0.0]
    finished = 0

    def wrapper(data):
        if rate_limit:
            with lock:
                if isinstance(rate_limit, tuple):
                    rate = random.uniform(*rate_limit)
                else:
                    rate = rate_limit
                interval = 1.0 / rate
                elapsed = time.time() - last_time[0]
                if elapsed < interval:
                    time.sleep(interval - elapsed)
                last_time[0] = time.time()


        for attempt in range(1, retries + 1):
            try:
                return func(data)
            except Exception as e:
                if attempt < retries:
                    logger.warning(f"ä»»åŠ¡å¤±è´¥ï¼Œé‡è¯• {attempt}/{retries} æ¬¡åç»§ç»­: {e}")
                    time.sleep(retry_delay * attempt)
                else:
                    logger.error(f"ä»»åŠ¡æœ€ç»ˆå¤±è´¥: {e}")
                    return e

    for start in range(0, total, batch_size or total):
        end = min(start + (batch_size or total), total)
        batch = datas[start:end]

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_index = {executor.submit(wrapper, d): i for i, d in enumerate(batch, start)}
            for future in as_completed(future_to_index):
                idx = future_to_index[future]
                results[idx] = future.result()
                finished += 1
                logger.info(f"è¿›åº¦: {finished}/{total} ({finished/total:.0%})")

        if end < total and batch_delay > 0:
            logger.info(f"æ‰¹æ¬¡å®Œæˆ {end}/{total}ï¼Œç­‰å¾… {batch_delay}s å†ç»§ç»­...")
            time.sleep(batch_delay)

    return results



def is_content_page(url, base_domain):
    parsed_url = urlparse(url)

    domain = urlparse(base_domain).netloc
    # **æ£€æŸ¥æ˜¯å¦ä¸ºç›¸åŒåŸŸå**
    if parsed_url.netloc != domain:
        return None

    path = parsed_url.path
    query = parsed_url.query

    # è§„åˆ™1: ç»Ÿè®¡ "/" çš„æ•°é‡
    slash_count = path.count("/")

    # è§„åˆ™2: åˆ¤æ–­æ˜¯å¦æœ‰æ–‡ä»¶åç¼€ï¼ˆå¦‚ .html, .phpï¼‰
    has_extension = bool(re.search(r'\.(html|php|asp|aspx|jsp|htm|shtml)$', path, re.IGNORECASE))

    # è§„åˆ™3: æ˜¯å¦åŒ…å«æ•°å­— IDï¼ˆå¸¸è§äºå†…å®¹é¡µï¼‰
    has_numeric_id = bool(re.search(r'/\d{4,}', path)) or bool(re.search(r'id=\d+', query))

    # è§„åˆ™4: åˆ—è¡¨é¡µé€šå¸¸çŸ­ä¸”ä»¥ "/" ç»“å°¾
    is_list_url = not has_extension and (path.endswith("/") or slash_count < 3)

    # è§„åˆ™5: åˆ—è¡¨é¡µå¯èƒ½åŒ…å«å…³é”®è¯
    list_keywords = ["category", "list", "news", "page", "archives", "tags"]
    contains_list_keyword = any(kw in path.lower() for kw in list_keywords)




    # **åˆ¤æ–­é€»è¾‘**ï¼šå¦‚æœç¬¦åˆå¤šä¸ªâ€œå†…å®¹é¡µâ€ç‰¹å¾ï¼Œåˆ™åˆ¤æ–­ä¸ºå†…å®¹é¡µ
    if has_extension or has_numeric_id:
        return url, True  # å¯èƒ½æ˜¯å†…å®¹é¡µ
    if is_list_url or contains_list_keyword:
        return url, False  # å¯èƒ½æ˜¯åˆ—è¡¨é¡µ

    # **é»˜è®¤æƒ…å†µ**ï¼šå¦‚æœ `/` å¾ˆå¤šï¼Œä¸”æ²¡æœ‰åç¼€ï¼Œå¯èƒ½æ˜¯å†…å®¹é¡µ
    return url, False


def categorize_urls(urls, base_domain):
    """åˆ†ç±»URLä¸ºå†…å®¹é¡µå’Œåˆ—è¡¨é¡µ"""
    content_page, list_page = [], []
    for url in urls:
        result = is_content_page(url, base_domain)
        if result:
            if result[1]==True:
                content_page.append(result[0])
            elif result[1]==False:
                list_page.append(result[0])
    return  content_page,list_page





class URLFilter:
    def __init__(self, capacity=1000000, error_rate=0.001, file_path='url_filter.pkl'):
        """
        åˆå§‹åŒ–URLè¿‡æ»¤å™¨
        :param capacity: å¸ƒéš†è¿‡æ»¤å™¨å®¹é‡
        :param error_rate: è¯¯åˆ¤ç‡
        :param file_path: è¿‡æ»¤å™¨ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        self.file_path = file_path
        self.capacity = capacity
        self.error_rate = error_rate
        self.bf = self.load_filter()

    def load_filter(self):
        """
        ä»æ–‡ä»¶åŠ è½½è¿‡æ»¤å™¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºåˆ™æ–°å»ºä¸€ä¸ªå¹¶ä¿å­˜åˆ°æ–‡ä»¶
        :return: åŠ è½½çš„å¸ƒéš†è¿‡æ»¤å™¨
        """
        if not os.path.exists(self.file_path) or os.path.getsize(self.file_path) == 0:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™æ–°å»ºä¸€ä¸ªè¿‡æ»¤å™¨å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            print(f"æ–‡ä»¶ {self.file_path} ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ–°å»ºå¸ƒéš†è¿‡æ»¤å™¨å¹¶ä¿å­˜")
            bf = BloomFilter(capacity=self.capacity, error_rate=self.error_rate)
            self.save_filter(bf)  # ä¿å­˜æ–°å»ºçš„è¿‡æ»¤å™¨
            return bf

        # æ–‡ä»¶å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼ŒåŠ è½½è¿‡æ»¤å™¨
        with open(self.file_path, 'rb') as f:
            return pickle.load(f)

    def save_filter(self, bf=None):
        """
        å°†è¿‡æ»¤å™¨ä¿å­˜åˆ°æ–‡ä»¶
        :param bf: è¦ä¿å­˜çš„å¸ƒéš†è¿‡æ»¤å™¨ï¼Œé»˜è®¤ä¸º self.bf
        """
        if bf is None:
            bf = self.bf
        with open(self.file_path, 'wb') as f:
            pickle.dump(bf, f)

    def is_url_new(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦ä¸ºæ–°URL
        :param url: è¦æ£€æŸ¥çš„URL
        :return: Trueè¡¨ç¤ºæ˜¯æ–°URLï¼ŒFalseè¡¨ç¤ºå·²å­˜åœ¨
        """
        if url not in self.bf:
            self.bf.add(url)
            self.save_filter()  # æ¯æ¬¡æ·»åŠ æ–°URLåä¿å­˜è¿‡æ»¤å™¨
            return True
        return False





def get_page_info(url, page_param=None, step=1, first_num=1, mode='direct', max_attempts=10, use_cache=False, cache_file=None, proxy=None,stop_max_attempt_number=3,sleep=1,has_index=False,custom_base_path=None):
    """
    è·å–é¡µé¢ä¿¡æ¯ï¼Œæ”¯æŒç›´æ¥è§£æå’ŒäºŒåˆ†æŸ¥æ‰¾ä¸¤ç§æ¨¡å¼
    :param url: åˆå§‹URL
    :param page_param: é¡µç å‚æ•°å
    :param step: é¡µç æ­¥é•¿,å¦‚æœé¡µç ä¸æ˜¯è¿ç»­çš„ï¼Œå¯ä»¥è®¾ç½®æ­¥é•¿
    :param first_num: èµ·å§‹é¡µç 
    :param mode: æ¨¡å¼é€‰æ‹©ï¼Œ'direct'ï¼ˆç›´æ¥è§£æï¼‰æˆ– 'binary'ï¼ˆäºŒåˆ†æŸ¥æ‰¾ï¼‰
    :param max_attempts: äºŒåˆ†æŸ¥æ‰¾æœ€å¤§å°è¯•æ¬¡æ•°
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤å…³é—­
    :param cache_file: ç¼“å­˜æ–‡ä»¶å
    :param proxy: ä»£ç†è®¾ç½®
    :return: é¡µé¢é“¾æ¥åˆ—è¡¨, æ€»é¡µæ•°
    :stop_max_attempt_number: é‡è¯•æ¬¡æ•°
    :sleep: é‡è¯•é—´éš”
    :has_index: æ˜¯å¦é¦–é¡µåŒ…æ‹¬é¡µç 

    """
    headers = {'user-agent': str(UserAgent().random)}
    if not cache_file:
        cache_file = 'total_pages_cache.json'

    def extract_base_url(url, page_param):
        if page_param.startswith('/'):
            pattern = rf'(.*{page_param})\d+'
        else:
            pattern = rf'(.*[?&]{page_param}=)\d+'
        match = re.search(pattern, url)
        return match.group(1) if match else url.split('?')[0]

    @retrying.retry(wait_fixed=2000, stop_max_attempt_number=3)
    def get_response(url, params=None, data=None, proxy=None):
        try:
            if data == None:
                if params == None:
                    response = requests.get(url, headers=headers, proxies=proxy, timeout=10)
                else:
                    response = requests.get(url, headers=headers, params=params, proxies=proxy, timeout=10)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    return response
            else:
                if params == None:
                    response = requests.post(url, headers=headers, data=data, proxies=proxy, timeout=10)
                else:
                    response = requests.post(url, headers=headers, data=data, params=params, proxies=proxy, timeout=10)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    return response
        except Exception as e:
            logger.info(f'æŠ“å–å¤±è´¥,é‡æ–°æŠ“å–ï¼š{url}.{e}')
            raise

    # ç¼“å­˜å¤„ç†
    def load_cache():
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_cache(cache):
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=4)

    # æ£€æŸ¥ç¼“å­˜
    if use_cache:
        cache = load_cache()
        base_url=extract_base_url(url, page_param)
        cached_data = cache.get(base_url) or cache.get(url)
        if cached_data:
            logger.success(f"ä»ç¼“å­˜ä¸­è¯»å–: \nbase_url={base_url}\næ€»é¡µæ•°={cached_data['total_pages']}\né“¾æ¥ï¼š{cached_data['all_pages_link'][:3]}...")
            return cached_data['all_pages_link'],cached_data['total_pages'],
        else:
            logger.info("ç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®")

    if mode == 'direct':
        # ç›´æ¥è§£ææ¨¡å¼
        response =get_response(url,proxy=proxy).text

        body = etree.HTML(response)
        if custom_base_path:
            page_url=body.xpath(custom_base_path)
        else:
            page_url = body.xpath("//a[text()='å°¾é¡µ' or text()='æœ«é¡µ' or text()='æœ€åä¸€é¡µ' or text()='å°¾ é¡µ' or text()='æœªé¡µ']/@href")

        if not page_url:
            logger.info("æœªæ‰¾åˆ°å°¾é¡µé“¾æ¥")
            return [url], 1

        full_url = urljoin(url, page_url[0])
        # å¤„ç†ä¸åŒå‚æ•°æ¨¡å¼
        if page_param:
            pattern = rf'(.*{page_param})(\d+)(\.[a-zA-Z0-9]+)?' if page_param.startswith(
                '/') else rf'(.*[?&]{page_param}=)(\d+)(\.[a-zA-Z0-9]+)?'
            match = re.search(pattern, full_url)
            if match:
                base_path = match.group(1)  # åŸºç¡€è·¯å¾„
                last_page_number = match.group(2)  # å½“å‰é¡µç 
                extension = match.group(3) or ''  # æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚ .htmlã€.php ç­‰ï¼‰

                # ç”Ÿæˆæ‰€æœ‰åˆ†é¡µé“¾æ¥
                all_pages_link = [
                    f"{base_path}{(int(last_page_number) - i) * step + first_num}{extension}"
                    for i in range(int(last_page_number) - 1, -1, -1)
                ]
                all_pages_link.append(url)
                logger.success(f"åŸºæœ¬è·¯å¾„: {base_path}, é¡µç : {last_page_number}, æ‰©å±•å: {extension}")
                logger.info(f"æ‰€æœ‰åˆ†é¡µé“¾æ¥: {all_pages_link[:3]}...")
                return all_pages_link, last_page_number

        # é»˜è®¤å¤„ç†é€»è¾‘
        match = re.match(r'(.*/)([^/]+?)_?(\d+)?_(\d+)(\.html)?', full_url)
        if match:
            # æå–åŸºç¡€è·¯å¾„ã€ä¸»ç¼–å·å’Œå½“å‰é¡µç 
            base_path = f"{match.group(1)}{match.group(2)}_{match.group(3)}_{{}}{match.group(5) or '.html'}"
            current_page_number = int(match.group(4))  # é¡µç 
            main_id = match.group(3)  # ä¸»ç¼–å·

            # ç”Ÿæˆæ‰€æœ‰åˆ†é¡µé“¾æ¥
            if has_index:
                all_pages_link = [
                    base_path.format(page) for page in range(1, current_page_number + 1)
                ]
            else:
                all_pages_link = [
                    base_path.format(page) for page in range(2, current_page_number + 1)
                ]
                all_pages_link.append(full_url)

            logger.success(f"\nâœ… åŸºæœ¬è·¯å¾„: {base_path}, ä¸»ç¼–å·: {main_id}, é¡µç : {current_page_number}")
            logger.info(f"\nâœ…æ‰€æœ‰åˆ†é¡µé“¾æ¥é¢„è§ˆ: {all_pages_link[:3]}...")
            if use_cache:
                cache = load_cache()
                cache['base_url'] = {
                    'total_pages': current_page_number,
                    'all_pages_link': all_pages_link
                }
                save_cache(cache)
                logger.success(f"ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")

            return all_pages_link, current_page_number
        else:
            logger.error("âŒ æ— æ³•åŒ¹é…é“¾æ¥æ ¼å¼")
            return [url], 1

    else:
        def is_page_valid(page_url):
            try:
                response = requests.get(page_url, headers=headers, proxies=proxy, timeout=10)
                logger.info(f"ğŸ“Šæ˜¯å¦æ˜¯æœ€åä¸€é¡µå‘¢?: {page_url}")
                return response.status_code == 200
            except requests.RequestException:
                return False
        def get_page_url(base_url, page_num, page_param):
            pattern = rf'(.*{page_param})(\d+)(\.[a-zA-Z0-9]+)?' if page_param.startswith(
                '/') else rf'(.*[?&]{page_param}=)(\d+)(\.[a-zA-Z0-9]+)?'
            match = re.search(pattern, base_url)
            if match:
                base_path = match.group(1)  # åŸºç¡€è·¯å¾„
                last_page_number = match.group(2)  # å½“å‰é¡µç 
                extension = match.group(3) or ''  # æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚ .htmlã€.php ç­‰ï¼‰
                return f'{base_path}{page_num}{extension}'
            else:
                logger.error(f'æå–é”™è¯¯')
                return []

        # æå–base_url
        base_url = extract_base_url(url, page_param)

        # åŠ¨æ€æ‰©å±•èŒƒå›´
        left, right = 1, 100
        attempts = 0
        while attempts < max_attempts:
            attempts += 1
            if is_page_valid(get_page_url(url, right, page_param)):
                left = right
                right *= 2
            else:
                break

        # äºŒåˆ†æŸ¥æ‰¾
        while left < right:
            mid = (left + right) // 2
            if is_page_valid(get_page_url(url, mid, page_param)):
                left = mid + 1
            else:
                right = mid
            time.sleep(sleep)

        total_pages = left - 1
        all_pages_link = [get_page_url(url, i, page_param) for i in range(1, total_pages + 1)]
        if use_cache:
            cache = load_cache()
            cache[f'{base_url}'] = {
                'total_pages': total_pages,
                'all_pages_link': all_pages_link
            }
            save_cache(cache)
            logger.success(f"ğŸ‰æœ€åä¸€é¡µ:{get_page_url(base_url, right, page_param)}")
            logger.success(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}ï¼Œæ€»é¡µæ•°: {total_pages}")


        return all_pages_link, total_pages









def get_links(url, xpath=None, proxy=None, article_nums=None, required_fields=None, base_url=None, regex=None,html=None,use_drission=False,sleep=0):
    def filter_by_fields(urls, required_fields, regex):
        """
        æ ¹æ®å¿…é¡»åŒ…å«çš„å­—æ®µå’Œæ­£åˆ™ç­›é€‰ URL
        :param urls: URL åˆ—è¡¨
        :param required_fields: å¿…é¡»åŒ…å«çš„å­—æ®µåˆ—è¡¨
        :param regex: æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¦‚æœä¸ä¸ºç©ºï¼Œåˆ™æ ¹æ®æ­£åˆ™ç­›é€‰ URL
        :return: ç­›é€‰åçš„ URL åˆ—è¡¨
        """
        if not required_fields and not regex:
            return urls

        filtered_urls = []

        for url in urls:
            # æ ¹æ®å­—æ®µç­›é€‰
            if required_fields and any(field in url for field in required_fields):
                filtered_urls.append(url)
            # æ ¹æ®æ­£åˆ™ç­›é€‰
            elif regex and re.match(regex, url):
                filtered_urls.append(url)

        return filtered_urls

    """
    æå–å†…å®¹é¡µ URL
    :param html: ç½‘é¡µ HTML å†…å®¹
    :param xpath_list: XPath è¡¨è¾¾å¼
    :param article_nums: æ§åˆ¶ç›¸ä¼¼ URL æ•°é‡
    :param required_fields: å¿…é¡»åŒ…å«çš„å­—æ®µåˆ—è¡¨ï¼ˆå¦‚ ['news', 'detail']ï¼‰
    :param regex: æ­£åˆ™è¡¨è¾¾å¼ç­›é€‰ URL
    :return: ç­›é€‰åçš„ URL åˆ—è¡¨
    """

    @retrying.retry(wait_fixed=2000, stop_max_attempt_number=3)
    def get_response(url, params=None, data=None, proxy=None):
        try:
            if data is None:
                if params is None:
                    response = requests.get(url, headers=headers, proxies=proxy, timeout=10, verify=False)
                else:
                    response = requests.get(url, headers=headers, params=params, proxies=proxy, timeout=10)
                if '20' in f'{response.status_code}':
                    response.encoding = response.apparent_encoding
                    return response
            else:
                if params is None:
                    response = requests.post(url, headers=headers, data=data, proxies=proxy, timeout=10)
                else:
                    response = requests.post(url, headers=headers, data=data, params=params, proxies=proxy, timeout=10)
                if '20' in f'{response.status_code}':
                    response.encoding = response.apparent_encoding
                    return response
        except Exception as e:
            logger.info(f'æŠ“å–å¤±è´¥,é‡æ–°æŠ“å–ï¼š{url}')
            logger.info(f'error:{e}')
            raise

    try:
        lx = LxParse()
        if html!=None:
            response=html
        elif use_drission==True:
            logger.info(f'ä½¿ç”¨DrissionPage')
            drission_options = {'proxy': proxy['http']} if proxy else {}
            page = WebPage(**drission_options)
            page.get(url)
            time.sleep(sleep)
            response = page.html
        else:
            response = get_response(url, proxy=proxy)
            if response is None:
                logger.error(f'æŠ“å–å¤±è´¥ï¼Œè¿”å› None: {url}')
                return []
            else:
                response = response.text

        if article_nums is not None:
            detail_url_list = lx.parse_list(response, article_nums=article_nums, xpath_list=xpath)
        elif xpath:
            detail_url_list = lx.parse_list(response, xpath_list=xpath)
        else:
            detail_url_list = lx.parse_list(response)

        if base_url:
            urls = [urljoin(base_url, detail_url) for detail_url in detail_url_list]
        else:
            urls = [urljoin(url, detail_url) for detail_url in detail_url_list]

        if required_fields or regex:
            urls = filter_by_fields(urls, required_fields, regex)

        if len(urls) > 0:
            logger.success(f"url:{url}ï¼›è§£æå‡ºé“¾æ¥{len(urls)}æ¡")
            return list(set(urls))
        else:
            logger.error(f"url:{url}ï¼›æœªè§£æåˆ°é“¾æ¥,å¯ä¼ å…¥xpath")
            return []

    except Exception as e:
        logger.error(f'è§£æå¤±è´¥:{e}')
        pass



@retrying.retry(wait_fixed=1000, stop_max_attempt_number=3)
def get_article(url, proxy=None, parsing_mode='lx', headers=headers, Filter=False,url_filter=None,use_drission=False,xpath_item=None,sleep=0):
    def clean_text_bs(html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text()

    def clean_text_regex(text):
        return re.sub(r'\s+', ' ', text).strip()


    try:

        if use_drission:
            drission_options = {'proxy': proxy['http']} if proxy else {}
            tab = WebPage(**drission_options)
            tab.get(url)
            time.sleep(sleep)
            response=tab.html
        else:
            response = requests.get(url, headers=headers, proxies=proxy, timeout=5, verify=False)
            response.encoding = response.apparent_encoding
            response = response.text
        dit = {}
        if Filter and response.status_code == 200 and not url_filter.is_url_new(url):
            logger.info(f'è¯·æ±‚å·²å­˜åœ¨ï¼Œè·³è¿‡: {url}')
            return

        extractor = GeneralNewsExtractor()
        result_gne = extractor.extract(response)
        lx = LxParse()

        result = lx.parse_detail(response)
        _id = hashlib.md5(url.encode('utf-8')).hexdigest()
        dit['_id'] = _id
        dit['url'] = url
        dit['title'] = result.get('title', '').strip()
        if parsing_mode == 'lx':
            dit['content'] = clean_text_regex(result.get('content_format', ''))
        elif parsing_mode == 'gne':
            dit['content'] = clean_text_bs(clean_text_regex(result_gne.get('content', '')))
        elif parsing_mode=='xpath' and xpath_item:
            dit['title'] = clean_text_regex(lx.parse_detail(response, xpath_item['xpath_title']))
            dit['content'] = clean_text_regex(lx.parse_detail(response, xpath_item['xpath_content']))
        dit['updateTime'] = str(datetime.now())[:19]
        dit['addDateTime'] = str(datetime.now())[:19]
        dit['publish_time'] = find_date(response)
        if len(dit['content']) > 0:
            return dit
        else:
            logger.error(f'å†…å®¹ä¸ºç©º: {url}')
            return None
    except Exception as e:
        logger.error(f'é‡æ–°æŠ“å–ï¼š{url} ï¼Œå‡ºé”™äº†')
        logger.error(f'error:{e}')
        raise
