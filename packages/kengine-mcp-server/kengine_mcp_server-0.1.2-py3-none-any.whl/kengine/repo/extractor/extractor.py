"""
ä¸»æå–å™¨

ä»£ç åº“ç«¯ç‚¹æå–å™¨çš„ä¸»è¦å®ç°ï¼Œè´Ÿè´£åè°ƒå„ä¸ªç»„ä»¶å®Œæˆç«¯ç‚¹æå–ä»»åŠ¡ã€‚
"""

import os
import re
import time
import hashlib
import threading
import logging
from pathlib import Path
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from .models import EndpointCandidate
from .rule_engine import LanguageRuleEngine
from ...code.skeleton import extract_skeleton, validate_file_for_skeleton_extraction

logger = logging.getLogger(__name__)


class RepoEndpointExtractor:
    """ä»£ç åº“ç«¯ç‚¹æå–å™¨ä¸»ç±»"""
    
    def __init__(self, max_workers: int = 4, enable_cache: bool = True):
        self.rule_engine = LanguageRuleEngine()
        self.max_workers = max_workers
        self.enable_cache = enable_cache
        self.file_cache = {}
        self.cache_lock = threading.Lock()
    
    def _get_file_hash(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶å†…å®¹å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        try:
            stat = os.stat(file_path)
            return hashlib.md5(f"{file_path}:{stat.st_mtime}:{stat.st_size}".encode()).hexdigest()
        except:
            return ""
    
    def _collect_source_files(self, repo_path: str) -> List[str]:
        """æ”¶é›†ä»£ç åº“ä¸­çš„æºä»£ç æ–‡ä»¶"""
        source_files = []
        supported_extensions = {'.java', '.py', '.js', '.ts', '.go', '.cs', '.php'}
        
        try:
            for root, dirs, files in os.walk(repo_path):
                # è·å–å½“å‰ç›®å½•çš„æœ«çº§è·¯å¾„å
                base_dir_name = os.path.basename(root).lower()
                
                # è·³è¿‡å¸¸è§çš„éæºç ç›®å½•
                dirs[:] = [d for d in dirs if not d.startswith('.') and
                          d not in {'node_modules', 'target', 'build', 'dist', '__pycache__', 'vendor'}]
                
                for file in files:
                    if Path(file).suffix.lower() in supported_extensions:
                        file_path = os.path.join(root, file)
                        
                        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ˜¯æµ‹è¯•æ–‡ä»¶ (æ›´ç²¾ç¡®çš„åŒ¹é…)
                        # æ’é™¤ tests/ ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œæˆ–è€…æ–‡ä»¶åä»¥ test_ å¼€å¤´æˆ– _test ç»“å°¾çš„æ–‡ä»¶
                        if (re.search(r'/tests?/', file_path, re.IGNORECASE) or
                            re.search(r'[/\\]?test_[^/\\]*\.[^/\\]+$', file_path, re.IGNORECASE) or
                            re.search(r'[/\\]?[^/\\]*_test\.[^/\\]+$', file_path, re.IGNORECASE)):
                            logger.debug(f"æ’é™¤æµ‹è¯•ç›¸å…³æ–‡ä»¶: {file_path}")
                            continue
                            
                        source_files.append(file_path)
        
        except Exception as e:
            logger.error(f"æ”¶é›†æºæ–‡ä»¶æ—¶å‡ºé”™: {e} at {repo_path}")
        
        return source_files
    
    def _process_file_batch(self, files: List[str]) -> List[EndpointCandidate]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
        candidates = []
        
        for file_path in files:
            try:
                # æ£€æŸ¥ç¼“å­˜
                if self.enable_cache:
                    file_hash = self._get_file_hash(file_path)
                    with self.cache_lock:
                        if file_hash in self.file_cache:
                            cached_result = self.file_cache[file_hash]
                            if cached_result:
                                candidates.append(cached_result)
                            continue
                
                # è¯„ä¼°æ–‡ä»¶
                candidate = self.rule_engine.evaluate_file(file_path)
                
                # æ›´æ–°ç¼“å­˜
                if self.enable_cache and file_hash:
                    with self.cache_lock:
                        self.file_cache[file_hash] = candidate
                
                if candidate:
                    candidates.append(candidate)
                    
            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
        
        return candidates
    
    def _extract_skeletons_for_candidates(self, candidates: List[EndpointCandidate]) -> List[EndpointCandidate]:
        """ä¸ºå€™é€‰é¡¹æå–è¯¦ç»†çš„ä»£ç éª¨æ¶ä¿¡æ¯"""
        def extract_skeleton_info(candidate: EndpointCandidate) -> EndpointCandidate:
            try:
                # éªŒè¯æ–‡ä»¶æ˜¯å¦æ”¯æŒéª¨æ¶æå–
                is_valid, validation_msg = validate_file_for_skeleton_extraction(candidate.file_path)
                if is_valid:
                    skeleton = extract_skeleton(candidate.file_path)
                    candidate.skeleton = skeleton
                    
                    # æå–ç«¯ç‚¹æ¨¡å¼
                    endpoint_patterns = self.rule_engine.annotation_detector.extract_endpoint_patterns(
                        candidate.file_path, candidate.language
                    )
                    if endpoint_patterns:
                        # å°†ç«¯ç‚¹æ¨¡å¼æ·»åŠ åˆ°éª¨æ¶ä¿¡æ¯ä¸­
                        candidate.skeleton += f"\n\n# ç«¯ç‚¹æ¨¡å¼:\n# {', '.join(endpoint_patterns)}"
                else:
                    logger.warning(f"æ–‡ä»¶éªŒè¯å¤±è´¥ {candidate.file_path}: {validation_msg}")
                        
            except Exception as e:
                logger.warning(f"æå–éª¨æ¶æ—¶å‡ºé”™ {candidate.file_path}: {e}")
            
            return candidate
        
        # å¹¶è¡Œæå–éª¨æ¶ä¿¡æ¯
        with ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix='repo-extractor-') as executor:
            future_to_candidate = {executor.submit(extract_skeleton_info, candidate): candidate 
                                 for candidate in candidates}
            
            results = []
            for future in as_completed(future_to_candidate):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.warning(f"å¹¶è¡Œæå–éª¨æ¶æ—¶å‡ºé”™: {e}")
        
        return results
    
    def extract_endpoints(self, repo_path: str) -> Dict[str, Any]:
        """æå–ä»£ç åº“ç«¯ç‚¹å®šä¹‰"""
        start_time = time.time()
        
        if not os.path.exists(repo_path):
            error_msg = f'ä»£ç åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}'
            logger.error(error_msg)
            return {
                'error': error_msg,
                'timestamp': time.time()
            }
            
        # 1. æ”¶é›†æºæ–‡ä»¶
        logger.info(f"ğŸ” æ‰«æä»£ç åº“: {repo_path}")
        source_files = self._collect_source_files(repo_path)
        logger.info(f"ğŸ“ å‘ç° {len(source_files)} ä¸ªæºæ–‡ä»¶")
        
        if not source_files:
            logger.warning("æœªå‘ç°ä»»ä½•æºæ–‡ä»¶")
            return {
                'repo_path': repo_path,
                'total_files': 0,
                'candidates': [],
                'summary': {'total_candidates': 0, 'languages': {}},
                'processing_time': time.time() - start_time,
                'timestamp': time.time()
            }
        
        # 2. å¹¶è¡Œè¯„ä¼°æ–‡ä»¶
        logger.info("ğŸ” è¯„ä¼°ç«¯ç‚¹å€™é€‰æ–‡ä»¶...")
        candidates = []
        
        # å°†æ–‡ä»¶åˆ†æ‰¹å¤„ç†
        batch_size = max(1, len(source_files) // self.max_workers)
        file_batches = [source_files[i:i + batch_size] 
                        for i in range(0, len(source_files), batch_size)]
        
        with ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix='repo-extractor-endpoints-') as executor:
            future_to_batch = {executor.submit(self._process_file_batch, batch): batch 
                                for batch in file_batches}
            
            for future in as_completed(future_to_batch):
                try:
                    batch_candidates = future.result()
                    candidates.extend(batch_candidates)
                except Exception as e:
                    logger.warning(f"æ‰¹é‡å¤„ç†æ—¶å‡ºé”™: {e}")
        
        logger.info(f"âœ… å‘ç° {len(candidates)} ä¸ªç«¯ç‚¹å€™é€‰æ–‡ä»¶")
        
        # 3. æŒ‰ç½®ä¿¡åº¦æ’åº
        candidates.sort(key=lambda x: x.confidence_score, reverse=True)
        
        # 4. æå–è¯¦ç»†éª¨æ¶ä¿¡æ¯
        if candidates:
            logger.info("ğŸ“ æå–è¯¦ç»†ä»£ç éª¨æ¶...")
            candidates = self._extract_skeletons_for_candidates(candidates)
            
            # 5. æŒ‰ æ–‡æœ¬ç›¸ä¼¼åº¦ å»é‡
            from kengine.utils.similarity_calc import deduplicate_by_text_similarity
            candidates = deduplicate_by_text_similarity(candidates, lambda o : o.skeleton, 0.95)
            
            # 6. å¦‚æœå€™é€‰é¡¹æ•°é‡è¶…è¿‡æ–‡ä»¶æ€»æ•°çš„ 1/5ï¼Œ åªå– 1/5, å¦‚æœå¤§äº100åªå–100ä¸ª
            # å¯¹äºå°é¡¹ç›®ï¼ˆ<10ä¸ªæ–‡ä»¶ï¼‰ï¼Œä¿ç•™æ‰€æœ‰å€™é€‰æ–‡ä»¶
            if len(source_files) < 10:
                max_candidates_count = min(100, len(candidates))
            else:
                max_candidates_count = min(100, max(1, int(len(source_files) / 5)))
            
            if len(candidates) > max_candidates_count:
                logger.info(f'å€™é€‰æ–‡ä»¶æ•°é‡ {len(candidates)} > {len(source_files)}/5 or 100, ä¿ç•™ {max_candidates_count} ä¸ª')
                candidates = candidates[:max_candidates_count]
        
        
        processing_time = time.time() - start_time
        logger.info(f"â±ï¸  å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
        
        return {
            'repo_path': repo_path,
            'total_files': len(source_files),
            'candidates': [self._candidate_to_dict(c) for c in candidates],
            'processing_time': processing_time,
            'timestamp': time.time()
        }
    
    def _candidate_to_dict(self, candidate: EndpointCandidate) -> Dict[str, Any]:
        """å°†å€™é€‰é¡¹è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'file_path': candidate.file_path,
            'language': candidate.language,
            'confidence_score': candidate.confidence_score,
            'match_reasons': candidate.match_reasons,
            'framework': candidate.framework,
            'skeleton': candidate.skeleton
        }