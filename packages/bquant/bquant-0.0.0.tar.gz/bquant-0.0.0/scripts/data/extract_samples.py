#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è sample –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö CSV —Ñ–∞–π–ª–æ–≤
–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedded Python —Å—Ç—Ä—É–∫—Ç—É—Ä.

Usage:
    python scripts/data/extract_samples.py --extract-all
    python scripts/data/extract_samples.py --dataset tv_xauusd_1h --source "path/to/source.csv"
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from bquant.core.logging_config import get_logger

logger = get_logger(__name__)


class SampleDataExtractor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è sample –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö CSV —Ñ–∞–π–ª–æ–≤
    –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedded Python —Å—Ç—Ä—É–∫—Ç—É—Ä.
    """
    
    def __init__(self):
        self.logger = get_logger(f"{__name__}.SampleDataExtractor")
        self.project_root = project_root
        self.samples_dir = self.project_root / "bquant" / "data" / "samples"
        self.embedded_dir = self.samples_dir / "embedded"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        self.data_sources = {
            'tv_xauusd_1h': {
                'name': 'TradingView XAUUSD 1H',
                'description': '–ß–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ XAUUSD —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏',
                'source': 'TradingView via OANDA',
                'symbol': 'XAUUSD',
                'timeframe': '1H',
                'source_file': r'C:\Users\Ivan\YandexDisk\pro\quant\data\OANDA_XAUUSD, 60.csv',
                'rows': 1000,
                'license': 'Open data, free for research and educational use',
                'disclaimer': 'For demonstration purposes only. Not for production trading.'
            },
            'mt_xauusd_m15': {
                'name': 'MetaTrader XAUUSD 15M',
                'description': '15-–º–∏–Ω—É—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ XAUUSD —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏',
                'source': 'MetaTrader',
                'symbol': 'XAUUSD',
                'timeframe': '15M',
                'source_file': r'C:\Users\Ivan\YandexDisk\pro\quant\data\alldata\XAUUSDM15.csv',
                'rows': 1000,
                'license': 'Open data, free for research and educational use',
                'disclaimer': 'For demonstration purposes only. Not for production trading.'
            }
        }
    
    def extract_sample_data(self, dataset_name: str, custom_source: Optional[str] = None) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ—á—å sample –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞.
        
        Args:
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ ('tv_xauusd_1h' –∏–ª–∏ 'mt_xauusd_m15')
            custom_source: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—É—Ç—å –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if dataset_name not in self.data_sources:
            raise ValueError(f"Unknown dataset: {dataset_name}. Available: {list(self.data_sources.keys())}")
        
        config = self.data_sources[dataset_name]
        source_file = custom_source or config['source_file']
        
        self.logger.info(f"Extracting sample data for {dataset_name} from {source_file}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if not os.path.exists(source_file):
            raise FileNotFoundError(f"Source file not found: {source_file}")
        
        # –ß–∏—Ç–∞–µ–º CSV —Ñ–∞–π–ª —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è UTF-8
            try:
                df = pd.read_csv(source_file, encoding='utf-8')
            except UnicodeDecodeError:
                # –ó–∞—Ç–µ–º –ø—ã—Ç–∞–µ–º—Å—è UTF-16 (–¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö MetaTrader —Ñ–∞–π–ª–æ–≤)
                try:
                    df = pd.read_csv(source_file, encoding='utf-16')
                    self.logger.info("Used utf-16 encoding")
                except UnicodeDecodeError:
                    # –ó–∞—Ç–µ–º –ø—ã—Ç–∞–µ–º—Å—è Windows-1251 (–¥–ª—è –¥—Ä—É–≥–∏—Ö MetaTrader —Ñ–∞–π–ª–æ–≤)
                    try:
                        df = pd.read_csv(source_file, encoding='windows-1251')
                        self.logger.info("Used windows-1251 encoding")
                    except UnicodeDecodeError:
                        # –ù–∞–∫–æ–Ω–µ—Ü –ø—ã—Ç–∞–µ–º—Å—è latin-1 (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞)
                        df = pd.read_csv(source_file, encoding='latin-1')
                        self.logger.info("Used latin-1 encoding")
            
            self.logger.info(f"Loaded {len(df)} rows from source file")
        except Exception as e:
            raise RuntimeError(f"Failed to read CSV file: {e}")
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å—Ç—Ä–æ–∫
        rows_to_extract = config['rows']
        if len(df) < rows_to_extract:
            self.logger.warning(f"Source has only {len(df)} rows, less than requested {rows_to_extract}")
            rows_to_extract = len(df)
        
        sample_df = df.tail(rows_to_extract).copy()
        self.logger.info(f"Extracted {len(sample_df)} rows")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ç–∏–ø–∏–∑–∞—Ü–∏–µ–π
        data_list = self._convert_to_typed_dicts(sample_df, dataset_name)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = self._create_metadata(config, sample_df, dataset_name)
        
        return {
            'metadata': metadata,
            'data': data_list,
            'original_rows': len(df),
            'extracted_rows': len(sample_df)
        }
    
    def _convert_to_typed_dicts(self, df: pd.DataFrame, dataset_name: str) -> List[Dict[str, Any]]:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å DataFrame –≤ —Å–ø–∏—Å–æ–∫ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π.
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        """
        self.logger.info(f"Converting {len(df)} rows to typed dictionaries")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        df_normalized = df.copy()
        df_normalized.columns = self._normalize_column_names(df.columns, dataset_name)
        
        data_list = []
        
        for idx, row in df_normalized.iterrows():
            record = {}
            
            for column, value in row.items():
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–∏–ø—ã
                typed_value = self._convert_value_type(value, column)
                record[column] = typed_value
            
            data_list.append(record)
        
        self.logger.info(f"Successfully converted {len(data_list)} records")
        return data_list
    
    def _normalize_column_names(self, columns: List[str], dataset_name: str) -> List[str]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è consistency.
        
        Args:
            columns: –ò—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        """
        normalized = []
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è MetaTrader —Ñ–∞–π–ª–æ–≤
        if dataset_name == 'mt_xauusd_m15':
            # MetaTrader —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: Date Time, Open, High, Low, Close, Volume, Spread
            if len(columns) >= 7:
                expected_columns = ['time', 'open', 'high', 'low', 'close', 'volume', 'spread']
                return expected_columns[:len(columns)]
        
        for col in columns:
            # –û–±—â–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            normalized_col = col.lower().strip()
            
            # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ–Ω—ã
            replacements = {
                'accumulation/distribution': 'accumulation_distribution',
                'rsi-based ma': 'rsi_based_ma',
                'regular bullish': 'regular_bullish',
                'regular bullish label': 'regular_bullish_label', 
                'regular bearish': 'regular_bearish',
                'regular bearish label': 'regular_bearish_label'
            }
            
            for old, new in replacements.items():
                if normalized_col == old:
                    normalized_col = new
                    break
            
            # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
            normalized_col = normalized_col.replace(' ', '_').replace('/', '_').replace('-', '_')
            
            normalized.append(normalized_col)
        
        return normalized
    
    def _convert_value_type(self, value: Any, column: str) -> Any:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∏–ø.
        
        Args:
            value: –ò—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        
        Returns:
            –¢–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        """
        # NaN –∏ None
        if pd.isna(value) or value is None:
            return None
        
        # –í—Ä–µ–º—è –æ—Å—Ç–∞–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π
        if column in ['time', 'timestamp', 'date']:
            return str(value)
        
        # –ß–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if isinstance(value, (int, float, np.integer, np.floating)):
            if np.isnan(value):
                return None
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float –¥–ª—è consistency
            return float(value)
        
        # –°—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if isinstance(value, str):
            # –ü—ã—Ç–∞–µ–º—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
            try:
                float_val = float(value)
                return float_val
            except (ValueError, TypeError):
                # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
                return str(value)
        
        # –û—Å—Ç–∞–ª—å–Ω–æ–µ –∫–∞–∫ –µ—Å—Ç—å
        return value
    
    def _create_metadata(self, config: Dict[str, Any], df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
        time_column = self._identify_time_column(df.columns)
        period_start = None
        period_end = None
        
        if time_column and len(df) > 0:
            try:
                period_start = str(df[time_column].iloc[0])
                period_end = str(df[time_column].iloc[-1])
            except Exception as e:
                self.logger.warning(f"Could not determine time period: {e}")
        
        metadata = {
            'name': config['name'],
            'description': config['description'],
            'source': config['source'],
            'symbol': config['symbol'],
            'timeframe': config['timeframe'],
            'rows': len(df),
            'columns': df.columns.tolist(),
            'period_start': period_start,
            'period_end': period_end,
            'license': config['license'],
            'disclaimer': config['disclaimer'],
            'updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'extracted_from': config['source_file']
        }
        
        return metadata
    
    def _identify_time_column(self, columns: List[str]) -> Optional[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
        
        Args:
            columns: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
        
        Returns:
            –ù–∞–∑–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ –∏–ª–∏ None
        """
        time_patterns = ['time', 'timestamp', 'date', 'datetime']
        
        for col in columns:
            if any(pattern in col.lower() for pattern in time_patterns):
                return col
        
        return None
    
    def generate_embedded_file(self, dataset_name: str, extracted_data: Dict[str, Any]) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Python —Ñ–∞–π–ª —Å embedded –¥–∞–Ω–Ω—ã–º–∏.
        
        Args:
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            extracted_data: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        self.logger.info(f"Generating embedded file for {dataset_name}")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.embedded_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        output_file = self.embedded_dir / f"{dataset_name}.py"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        file_content = self._generate_file_content(dataset_name, extracted_data)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(file_content)
        
        self.logger.info(f"Generated embedded file: {output_file}")
        self.logger.info(f"File size: {os.path.getsize(output_file)} bytes")
        
        return str(output_file)
    
    def _generate_file_content(self, dataset_name: str, extracted_data: Dict[str, Any]) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ Python —Ñ–∞–π–ª–∞ —Å embedded –¥–∞–Ω–Ω—ã–º–∏.
        
        Args:
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            extracted_data: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Returns:
            –°–æ–¥–µ—Ä–∂–∏–º–æ–µ Python —Ñ–∞–π–ª–∞
        """
        metadata = extracted_data['metadata']
        data = extracted_data['data']
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª–∞ (–±–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        source_filename = os.path.basename(metadata['extracted_from'])
        header = f'''"""
Embedded sample data for {metadata['name']}

Auto-generated from original {metadata['source']} dataset
Generated on: {metadata['updated']}
Rows: {metadata['rows']}
Source: {metadata['source']}

DO NOT EDIT THIS FILE MANUALLY!
Use scripts/data/extract_samples.py to regenerate.
"""

from typing import Dict, List, Any

'''
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—É–¥–∞–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏)
        metadata_safe = metadata.copy()
        if 'extracted_from' in metadata_safe:
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞, –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø—É—Ç–∏
            metadata_safe['extracted_from'] = os.path.basename(metadata_safe['extracted_from'])
        
        metadata_section = f'''DATASET_INFO = {repr(metadata_safe)}

'''
        
        # –î–∞–Ω–Ω—ã–µ (—Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏)
        data_header = "DATA = [\n"
        data_footer = "]\n"
        
        data_entries = []
        for record in data:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –∑–∞–ø–∏—Å—å
            formatted_record = "    {\n"
            for key, value in record.items():
                formatted_value = repr(value)
                formatted_record += f"        '{key}': {formatted_value},\n"
            formatted_record += "    },"
            data_entries.append(formatted_record)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏
        data_section = data_header + "\n".join(data_entries) + "\n" + data_footer
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        content = header + metadata_section + data_section
        
        return content
    
    def extract_all_datasets(self) -> Dict[str, str]:
        """
        –ò–∑–≤–ª–µ—á—å –≤—Å–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å: dataset_name -> –ø—É—Ç—å –∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        self.logger.info("Extracting all configured datasets")
        
        results = {}
        
        for dataset_name in self.data_sources.keys():
            try:
                self.logger.info(f"Processing dataset: {dataset_name}")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                extracted_data = self.extract_sample_data(dataset_name)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedded —Ñ–∞–π–ª
                output_file = self.generate_embedded_file(dataset_name, extracted_data)
                
                results[dataset_name] = output_file
                
                self.logger.info(f"Successfully processed {dataset_name}")
                
            except Exception as e:
                self.logger.error(f"Failed to process {dataset_name}: {e}")
                raise
        
        self.logger.info(f"Successfully extracted {len(results)} datasets")
        return results
    
    def validate_source_files(self) -> Dict[str, bool]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å: dataset_name -> —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        """
        self.logger.info("Validating source files")
        
        results = {}
        
        for dataset_name, config in self.data_sources.items():
            source_file = config['source_file']
            exists = os.path.exists(source_file)
            results[dataset_name] = exists
            
            if exists:
                size = os.path.getsize(source_file) / (1024 * 1024)  # MB
                self.logger.info(f"‚úì {dataset_name}: {source_file} ({size:.1f} MB)")
            else:
                self.logger.warning(f"‚úó {dataset_name}: {source_file} - file not found")
        
        return results


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI."""
    parser = argparse.ArgumentParser(
        description="Extract sample data from source CSV files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/data/extract_samples.py --extract-all
  python scripts/data/extract_samples.py --dataset tv_xauusd_1h
  python scripts/data/extract_samples.py --validate-sources
        """
    )
    
    parser.add_argument(
        '--extract-all',
        action='store_true',
        help='Extract all configured datasets'
    )
    
    parser.add_argument(
        '--dataset',
        type=str,
        help='Extract specific dataset (tv_xauusd_1h or mt_xauusd_m15)'
    )
    
    parser.add_argument(
        '--source',
        type=str,
        help='Custom source file path (use with --dataset)'
    )
    
    parser.add_argument(
        '--validate-sources',
        action='store_true',
        help='Validate that all source files exist'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.DEBUG)
    
    extractor = SampleDataExtractor()
    
    try:
        if args.validate_sources:
            print("Validating source files...")
            results = extractor.validate_source_files()
            
            all_valid = all(results.values())
            
            print("\nValidation Results:")
            for dataset, valid in results.items():
                status = "‚úì" if valid else "‚úó"
                print(f"  {status} {dataset}")
            
            if all_valid:
                print("\nüéâ All source files are available!")
                return 0
            else:
                print("\n‚ö†Ô∏è  Some source files are missing!")
                return 1
        
        elif args.extract_all:
            print("Extracting all datasets...")
            results = extractor.extract_all_datasets()
            
            print("\nExtraction Results:")
            for dataset, file_path in results.items():
                print(f"  ‚úì {dataset} -> {file_path}")
            
            print(f"\nüéâ Successfully extracted {len(results)} datasets!")
            return 0
        
        elif args.dataset:
            print(f"Extracting dataset: {args.dataset}")
            
            extracted_data = extractor.extract_sample_data(args.dataset, args.source)
            output_file = extractor.generate_embedded_file(args.dataset, extracted_data)
            
            print(f"\nExtracted {extracted_data['extracted_rows']} rows from {extracted_data['original_rows']} total")
            print(f"Generated: {output_file}")
            print("\nüéâ Extraction completed successfully!")
            return 0
        
        else:
            parser.print_help()
            return 1
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        logger.error(f"Extraction failed: {e}", exc_info=True)
        return 1


if __name__ == '__main__':
    sys.exit(main())
