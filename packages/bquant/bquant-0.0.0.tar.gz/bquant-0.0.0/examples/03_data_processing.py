#!/usr/bin/env python3
"""
BQuant - –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏

–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
1. –ó–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (CSV, sample data)
2. –û–±—Ä–∞–±–æ—Ç–∫—É –∏ –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö
3. –í–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ö–µ–º
4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π BQuant –ø–∞–∫–µ—Ç: pip install -e .
- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–æ—Ä–º–∞—Ç–µ
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ BQuant –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from bquant.data import (
    create_sample_data, load_ohlcv_csv, get_available_timeframes,
    validate_ohlcv_data, clean_data, remove_outliers,
    resample_data, calculate_derived_indicators,
    create_lagged_features, normalize_data
)
from bquant.data.schemas import OHLCVRecord, DataSourceConfig, ValidationResult
from bquant.core.config import get_data_path, SUPPORTED_TIMEFRAMES


def demonstrate_data_creation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö."""
    
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö sample –¥–∞–Ω–Ω—ã—Ö
    print(f"\n1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ sample –¥–∞–Ω–Ω—ã—Ö:")
    
    sample_data = create_sample_data(
        symbol="XAUUSD",
        start_date="2024-01-01",
        end_date="2024-01-10",
        timeframe="1h"
    )
    
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(sample_data)} –±–∞—Ä–æ–≤ –¥–ª—è XAUUSD")
    print(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {sample_data.index[0]} - {sample_data.index[-1]}")
    print(f"   üí∞ –¶–µ–Ω–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: ${sample_data['close'].min():.2f} - ${sample_data['close'].max():.2f}")
    print(f"   üìä –ö–æ–ª–æ–Ω–∫–∏: {list(sample_data.columns)}")
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
    print(f"\n2Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ç—Ä–µ–Ω–¥–æ–º:")
    
    trending_data = create_trending_dataset(300, "EURUSD", trend_strength=0.002)
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(trending_data)} –±–∞—Ä–æ–≤ —Å —Ç—Ä–µ–Ω–¥–æ–º")
    
    volatile_data = create_volatile_dataset(200, "BTCUSD", volatility=0.05)
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(volatile_data)} –±–∞—Ä–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é")
    
    return sample_data, trending_data, volatile_data


def create_trending_dataset(rows: int, symbol: str, trend_strength: float = 0.001) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–º —Ç—Ä–µ–Ω–¥–æ–º."""
    
    dates = pd.date_range(start='2024-01-01', periods=rows, freq='4H')
    np.random.seed(42)
    
    if symbol == "EURUSD":
        base_price = 1.1000
    elif symbol == "BTCUSD":
        base_price = 50000
    else:
        base_price = 2000
    
    prices = [base_price]
    
    for i in range(1, rows):
        # –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥ + —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
        trend = trend_strength * i
        noise = np.random.normal(0, 0.005)
        change = trend + noise
        
        new_price = prices[-1] * (1 + change)
        prices.append(max(new_price, base_price * 0.1))
    
    # –°–æ–∑–¥–∞–µ–º OHLCV
    data = []
    for i, close_price in enumerate(prices):
        open_price = prices[i-1] if i > 0 else close_price
        high = max(open_price, close_price) * (1 + abs(np.random.normal(0, 0.003)))
        low = min(open_price, close_price) * (1 - abs(np.random.normal(0, 0.003)))
        volume = np.random.randint(10000, 100000)
        
        data.append({
            'open': open_price,
            'high': high,
            'low': low,
            'close': close_price,
            'volume': volume
        })
    
    return pd.DataFrame(data, index=dates)


def create_volatile_dataset(rows: int, symbol: str, volatility: float = 0.02) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é."""
    
    dates = pd.date_range(start='2024-01-01', periods=rows, freq='15min')
    np.random.seed(123)
    
    if symbol == "BTCUSD":
        base_price = 50000
    else:
        base_price = 2000
    
    prices = [base_price]
    
    for i in range(1, rows):
        # –í—ã—Å–æ–∫–∞—è —Å–ª—É—á–∞–π–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        change = np.random.normal(0, volatility)
        new_price = prices[-1] * (1 + change)
        prices.append(max(new_price, base_price * 0.1))
    
    # –°–æ–∑–¥–∞–µ–º OHLCV
    data = []
    for i, close_price in enumerate(prices):
        open_price = prices[i-1] if i > 0 else close_price
        high = max(open_price, close_price) * (1 + abs(np.random.normal(0, 0.01)))
        low = min(open_price, close_price) * (1 - abs(np.random.normal(0, 0.01)))
        volume = np.random.randint(50000, 500000)
        
        data.append({
            'open': open_price,
            'high': high,
            'low': low,
            'close': close_price,
            'volume': volume
        })
    
    return pd.DataFrame(data, index=dates)


def demonstrate_data_validation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""
    
    print(f"\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    
    # 1. –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    good_data = create_sample_data("EURUSD", "2024-01-01", "2024-01-05", "1h")
    
    print(f"\n1Ô∏è‚É£ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    validation_result = validate_ohlcv_data(good_data)
    
    print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {validation_result}")
    
    if isinstance(validation_result, dict):
        is_valid = validation_result.get('is_valid', False)
        print(f"   üìä –î–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã: {'–î–∞' if is_valid else '–ù–µ—Ç'}")
        
        if 'issues' in validation_result:
            issues = validation_result['issues']
            if issues:
                print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(issues)}")
                for issue in issues[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                    print(f"      ‚Ä¢ {issue}")
            else:
                print(f"   ‚úÖ –ü—Ä–æ–±–ª–µ–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 2. –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
    print(f"\n2Ô∏è‚É£ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏:")
    
    # –î–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
    bad_data = good_data.copy()
    bad_data.loc[bad_data.index[10:15], 'close'] = np.nan  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    bad_data.loc[bad_data.index[20], 'high'] = bad_data.loc[bad_data.index[20], 'low'] - 10  # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ü–µ–Ω–∞
    
    validation_result_bad = validate_ohlcv_data(bad_data)
    
    if isinstance(validation_result_bad, dict):
        is_valid = validation_result_bad.get('is_valid', False)
        print(f"   üìä –î–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã: {'–î–∞' if is_valid else '–ù–µ—Ç'}")
        
        if 'issues' in validation_result_bad:
            issues = validation_result_bad['issues']
            if issues:
                print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(issues)}")
                for issue in issues[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                    print(f"      ‚Ä¢ {issue}")


def demonstrate_data_cleaning():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
    
    print(f"\nüßπ –û—á–∏—Å—Ç–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
    dirty_data = create_volatile_dataset(150, "XAUUSD", volatility=0.03)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
    dirty_data.loc[dirty_data.index[10:15], 'volume'] = np.nan  # –ü—Ä–æ–ø—É—Å–∫–∏ –≤ –æ–±—ä–µ–º–µ
    dirty_data.loc[dirty_data.index[50], 'high'] = dirty_data.loc[dirty_data.index[50], 'close'] * 2  # –í—ã–±—Ä–æ—Å
    dirty_data.loc[dirty_data.index[80:85], 'close'] = np.nan  # –ü—Ä–æ–ø—É—Å–∫–∏ –≤ —Ü–µ–Ω–µ –∑–∞–∫—Ä—ã—Ç–∏—è
    
    print(f"\n1Ô∏è‚É£ –ò—Å—Ö–æ–¥–Ω—ã–µ '–≥—Ä—è–∑–Ω—ã–µ' –¥–∞–Ω–Ω—ã–µ:")
    print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(dirty_data)}")
    print(f"   üï≥Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö:")
    missing_info = dirty_data.isnull().sum()
    for col, missing_count in missing_info.items():
        if missing_count > 0:
            print(f"      {col}: {missing_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤")
    
    # 2. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n2Ô∏è‚É£ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö:")
    
    try:
        cleaned_data = clean_data(dirty_data)
        print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")
        print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_data)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        missing_after = cleaned_data.isnull().sum()
        total_missing = missing_after.sum()
        if total_missing == 0:
            print(f"   ‚úÖ –í—Å–µ –ø—Ä–æ–ø—É—Å–∫–∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã")
        else:
            print(f"   ‚ö†Ô∏è –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–ø—É—Å–∫–æ–≤: {total_missing}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
        cleaned_data = dirty_data  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    
    # 3. –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
    print(f"\n3Ô∏è‚É£ –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤:")
    
    try:
        outlier_free_data = remove_outliers(cleaned_data, method='iqr', columns=['close', 'high', 'low'])
        print(f"   ‚úÖ –í—ã–±—Ä–æ—Å—ã —É–¥–∞–ª–µ–Ω—ã")
        print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(cleaned_data)} ‚Üí {len(outlier_free_data)}")
        
        removed_count = len(cleaned_data) - len(outlier_free_data)
        if removed_count > 0:
            print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {removed_count}")
        else:
            print(f"   ‚úÖ –í—ã–±—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {e}")
        outlier_free_data = cleaned_data
    
    return outlier_free_data


def demonstrate_data_transformations():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""
    
    print(f"\nüîÑ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    base_data = create_sample_data("GBPUSD", "2024-01-01", "2024-01-15", "1h")
    
    print(f"\n1Ô∏è‚É£ –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"   üìä –†–∞–∑–º–µ—Ä: {base_data.shape}")
    print(f"   üìÖ Timeframe: 1h")
    print(f"   üìà –¶–µ–Ω–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {base_data['close'].min():.4f} - {base_data['close'].max():.4f}")
    
    # 2. –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n2Ô∏è‚É£ –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö:")
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 4-—á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        resampled_4h = resample_data(base_data, target_timeframe='4h')
        print(f"   ‚úÖ –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –≤ 4h: {len(base_data)} ‚Üí {len(resampled_4h)} –±–∞—Ä–æ–≤")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –¥–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        resampled_1d = resample_data(base_data, target_timeframe='1d')
        print(f"   ‚úÖ –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –≤ 1d: {len(base_data)} ‚Üí {len(resampled_1d)} –±–∞—Ä–æ–≤")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ä–µ—Å—ç–º–ø–ª–∏–Ω–≥–∞: {e}")
        resampled_4h = base_data
    
    # 3. –†–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    print(f"\n3Ô∏è‚É£ –†–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:")
    
    try:
        derived_data = calculate_derived_indicators(base_data)
        new_columns = [col for col in derived_data.columns if col not in base_data.columns]
        
        print(f"   ‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {len(new_columns)}")
        print(f"   üìä –ù–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(new_columns[:5])}{'...' if len(new_columns) > 5 else ''}")
        print(f"   üìà –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {base_data.shape} ‚Üí {derived_data.shape}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö: {e}")
        derived_data = base_data
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\n4Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    
    try:
        lagged_data = create_lagged_features(
            derived_data, 
            columns=['close', 'volume'], 
            lags=[1, 2, 5, 10]
        )
        
        lag_columns = [col for col in lagged_data.columns if '_lag_' in col]
        print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(lag_columns)}")
        print(f"   üìä –ü—Ä–∏–º–µ—Ä—ã: {', '.join(lag_columns[:3])}{'...' if len(lag_columns) > 3 else ''}")
        print(f"   üìà –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {derived_data.shape} ‚Üí {lagged_data.shape}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ª–∞–≥–æ–≤: {e}")
        lagged_data = derived_data
    
    # 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print(f"\n5Ô∏è‚É£ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:")
    
    try:
        numeric_columns = ['close', 'high', 'low', 'open', 'volume']
        available_columns = [col for col in numeric_columns if col in lagged_data.columns]
        
        normalized_data = normalize_data(lagged_data, columns=available_columns, method='minmax')
        
        print(f"   ‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(available_columns)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        for col in available_columns[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
            norm_col = f"{col}_normalized"
            if norm_col in normalized_data.columns:
                min_val = normalized_data[norm_col].min()
                max_val = normalized_data[norm_col].max()
                print(f"      {col}: [{min_val:.3f}, {max_val:.3f}]")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        normalized_data = lagged_data
    
    return normalized_data


def demonstrate_data_schemas():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å–æ —Å—Ö–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö."""
    
    print(f"\nüìã –†–∞–±–æ—Ç–∞ —Å–æ —Å—Ö–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö")
    print("-" * 40)
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ OHLCV
    print(f"\n1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ OHLCVRecord:")
    
    try:
        sample_record = OHLCVRecord(
            timestamp=datetime.now(),
            open=1.1234,
            high=1.1250,
            low=1.1220,
            close=1.1245,
            volume=100000
        )
        
        print(f"   ‚úÖ –ó–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞:")
        print(f"      –í—Ä–µ–º—è: {sample_record.timestamp}")
        print(f"      OHLC: {sample_record.open}/{sample_record.high}/{sample_record.low}/{sample_record.close}")
        print(f"      –û–±—ä–µ–º: {sample_record.volume:,}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏: {e}")
    
    # 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n2Ô∏è‚É£ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    
    try:
        data_config = DataSourceConfig(
            name="MT5_EURUSD",
            file_pattern="EURUSD_*.csv",
            timeframe_mapping={'1h': '60', '4h': '240', '1d': '1440'},
            quote_providers=['MetaTrader5', 'TradingView']
        )
        
        print(f"   ‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞:")
        print(f"      –ù–∞–∑–≤–∞–Ω–∏–µ: {data_config.name}")
        print(f"      –ü–∞—Ç—Ç–µ—Ä–Ω —Ñ–∞–π–ª–æ–≤: {data_config.file_pattern}")
        print(f"      –¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {data_config.timeframe_mapping}")
        print(f"      –ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã: {data_config.quote_providers}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    
    # 3. –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    print(f"\n3Ô∏è‚É£ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
    
    try:
        validation_result = ValidationResult(
            issues=["Missing data in close column", "Price gaps detected"],
            stats={'total_rows': 1000, 'missing_values': 5, 'outliers': 2},
            recommendations=["Fill missing values", "Review price gaps"]
        )
        
        print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω:")
        print(f"      –ü—Ä–æ–±–ª–µ–º—ã: {len(validation_result.issues)}")
        for issue in validation_result.issues:
            print(f"        ‚Ä¢ {issue}")
        print(f"      –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {validation_result.stats}")
        print(f"      –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {len(validation_result.recommendations)}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")


def save_processed_data(data: pd.DataFrame, filename: str = "processed_data.csv"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    
    try:
        filepath = os.path.join("examples", filename)
        data.to_csv(filepath)
        
        print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
        print(f"   üìä –†–∞–∑–º–µ—Ä: {data.shape}")
        print(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {data.index[0]} - {data.index[-1]}")
        print(f"   üìà –ö–æ–ª–æ–Ω–∫–∏: {len(data.columns)} ({', '.join(data.columns[:5])}{'...' if len(data.columns) > 5 else ''})")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(filepath)
        size_mb = file_size / (1024 * 1024)
        print(f"   üíΩ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {size_mb:.2f} MB")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")


def demonstrate_timeframes_support():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤."""
    
    print(f"\n‚è∞ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤")
    print("-" * 40)
    
    print(f"\nüìä –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã:")
    for tf in SUPPORTED_TIMEFRAMES:
        print(f"   ‚úì {tf}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã –¥–ª—è —Å–∏–º–≤–æ–ª–∞
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤:")
    
    try:
        available_tf = get_available_timeframes("XAUUSD")
        print(f"   üìà –î–ª—è XAUUSD –¥–æ—Å—Ç—É–ø–Ω–æ: {len(available_tf)} —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤")
        for tf in available_tf[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"      ‚Ä¢ {tf}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤: {e}")


if __name__ == "__main__":
    try:
        print("üöÄ BQuant - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏")
        print("=" * 60)
        
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        sample_data, trending_data, volatile_data = demonstrate_data_creation()
        
        # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        demonstrate_data_validation()
        
        # 3. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        cleaned_data = demonstrate_data_cleaning()
        
        # 4. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        processed_data = demonstrate_data_transformations()
        
        # 5. –†–∞–±–æ—Ç–∞ —Å–æ —Å—Ö–µ–º–∞–º–∏
        demonstrate_data_schemas()
        
        # 6. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        demonstrate_timeframes_support()
        
        # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_processed_data(processed_data, "comprehensive_processed_data.csv")
        save_processed_data(sample_data, "sample_ohlcv_data.csv")
        save_processed_data(trending_data, "trending_data.csv")
        save_processed_data(volatile_data, "volatile_data.csv")
        
        print(f"\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"\nüí° –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
        print(f"   ‚úì –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        print(f"   ‚úì –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        print(f"   ‚úì –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Ä–µ—Å—ç–º–ø–ª–∏–Ω–≥")
        print(f"   ‚úì –†–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        print(f"   ‚úì –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"   ‚úì –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
        print(f"   ‚úì –†–∞–±–æ—Ç–∞ —Å–æ —Å—Ö–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö")
        print(f"   ‚úì –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
