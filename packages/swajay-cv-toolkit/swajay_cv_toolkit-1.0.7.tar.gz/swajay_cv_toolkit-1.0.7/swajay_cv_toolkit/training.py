"""
Complete Training Pipeline for Computer Vision - Updated v1.1.0
Full implementation with all classes and advanced techniques

NEW FEATURES:
- Stochastic Weight Averaging (SWA)
- SAM optimizer support
- Progressive resizing training
- Advanced scheduling strategies
- Complete TTA implementation
- Model ensemble training
- Comprehensive evaluation tools

Sources:
- SWA: Izmailov et al., 2018 - https://arxiv.org/abs/1803.05407
- SAM: Foret et al., 2020 - https://arxiv.org/abs/2010.01412
- Progressive Resizing: fastai technique
- TTA: Krizhevsky et al., 2012 - ImageNet Classification
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.optim.swa_utils import AveragedModel, SWALR, update_bn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import math
from typing import Dict, List, Optional, Tuple, Callable, Union
from collections import defaultdict
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')


class SAM(torch.optim.Optimizer):
    """
    Sharpness-Aware Minimization (SAM) optimizer
    Reference: "Sharpness-Aware Minimization for Efficiently Improving Generalization"
    https://arxiv.org/abs/2010.01412
    """
    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):
        assert rho >= 0.0, f"Invalid rho, should be non-negative: {rho}"

        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)
        super(SAM, self).__init__(params, defaults)

        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)
        self.param_groups = self.base_optimizer.param_groups
        self.defaults.update(self.base_optimizer.defaults)

    @torch.no_grad()
    def first_step(self, zero_grad=False):
        grad_norm = self._grad_norm()
        for group in self.param_groups:
            scale = group["rho"] / (grad_norm + 1e-12)

            for p in group["params"]:
                if p.grad is None: continue
                self.state[p]["old_p"] = p.data.clone()
                e_w = (torch.pow(p, 2) if group["adaptive"] else 1.0) * p.grad * scale.to(p)
                p.add_(e_w)  # climb to the local maximum "w + e(w)"

        if zero_grad: self.zero_grad()

    @torch.no_grad()
    def second_step(self, zero_grad=False):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None: continue
                p.data = self.state[p]["old_p"]  # get back to "w" from "w + e(w)"

        self.base_optimizer.step()  # do the actual "sharpness-aware" update

        if zero_grad: self.zero_grad()

    @torch.no_grad()
    def step(self, closure=None):
        assert closure is not None, "SAM requires closure, but it was not provided"
        closure = torch.enable_grad()(closure)

        self.first_step(zero_grad=True)
        closure()
        self.second_step()

    def _grad_norm(self):
        shared_device = self.param_groups[0]["params"][0].device
        norm = torch.norm(
                    torch.stack([
                        ((torch.abs(p) if group["adaptive"] else 1.0) * p.grad).norm(dtype=torch.float32)
                        for group in self.param_groups for p in group["params"]
                        if p.grad is not None
                    ]),
                    dtype=torch.float32
                )
        return norm.to(shared_device)


class ProgressiveTrainer:
    """
    Progressive training with multiple image sizes
    """
    def __init__(self, model, criterion, device='cuda', 
                 image_sizes=[224, 256, 288], 
                 stage_epochs=[12, 15, 18]):
        self.model = model
        self.criterion = criterion
        self.device = device
        self.image_sizes = image_sizes
        self.stage_epochs = stage_epochs
        self.stage_histories = []
        
    def train_stage(self, train_loader, val_loader, 
                   stage_idx, optimizer, scheduler,
                   mixup_cutmix=None, use_swa=False,
                   swa_start_epoch=8, verbose=True):
        """Train a single progressive stage"""
        
        image_size = self.image_sizes[stage_idx]
        epochs = self.stage_epochs[stage_idx]
        
        print(f"\nðŸŽ¯ Progressive Stage {stage_idx+1}: {image_size}px for {epochs} epochs")
        
        # SWA setup
        swa_model = None
        swa_scheduler = None
        if use_swa and stage_idx == len(self.image_sizes) - 1:  # Only in final stage
            swa_model = AveragedModel(self.model)
            swa_scheduler = SWALR(optimizer, swa_lr=0.01)
            print("âœ… SWA enabled for final stage")
        
        best_val_acc = 0.0
        stage_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        
        for epoch in range(epochs):
            # Training
            train_loss, train_acc = self._train_epoch(
                train_loader, optimizer, scheduler, epoch,
                mixup_cutmix, verbose and epoch % 3 == 0
            )
            
            # Validation (every 2 epochs to save time)
            if epoch % 2 == 0 or epoch == epochs - 1:
                val_loss, val_acc = self._validate_epoch(val_loader, verbose)
                
                stage_history['train_loss'].append(train_loss)
                stage_history['train_acc'].append(train_acc)
                stage_history['val_loss'].append(val_loss)
                stage_history['val_acc'].append(val_acc)
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    torch.save(self.model.state_dict(), f'best_model_stage_{stage_idx}.pth')
                    if verbose:
                        print(f"âœ… New best for stage {stage_idx+1}: {val_acc*100:.3f}%")
            
            # SWA update
            if swa_model is not None and epoch >= swa_start_epoch:
                swa_model.update_parameters(self.model)
                swa_scheduler.step()
        
        # Finalize SWA
        if swa_model is not None:
            print("ðŸ”„ Updating batch norm for SWA model...")
            update_bn(train_loader, swa_model)
            torch.save(swa_model.state_dict(), f'swa_model_stage_{stage_idx}.pth')
            
            # Evaluate SWA model
            swa_val_loss, swa_val_acc = self._validate_epoch(val_loader, verbose=False, model=swa_model)
            print(f"ðŸ“Š SWA Model Accuracy: {swa_val_acc*100:.3f}%")
            
            if swa_val_acc > best_val_acc:
                torch.save(swa_model.state_dict(), f'best_swa_model_stage_{stage_idx}.pth')
                print("âœ… SWA model is better than regular model!")
        
        self.stage_histories.append(stage_history)
        return best_val_acc, swa_model
    
    def _train_epoch(self, train_loader, optimizer, scheduler, epoch,
                    mixup_cutmix=None, verbose=True):
        """Train for one epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            # Apply mixup/cutmix
            if mixup_cutmix and torch.rand(1) < 0.6:
                data, target_a, target_b, lam = mixup_cutmix(data, target)
                mixed_target = True
            else:
                target_a, target_b, lam = target, target, 1.0
                mixed_target = False
            
            # Forward pass
            def closure():
                optimizer.zero_grad()
                with torch.cuda.amp.autocast():
                    outputs = self.model(data)
                    
                    if mixed_target:
                        loss_a = self.criterion(outputs, target_a)
                        loss_b = self.criterion(outputs, target_b)
                        loss = lam * loss_a + (1 - lam) * loss_b
                    else:
                        loss = self.criterion(outputs, target)
                
                loss.backward()
                return loss
            
            # Check if using SAM optimizer
            if hasattr(optimizer, 'first_step'):
                # SAM optimizer
                loss = closure()
                optimizer.first_step(zero_grad=True)
                closure()
                optimizer.second_step(zero_grad=True)
            else:
                # Regular optimizer
                loss = closure()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
            
            if scheduler is not None:
                scheduler.step()
            
            # Update EMA if model supports it
            if hasattr(self.model, 'update_ema'):
                self.model.update_ema()
            
            # Statistics
            running_loss += loss.item()
            with torch.no_grad():
                outputs = self.model(data)
                pred = outputs.argmax(dim=1)
                if mixed_target:
                    correct += (lam * pred.eq(target_a).float() + (1-lam) * pred.eq(target_b).float()).sum().item()
                else:
                    correct += pred.eq(target).sum().item()
                total += target.size(0)
            
            if verbose and batch_idx % 50 == 0:
                print(f'  Epoch {epoch+1}, Batch {batch_idx}: Loss: {loss.item():.6f}, Acc: {100.*correct/total:.2f}%')
        
        return running_loss / len(train_loader), correct / total
    
    def _validate_epoch(self, val_loader, verbose=True, model=None):
        """Validate for one epoch"""
        eval_model = model if model is not None else self.model
        eval_model.eval()
        
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                with torch.cuda.amp.autocast():
                    outputs = eval_model(data)
                
                val_loss += F.cross_entropy(outputs, target).item()
                pred = outputs.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        val_loss = val_loss / len(val_loader)
        val_acc = correct / total
        
        if verbose:
            print(f"  ðŸ“Š Validation: Loss: {val_loss:.6f}, Acc: {val_acc*100:.3f}%")
        
        return val_loss, val_acc


class AdvancedTrainer:
    """
    Enhanced trainer with cutting-edge techniques
    """
    
    def __init__(self, 
                 model: nn.Module,
                 criterion: nn.Module,
                 optimizer: optim.Optimizer,
                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
                 device: str = 'cuda',
                 mixed_precision: bool = True,
                 gradient_clip_norm: float = 1.0,
                 use_swa: bool = False,
                 swa_start: int = 20):
        
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.mixed_precision = mixed_precision
        self.gradient_clip_norm = gradient_clip_norm
        self.use_swa = use_swa
        self.swa_start = swa_start
        
        # Mixed precision setup
        if mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()
        
        # SWA setup
        self.swa_model = None
        self.swa_scheduler = None
        if use_swa:
            self.swa_model = AveragedModel(model)
            if hasattr(optimizer, 'param_groups'):
                base_lr = optimizer.param_groups[0]['lr']
                self.swa_scheduler = SWALR(optimizer, swa_lr=base_lr * 0.1)
        
        # Training history
        self.history = defaultdict(list)
        
        # Best model tracking
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.best_epoch = 0
        
    def train_epoch(self, 
                   train_loader: DataLoader,
                   epoch: int,
                   mixup_cutmix: Optional[Callable] = None,
                   verbose: bool = True) -> Dict[str, float]:
        """Enhanced training epoch with advanced techniques"""
        
        self.model.train()
        running_loss = 0.0
        running_corrects = 0
        total_samples = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            # Apply mixup/cutmix if provided
            if mixup_cutmix is not None and torch.rand(1) < 0.5:
                data, target_a, target_b, lam = mixup_cutmix(data, target)
                mixed_target = True
            else:
                target_a, target_b, lam = target, target, 1.0
                mixed_target = False
            
            # Define closure for SAM optimizer
            def closure():
                if hasattr(self.optimizer, 'zero_grad'):
                    self.optimizer.zero_grad()
                
                if self.mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(data)
                        if mixed_target:
                            loss = lam * self.criterion(outputs, target_a) + (1 - lam) * self.criterion(outputs, target_b)
                        else:
                            loss = self.criterion(outputs, target)
                    
                    self.scaler.scale(loss).backward()
                    return loss
                else:
                    outputs = self.model(data)
                    if mixed_target:
                        loss = lam * self.criterion(outputs, target_a) + (1 - lam) * self.criterion(outputs, target_b)
                    else:
                        loss = self.criterion(outputs, target)
                    
                    loss.backward()
                    return loss
            
            # Handle SAM vs regular optimizers
            if hasattr(self.optimizer, 'first_step'):
                # SAM optimizer
                loss = closure()
                if self.mixed_precision:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_norm)
                    self.scaler.step(lambda: self.optimizer.first_step(zero_grad=True))
                    self.scaler.scale(closure()).backward()
                    self.scaler.step(lambda: self.optimizer.second_step(zero_grad=True))
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_norm)
                    self.optimizer.first_step(zero_grad=True)
                    closure()
                    self.optimizer.second_step(zero_grad=True)
            else:
                # Regular optimizer
                loss = closure()
                
                if self.mixed_precision:
                    if self.gradient_clip_norm > 0:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_norm)
                    
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    if self.gradient_clip_norm > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_norm)
                    self.optimizer.step()
            
            if self.scheduler is not None:
                self.scheduler.step()
            
            # Update SWA model if enabled and past start epoch
            if self.swa_model is not None and epoch >= self.swa_start:
                self.swa_model.update_parameters(self.model)
                if self.swa_scheduler is not None:
                    self.swa_scheduler.step()
            
            # Update EMA if model supports it
            if hasattr(self.model, 'update_ema'):
                self.model.update_ema()
            
            # Statistics
            running_loss += loss.item()
            
            # Calculate accuracy
            with torch.no_grad():
                outputs = self.model(data)
                _, preds = torch.max(outputs, 1)
                if mixed_target:
                    running_corrects += (lam * preds.eq(target_a).float() + 
                                       (1 - lam) * preds.eq(target_b).float()).sum().item()
                else:
                    running_corrects += torch.sum(preds == target.data)
                
                total_samples += target.size(0)
            
            # Verbose logging
            if verbose and batch_idx % 40 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f'Epoch: {epoch}, Batch: {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item():.6f}, Acc: {100.*running_corrects/total_samples:.2f}%, '
                      f'LR: {current_lr:.8f}')
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = running_corrects / total_samples
        
        return {'loss': epoch_loss, 'accuracy': epoch_acc}
    
    def validate_epoch(self, val_loader: DataLoader, 
                      use_swa: bool = False, verbose: bool = True) -> Dict[str, float]:
        """Enhanced validation with SWA model option"""
        
        # Choose model for validation
        eval_model = self.swa_model if (use_swa and self.swa_model is not None) else self.model
        eval_model.eval()
        
        running_loss = 0.0
        running_corrects = 0
        total_samples = 0
        
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                if self.mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = eval_model(data)
                else:
                    outputs = eval_model(data)
                
                loss = F.cross_entropy(outputs, target)
                
                running_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                running_corrects += torch.sum(preds == target.data)
                total_samples += target.size(0)
                
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = running_corrects / total_samples
        
        if verbose:
            model_type = "SWA" if use_swa else "Regular"
            print(f'{model_type} Validation Loss: {epoch_loss:.6f}, '
                  f'{model_type} Validation Acc: {epoch_acc*100:.2f}%')
        
        return {
            'loss': epoch_loss, 
            'accuracy': epoch_acc,
            'predictions': all_preds,
            'targets': all_targets
        }
    
    def fit(self,
            train_loader: DataLoader,
            val_loader: DataLoader,
            epochs: int,
            mixup_cutmix: Optional[Callable] = None,
            early_stopping_patience: int = 15,
            save_path: str = 'best_model.pth',
            validate_swa: bool = True,
            verbose: bool = True) -> Dict[str, List]:
        """
        Enhanced training loop with SWA and advanced features
        """
        
        patience_counter = 0
        start_time = time.time()
        
        for epoch in range(epochs):
            if verbose:
                print(f'\nEpoch {epoch+1}/{epochs}')
                print('-' * 60)
            
            epoch_start = time.time()
            
            # Training
            train_metrics = self.train_epoch(
                train_loader, epoch, mixup_cutmix, verbose
            )
            
            # Regular validation
            val_metrics = self.validate_epoch(val_loader, use_swa=False, verbose=verbose)
            
            # SWA validation (if enabled and past start epoch)
            swa_metrics = None
            if (self.swa_model is not None and epoch >= self.swa_start and 
                validate_swa and epoch % 3 == 0):
                # Update batch norm for SWA model periodically
                if epoch % 5 == 0:
                    update_bn(train_loader, self.swa_model)
                swa_metrics = self.validate_epoch(val_loader, use_swa=True, verbose=verbose)
            
            # Update history
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['train_accuracy'].append(train_metrics['accuracy'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['val_accuracy'].append(val_metrics['accuracy'])
            
            if swa_metrics is not None:
                self.history.setdefault('swa_val_loss', []).append(swa_metrics['loss'])
                self.history.setdefault('swa_val_accuracy', []).append(swa_metrics['accuracy'])
            
            epoch_time = time.time() - epoch_start
            
            if verbose:
                print(f'Epoch time: {epoch_time:.2f}s')
            
            # Early stopping check (use SWA metrics if better)
            current_val_loss = val_metrics['loss']
            current_val_acc = val_metrics['accuracy']
            save_swa = False
            
            if swa_metrics is not None and swa_metrics['loss'] < val_metrics['loss']:
                current_val_loss = swa_metrics['loss']
                current_val_acc = swa_metrics['accuracy']
                save_swa = True
            
            if current_val_loss < self.best_val_loss:
                self.best_val_loss = current_val_loss
                self.best_val_acc = current_val_acc
                self.best_epoch = epoch
                patience_counter = 0
                
                # Save best model (regular or SWA)
                if save_swa:
                    torch.save(self.swa_model.state_dict(), save_path)
                    torch.save(self.swa_model.state_dict(), save_path.replace('.pth', '_swa.pth'))
                    print(f'âœ… New best SWA model saved! Val Loss: {self.best_val_loss:.6f}, '
                          f'Val Acc: {self.best_val_acc*100:.2f}%')
                else:
                    torch.save(self.model.state_dict(), save_path)
                    print(f'âœ… New best model saved! Val Loss: {self.best_val_loss:.6f}, '
                          f'Val Acc: {self.best_val_acc*100:.2f}%')
            else:
                patience_counter += 1
                
            # Early stopping
            if patience_counter >= early_stopping_patience and epoch > 10:
                print(f'Early stopping triggered after {epoch+1} epochs')
                break
        
        # Final SWA batch norm update
        if self.swa_model is not None:
            print("ðŸ”„ Final SWA batch norm update...")
            update_bn(train_loader, self.swa_model)
            torch.save(self.swa_model.state_dict(), save_path.replace('.pth', '_swa_final.pth'))
        
        total_time = time.time() - start_time
        print(f'\nTraining completed in {total_time/60:.2f} minutes')
        print(f'Best validation loss: {self.best_val_loss:.6f}')
        print(f'Best validation accuracy: {self.best_val_acc*100:.2f}%')
        
        return dict(self.history)


class TTAPredictor:
    """
    Complete Test-Time Augmentation predictor implementation
    """
    
    def __init__(self, model: nn.Module, device: str = 'cuda'):
        self.model = model
        self.device = device
        self.model.eval()
    
    def predict_with_tta(self,
                        test_dataset,
                        tta_transforms: List,
                        weights: Optional[List[float]] = None,
                        batch_size: int = 32) -> np.ndarray:
        """
        Predict using Test-Time Augmentation
        
        Args:
            test_dataset: Dataset to predict on
            tta_transforms: List of albumentations transforms
            weights: Optional weights for TTA transforms
            batch_size: Batch size for prediction
            
        Returns:
            Array of predicted class indices
        """
        if weights is None:
            weights = [1.0 / len(tta_transforms)] * len(tta_transforms)
        
        all_predictions = []
        
        with torch.no_grad():
            for idx in range(len(test_dataset)):
                if idx % 500 == 0:
                    print(f"Processing image {idx}/{len(test_dataset)}")
                
                # Get image
                if hasattr(test_dataset, '__getitem__'):
                    image, _ = test_dataset[idx]
                else:
                    image = test_dataset[idx]
                
                # Convert PIL to numpy if needed
                if hasattr(image, 'mode'):  # PIL Image
                    image = np.array(image)
                elif isinstance(image, torch.Tensor):
                    image = image.permute(1, 2, 0).numpy()
                    # Denormalize if needed
                    if image.min() < 0:
                        image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]
                        image = np.clip(image, 0, 1)
                        image = (image * 255).astype(np.uint8)
                
                tta_preds = []
                
                # Apply each TTA transform
                for transform in tta_transforms:
                    try:
                        augmented = transform(image=image)
                        img_tensor = augmented['image'].unsqueeze(0).to(self.device)
                        
                        output = self.model(img_tensor)
                        prob = torch.softmax(output, dim=1)
                        tta_preds.append(prob.cpu().numpy())
                    except Exception as e:
                        print(f"Error in TTA transform: {e}")
                        # Skip this transform
                        continue
                
                if not tta_preds:
                    print(f"Warning: No TTA predictions for image {idx}")
                    # Fallback: use original image
                    if isinstance(image, np.ndarray):
                        img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
                        img_tensor = img_tensor.unsqueeze(0).to(self.device)
                        output = self.model(img_tensor)
                        final_pred = output.argmax(dim=1).item()
                    else:
                        final_pred = 0  # Default prediction
                else:
                    # Weighted average of predictions
                    valid_weights = weights[:len(tta_preds)]
                    if len(valid_weights) != len(tta_preds):
                        valid_weights = [1.0 / len(tta_preds)] * len(tta_preds)
                    
                    weighted_pred = np.average(tta_preds, axis=0, weights=valid_weights)
                    final_pred = np.argmax(weighted_pred)
                
                all_predictions.append(final_pred)
        
        return np.array(all_predictions)
    
    def predict_batch_tta(self,
                         test_loader: DataLoader,
                         tta_transforms: List,
                         weights: Optional[List[float]] = None) -> np.ndarray:
        """
        Batch prediction with TTA (more memory efficient for large datasets)
        """
        if weights is None:
            weights = [1.0 / len(tta_transforms)] * len(tta_transforms)
        
        all_predictions = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                if isinstance(batch, (list, tuple)):
                    images = batch[0]
                else:
                    images = batch
                
                batch_tta_preds = []
                
                # Apply each TTA transform to the batch
                for transform_idx, transform in enumerate(tta_transforms):
                    batch_preds = []
                    
                    for img_idx in range(images.size(0)):
                        image = images[img_idx]
                        
                        # Convert to numpy for albumentations
                        if isinstance(image, torch.Tensor):
                            image = image.permute(1, 2, 0).numpy()
                            if image.min() < 0:  # Denormalize if needed
                                image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]
                                image = np.clip(image, 0, 1)
                            image = (image * 255).astype(np.uint8)
                        
                        # Apply transform
                        augmented = transform(image=image)
                        img_tensor = augmented['image'].unsqueeze(0)
                        batch_preds.append(img_tensor)
                    
                    # Stack batch and predict
                    batch_tensor = torch.cat(batch_preds, dim=0).to(self.device)
                    outputs = self.model(batch_tensor)
                    probs = torch.softmax(outputs, dim=1)
                    batch_tta_preds.append(probs.cpu().numpy())
                
                # Weighted average across TTA transforms
                batch_weighted_preds = np.average(batch_tta_preds, axis=0, weights=weights)
                batch_final_preds = np.argmax(batch_weighted_preds, axis=1)
                
                all_predictions.extend(batch_final_preds)
                
                if batch_idx % 10 == 0:
                    print(f"Processed batch {batch_idx}/{len(test_loader)}")
        
        return np.array(all_predictions)


class ModelEvaluator:
    """
    Comprehensive model evaluation tools
    """
    
    @staticmethod
    def evaluate_model(model: nn.Module,
                      test_loader: DataLoader,
                      class_names: List[str],
                      device: str = 'cuda') -> Dict:
        """
        Comprehensive model evaluation
        
        Args:
            model: PyTorch model to evaluate
            test_loader: DataLoader for test data
            class_names: List of class names
            device: Device to run evaluation on
            
        Returns:
            Dictionary with comprehensive evaluation results
        """
        model.eval()
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                outputs = model(data)
                probs = torch.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)
                
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        # Calculate metrics
        accuracy = accuracy_score(all_targets, all_preds)
        
        # Classification report
        report = classification_report(
            all_targets, all_preds, 
            target_names=class_names, 
            output_dict=True
        )
        
        # Confusion matrix
        cm = confusion_matrix(all_targets, all_preds)
        
        # Per-class accuracy
        per_class_acc = cm.diagonal() / cm.sum(axis=1)
        
        # Top-k accuracy
        all_probs = np.array(all_probs)
        top5_acc = ModelEvaluator._top_k_accuracy(all_probs, all_targets, k=5)
        
        return {
            'accuracy': accuracy,
            'top5_accuracy': top5_acc,
            'classification_report': report,
            'confusion_matrix': cm,
            'per_class_accuracy': per_class_acc,
            'predictions': all_preds,
            'targets': all_targets,
            'probabilities': all_probs
        }
    
    @staticmethod
    def _top_k_accuracy(probs: np.ndarray, targets: np.ndarray, k: int = 5) -> float:
        """Calculate top-k accuracy"""
        top_k_preds = np.argsort(probs, axis=1)[:, -k:]
        correct = 0
        for i, target in enumerate(targets):
            if target in top_k_preds[i]:
                correct += 1
        return correct / len(targets)
    
    @staticmethod
    def plot_confusion_matrix(cm: np.ndarray, class_names: List[str], 
                            normalize: bool = True, figsize: Tuple[int, int] = (12, 10)):
        """
        Plot confusion matrix
        
        Args:
            cm: Confusion matrix
            class_names: List of class names
            normalize: Whether to normalize the confusion matrix
            figsize: Figure size
        """
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            title = 'Normalized Confusion Matrix'
            fmt = '.2f'
        else:
            title = 'Confusion Matrix'
            fmt = 'd'
        
        plt.figure(figsize=figsize)
        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names)
        plt.title(title)
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def plot_classification_report(report: Dict, figsize: Tuple[int, int] = (10, 8)):
        """Plot classification report as heatmap"""
        # Extract metrics for each class
        classes = list(report.keys())[:-3]  # Exclude avg metrics
        metrics = ['precision', 'recall', 'f1-score']
        
        data = []
        for cls in classes:
            row = [report[cls][metric] for metric in metrics]
            data.append(row)
        
        data = np.array(data)
        
        plt.figure(figsize=figsize)
        sns.heatmap(data, annot=True, fmt='.3f', cmap='RdYlGn',
                    xticklabels=metrics, yticklabels=classes)
        plt.title('Classification Report')
        plt.xlabel('Metrics')
        plt.ylabel('Classes')
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def create_submission(predictions: np.ndarray,
                         class_names: List[str],
                         filename: str = 'submission.csv') -> pd.DataFrame:
        """
        Create submission file for competitions
        
        Args:
            predictions: Array of predicted class indices
            class_names: List of class names
            filename: Output filename
            
        Returns:
            DataFrame with submission format
        """
        pred_labels = [class_names[pred] for pred in predictions]
        
        submission_df = pd.DataFrame({
            'ID': range(len(predictions)),
            'Label': pred_labels
        })
        
        submission_df.to_csv(filename, index=False)
        print(f"Submission saved as '{filename}'")
        print(f"Total predictions: {len(predictions)}")
        print(f"Class distribution:")
        print(submission_df['Label'].value_counts())
        
        return submission_df
    
    @staticmethod
    def analyze_predictions(targets: np.ndarray, predictions: np.ndarray, 
                          class_names: List[str]) -> Dict:
        """
        Analyze prediction patterns and errors
        
        Args:
            targets: Ground truth labels
            predictions: Predicted labels
            class_names: List of class names
            
        Returns:
            Dictionary with analysis results
        """
        from collections import defaultdict
        
        # Most confused classes
        cm = confusion_matrix(targets, predictions)
        confused_pairs = []
        
        for i in range(len(class_names)):
            for j in range(len(class_names)):
                if i != j and cm[i, j] > 0:
                    confused_pairs.append({
                        'true_class': class_names[i],
                        'pred_class': class_names[j],
                        'count': cm[i, j],
                        'percentage': cm[i, j] / cm[i].sum() * 100
                    })
        
        # Sort by count
        confused_pairs = sorted(confused_pairs, key=lambda x: x['count'], reverse=True)
        
        # Hardest classes (lowest recall)
        report = classification_report(targets, predictions, 
                                     target_names=class_names, output_dict=True)
        
        hardest_classes = []
        for class_name in class_names:
            if class_name in report:
                hardest_classes.append({
                    'class': class_name,
                    'recall': report[class_name]['recall'],
                    'precision': report[class_name]['precision'],
                    'f1': report[class_name]['f1-score']
                })
        
        hardest_classes = sorted(hardest_classes, key=lambda x: x['recall'])
        
        return {
            'most_confused_pairs': confused_pairs[:10],  # Top 10
            'hardest_classes': hardest_classes[:5],      # Bottom 5
            'overall_accuracy': accuracy_score(targets, predictions),
            'confusion_matrix': cm
        }


# Enhanced optimizer creation with SAM support
def create_optimizer(model: nn.Module,
                    optimizer_name: str = 'adamw',
                    learning_rate: float = 1e-3,
                    weight_decay: float = 1e-4,
                    differential_lr: bool = True,
                    use_sam: bool = False,
                    sam_rho: float = 0.05) -> optim.Optimizer:
    """
    Create optimizer with SAM support and differential learning rates
    
    Args:
        model: PyTorch model
        optimizer_name: Name of optimizer ('adamw', 'adam', 'sgd')
        learning_rate: Base learning rate
        weight_decay: Weight decay factor
        differential_lr: Use different LR for backbone vs classifier
        use_sam: Whether to use SAM optimizer
        sam_rho: SAM rho parameter
        
    Returns:
        PyTorch optimizer
    """
    
    if differential_lr:
        # Separate backbone and classifier parameters
        backbone_params = []
        classifier_params = []
        
        for name, param in model.named_parameters():
            if any(keyword in name for keyword in ['classifier', 'head', 'fc']):
                classifier_params.append(param)
            else:
                backbone_params.append(param)
        
        param_groups = [
            {'params': backbone_params, 'lr': learning_rate * 0.1, 'weight_decay': weight_decay},
            {'params': classifier_params, 'lr': learning_rate, 'weight_decay': weight_decay * 10}
        ]
    else:
        param_groups = model.parameters()
    
    # Base optimizer selection
    if optimizer_name.lower() == 'adamw':
        base_optimizer = lambda params, **kwargs: optim.AdamW(params, **kwargs, eps=1e-8)
    elif optimizer_name.lower() == 'adam':
        base_optimizer = lambda params, **kwargs: optim.Adam(params, **kwargs)
    elif optimizer_name.lower() == 'sgd':
        base_optimizer = lambda params, **kwargs: optim.SGD(params, momentum=0.9, **kwargs)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")
    
    # Apply SAM wrapper if requested
    if use_sam:
        return SAM(param_groups, base_optimizer, rho=sam_rho)
    else:
        return base_optimizer(param_groups, lr=learning_rate, weight_decay=weight_decay)


def create_scheduler(optimizer: optim.Optimizer,
                    scheduler_name: str = 'cosine',
                    total_steps: int = None,
                    warmup_steps: int = None,
                    **kwargs) -> optim.lr_scheduler._LRScheduler:
    """
    Create learning rate scheduler
    
    Args:
        optimizer: PyTorch optimizer
        scheduler_name: Type of scheduler ('cosine', 'step', 'plateau')
        total_steps: Total training steps (required for cosine)
        warmup_steps: Warmup steps
        **kwargs: Additional scheduler arguments
        
    Returns:
        PyTorch scheduler
    """
    
    if scheduler_name.lower() == 'cosine':
        if total_steps is None:
            raise ValueError("total_steps required for cosine scheduler")
        
        def lr_lambda(current_step):
            if warmup_steps and current_step < warmup_steps:
                return float(current_step) / float(max(1, warmup_steps))
            progress = float(current_step - (warmup_steps or 0)) / float(max(1, total_steps - (warmup_steps or 0)))
            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
        
        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        
    elif scheduler_name.lower() == 'step':
        step_size = kwargs.get('step_size', 10)
        gamma = kwargs.get('gamma', 0.1)
        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
        
    elif scheduler_name.lower() == 'plateau':
        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)
        
    else:
        raise ValueError(f"Unknown scheduler: {scheduler_name}")


# Training configuration presets
TRAINING_PRESETS = {
    'lightweight': {
        'optimizer': 'adamw',
        'learning_rate': 3e-4,
        'weight_decay': 1e-4,
        'scheduler': 'cosine',
        'epochs': 20,
        'batch_size': 64,
        'mixed_precision': True,
        'use_swa': False
    },
    'standard': {
        'optimizer': 'adamw',
        'learning_rate': 1e-3,
        'weight_decay': 1e-4,
        'scheduler': 'cosine',
        'epochs': 30,
        'batch_size': 32,
        'mixed_precision': True,
        'use_swa': False
    },
    'competition': {
        'optimizer': 'adamw',
        'learning_rate': 8e-4,
        'weight_decay': 1e-3,
        'scheduler': 'cosine',
        'epochs': 40,
        'batch_size': 32,
        'mixed_precision': True,
        'differential_lr': True,
        'gradient_clip_norm': 0.8,
        'use_swa': True,
        'swa_start': 25
    },
    'ultimate': {  # NEW: Maximum performance
        'optimizer': 'adamw',
        'learning_rate': 6e-4,
        'weight_decay': 1e-3,
        'scheduler': 'cosine',
        'epochs': 45,
        'batch_size': 24,
        'mixed_precision': True,
        'differential_lr': True,
        'gradient_clip_norm': 1.0,
        'use_swa': True,
        'swa_start': 30,
        'use_sam': False,  # Can enable for even better results
        'sam_rho': 0.05
    }
}


def get_training_preset(preset_name: str) -> Dict:
    """Get predefined training configuration"""
    return TRAINING_PRESETS.get(preset_name, TRAINING_PRESETS['standard'])


# Utility functions for training analysis
def plot_training_history(history: Dict[str, List], figsize: Tuple[int, int] = (15, 5)):
    """
    Plot training history
    
    Args:
        history: Training history dictionary
        figsize: Figure size
    """
    fig, axes = plt.subplots(1, 2, figsize=figsize)
    
    # Loss plot
    axes[0].plot(history['train_loss'], label='Train Loss', color='blue', marker='o')
    axes[0].plot(history['val_loss'], label='Val Loss', color='red', marker='s')
    
    if 'swa_val_loss' in history:
        axes[0].plot(history['swa_val_loss'], label='SWA Val Loss', color='green', marker='^')
    
    axes[0].set_title('Training & Validation Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Accuracy plot
    train_acc = [acc*100 if acc <= 1 else acc for acc in history['train_accuracy']]
    val_acc = [acc*100 if acc <= 1 else acc for acc in history['val_accuracy']]
    
    axes[1].plot(train_acc, label='Train Acc', color='blue', marker='o')
    axes[1].plot(val_acc, label='Val Acc', color='red', marker='s')
    
    if 'swa_val_accuracy' in history:
        swa_val_acc = [acc*100 if acc <= 1 else acc for acc in history['swa_val_accuracy']]
        axes[1].plot(swa_val_acc, label='SWA Val Acc', color='green', marker='^')
    
    axes[1].set_title('Training & Validation Accuracy')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy (%)')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


# Export all classes and functions
__all__ = [
    'SAM',
    'ProgressiveTrainer', 
    'AdvancedTrainer',
    'TTAPredictor',
    'ModelEvaluator',
    'create_optimizer',
    'create_scheduler',
    'get_training_preset',
    'plot_training_history',
    'TRAINING_PRESETS'
]


if __name__ == "__main__":
    print("ðŸ‹ï¸ swajay-cv-toolkit Training v1.1.0 - Complete Implementation")
    print("=" * 60)
    print("âœ… SAM Optimizer")
    print("âœ… Progressive Training")  
    print("âœ… Advanced Trainer with SWA")
    print("âœ… Complete TTA Predictor")
    print("âœ… Comprehensive Model Evaluator")
    print("âœ… All utility functions")
    print("=" * 60)
    print("Ready for ultimate 98%+ training! ðŸš€")