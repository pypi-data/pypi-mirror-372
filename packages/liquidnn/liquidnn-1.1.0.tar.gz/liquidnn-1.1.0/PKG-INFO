Metadata-Version: 2.4
Name: liquidnn
Version: 1.1.0
Summary: A PyTorch based module for Liquid Networks
Author: Nakshatra Yadav
Author-email: nakshatrayadav1729@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
License-File: License
Requires-Dist: torch
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: license-file
Dynamic: requires-dist
Dynamic: summary

ðŸ§  liquidnn

A PyTorch implementation of Liquid Neural Networks (LNNs) and their hybrids (LTC, CNN, RNN, LSTM variants).
This library brings continuous-time dynamics into deep learning architectures using learnable time constants.

âœ¨ Features

ðŸ”¹ LiquidNeuralNetwork â€“ stacked liquid neurons

ðŸ”¹ LTCLayer â€“ Liquid Time-Constant layer

ðŸ”¹ LiquidCNN â€“ CNN + Liquid update

ðŸ”¹ LiquidRNN â€“ RNN + Liquid update

ðŸ”¹ LiquidLSTM â€“ LSTM + Liquid update (with bidirectional support)

ðŸ”¹ Per-neuron learnable Ï„ (time constants) with stability clamping

ðŸ“¦ Installation
From PyPI (after publishing):
pip install liquidnn


ðŸš€ Quick Start (all models in one script)

Copy-paste this script to test every model in liquidnn:

import torch
from liquidnn import (
    LiquidNeuralNetwork,
    LTCLayer,
    LiquidCNN,
    LiquidRNN,
    LiquidLSTM
)

# 1. LiquidNeuralNetwork
x = torch.randn(4, 10, 8)  # batch=4, seq_len=10, input_size=8
model = LiquidNeuralNetwork(input_size=8, hidden_size=16, num_layers=2)
out = model(x)
print("LiquidNeuralNetwork output:", out.shape)  # [4, 16]

# 2. LTCLayer
x = torch.randn(5, 20, 10)  # batch=5, seq_len=20, input_size=10
ltc = LTCLayer(input_size=10, hidden_size=32, num_layers=2)
out = ltc(x)
print("LTCLayer output:", out.shape)  # [5, 32]

# 3. LiquidCNN
x = torch.randn(8, 1, 28, 28)  # MNIST-like input
model = LiquidCNN(input_channels=1, hidden_size=64, num_layers_liq=2, num_layers_conv=2)
out = model(x)
print("LiquidCNN output:", out.shape)  # [8, 64]

# 4. LiquidRNN
x = torch.randn(15, 4, 12)  # seq_len=15, batch=4, input_size=12
model = LiquidRNN(input_size=12, hidden_size=32, num_layers_liq=2, num_layers_rnn=1)
out = model(x)
print("LiquidRNN output:", out.shape)  # [4, 32]

# 5. LiquidLSTM
x = torch.randn(3, 12, 10)  # batch=3, seq_len=12, input_size=10
model = LiquidLSTM(input_size=10, hidden_size=32,
                   num_layers_liq=2, num_layers_lstm=1,
                   batch_first=True, bidirectional=False)
out = model(x)
print("LiquidLSTM output:", out.shape)  # [3, 32]

ðŸ“š Architectures

LiquidNeuralNetwork: Stacked liquid neurons

LTCLayer: Continuous-time RNN update rule

LiquidCNN: Convolutional backbone + LTC dynamics

LiquidRNN: RNN + Liquid refinement

LiquidLSTM: LSTM + Liquid refinement

Install dependencies:

pip install torch


ðŸŒŸ Contribute

PRs and issues are welcome! If you try new liquidized architectures, feel free to share ðŸš€
