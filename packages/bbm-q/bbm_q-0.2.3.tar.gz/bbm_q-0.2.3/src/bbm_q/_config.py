import base64

_KEY = "c2stb3ItdjEtZTIzOWM4M2M0ZjU2MjgwZmU3ZDVmNzllZDllZDFkYzM4MTRkZGFiNzExMDg5YmI5NDZjMjFkMmMxNjcxNDU4Nw=="

def _decode_key(enc: str) -> str:
    return base64.b64decode(enc.encode()).decode()

API_KEY = _decode_key(_KEY)
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

HTTP_REFERER = None
X_TITLE = None

MODEL_ALIASES = {
    "model1": "openai/gpt-5-chat",          # GPT-5 Chat :contentReference[oaicite:0]{index=0}
    "model2": "anthropic/claude-sonnet-4",  # Claude Sonnet 4 :contentReference[oaicite:1]{index=1}
    "model3": "google/gemini-2.5-pro",      # Gemini 2.5 Pro :contentReference[oaicite:2]{index=2}
    "model4": "deepseek/deepseek-chat-v3.1",# DeepSeek V3.1 (chat) :contentReference[oaicite:3]{index=3}
    # Любая бесплатная модель — возьмём стабильный бесплатный слот DeepSeek:
    "model5": "deepseek/deepseek-chat-v3-0324:free",  # free-слот на OpenRouter :contentReference[oaicite:4]{index=4}
}

# По умолчанию = model1
DEFAULT_MODEL = MODEL_ALIASES["model1"]


PROXY_HTTPS = "http://r0hoHu:5DM8re@200.10.39.70:8000"

SYSTEM_PROMPT = """
Решите следующие задачи, используя локально установленный дистрибутив Anaconda.
Разрешается использовать исключительно базовые методы Python, основные методы пакета matplotlib, методы пакета Numpy: array, zeros, zeros_like, linspace, eye, shape, random, poly, roots (только в случае поиска корней характеристического уравнения), transpose, sqrt, log, exp, sin, cos, atan, arctan, tan, mean, методы модуля sparse библиотеки scipy. Наличие иных методов приводит к аннулированию оценки работы. Обязательным требованием является подробное комментирование кода, выделение номера задания и ответа.
Ответы на теоретические вопросы должны быть записаны на экзаменационном листе и влияют на итоговую оценку. Отсутствие ответов на поставленные в задачах вопросы приводит к выставлению 0 баллов за задачу.

Твоя задача - писать сразу рабочий код, и в конце кратко, в 1-2 предложения давать ответы на поставленные теоретические вопросы. Писать ответы максимально человечно, не как ИИ.
Также код писать нужно правильным, корректным, но без заумных переменных, писать так будто ты простой студент, по возможности сокращать код но обязательно следуя правилам из разрешения. Запрещено писать любые комментарии в коде.
"""

TASK_PAIRS = {
"В экономике для определения равновесной цены товара используется уравнение f(x) = x^2 - ln(x) - 1 = 0. Используя метод функциональной итерации найдите корень с точностью 10^-5. Объясните, как выбор функции g(x) в методе функциональной итерации влияет на сходимость. Сравните метод функциональной итерации с методом секущих по вычислительной сложности и устойчивости. В каких случаях метод функциональной итерации предпочтительнее? Приведите пример функции, где он эффективен.": """
import numpy as np

def g(x):
    return np.sqrt(1 + np.log(x))

def f(x):
    return x**2 - np.log(x) - 1

def fixed_point(g, x0, tol=1e-5, max_iter=200):
    x = x0
    for _ in range(max_iter):
        x_new = g(x)
        if abs(x_new - x) < tol:
            return x_new
        x = x_new
    return x

x0 = 1.5
root = fixed_point(g, x0)
print("Приближённый корень:", round(root, 6))
print("Невязка f(root):", round(f(root), 8))

Краткий ответ: Сходимость определяется |g′(x*)|<1 в окрестности корня, удачный g(x) ускоряет процесс. Функциональная итерация проще и устойчивее к шуму, но медленнее секущих; её удобно применять, когда легко записать эквивалент x=g(x) (например, x=cos x).
""",

"В анализе сетей для задачи PageRank требуется вычисление собственных значений. Для матрицы A = [[5,1,2,3,4],[1,6,10,11,12],[2,10,7,13,14],[3,11,13,8,15],[4,12,14,15,9]] найдите наибольшее собственное значение методом степеней с начальным вектором (1,1,1,1,1) и точностью 10^-4. Как метод степеней использует итерации для нахождения доминирующего собственного значения? Как отношение Релея улучшает оценку? Как зазор между собственными значениями влияет на сходимость? Как модификации, такие как сдвиги, решают эти проблемы?": """
import numpy as np

def power_method(A, x0, tol=1e-4, max_iter=2000):
    n = A.shape[0]
    x = np.zeros(n)
    for i in range(n):
        x[i] = x0[i]*1.0
    s = 0.0
    for i in range(n):
        s += x[i]*x[i]
    s = s**0.5
    for i in range(n):
        x[i] /= s
    lam = 0.0
    for _ in range(max_iter):
        y = np.zeros(n)
        for i in range(n):
            t = 0.0
            for j in range(n):
                t += A[i, j] * x[j]
            y[i] = t
        s = 0.0
        for i in range(n):
            s += y[i]*y[i]
        s = s**0.5
        for i in range(n):
            y[i] /= s
        Ay = np.zeros(n)
        for i in range(n):
            t = 0.0
            for j in range(n):
                t += A[i, j]*y[j]
            Ay[i] = t
        lam_new = 0.0
        for i in range(n):
            lam_new += y[i]*Ay[i]
        diff = 0.0
        for i in range(n):
            d = y[i]-x[i]
            diff += d*d
        diff = diff**0.5
        x = y
        lam = lam_new
        if diff < tol:
            break
    return lam

A = np.array([
    [5, 1, 2, 3, 4],
    [1, 6, 10, 11, 12],
    [2, 10, 7, 13, 14],
    [3, 11, 13, 8, 15],
    [4, 12, 14, 15, 9]
], float)
x0 = np.zeros(5)
for i in range(5):
    x0[i] = 1.0
print("λ_max ≈", round(power_method(A, x0), 6))


Краткий ответ: Итерации усиливают компонент вдоль ведущего собственного вектора, оценку уточняют по Рэлею; сходимость быстрее при большем спектральном зазоре, сдвиги помогают отделить нужную часть спектра.
""",

"В популяционной динамике для моделирования роста бактерий решается ОДУ dy/dt = 0.5y(1 - y/2), y(0)=0.1 на [0;5] с шагом 0.1. Решите уравнение методами Эйлера и Рунге–Кутты 4-го порядка, сравните результаты. Что такое согласованность и устойчивость численных методов для ОДУ? Как они связаны с сходимостью? Как накопление ошибок округления влияет на устойчивость метода Эйлера?": """
import numpy as np
import matplotlib.pyplot as plt

def f(t, y):
    return 0.5*y*(1 - y/2)

t0, t1, h = 0.0, 5.0, 0.1
n = int((t1-t0)/h)+1
t = np.zeros(n)
for i in range(1, n):
    t[i] = t[i-1] + h

def euler(y0):
    y = np.zeros(n)
    y[0] = y0
    for i in range(1, n):
        y[i] = y[i-1] + h*f(t[i-1], y[i-1])
    return y

def rk4(y0):
    y = np.zeros(n)
    y[0] = y0
    for i in range(1, n):
        k1 = f(t[i-1], y[i-1])
        k2 = f(t[i-1]+h/2, y[i-1]+h*k1/2)
        k3 = f(t[i-1]+h/2, y[i-1]+h*k2/2)
        k4 = f(t[i-1]+h, y[i-1]+h*k3)
        y[i] = y[i-1] + h*(k1+2*k2+2*k3+k4)/6
    return y

y0 = 0.1
ye, yr = euler(y0), rk4(y0)
print("Эйлер y(5) =", round(ye[-1],6))
print("РК4   y(5) =", round(yr[-1],6))
print("Разница =", round(abs(yr[-1]-ye[-1]),6))

plt.plot(t, ye, label="Эйлер")
plt.plot(t, yr, label="Рунге–Кутта 4")
plt.grid(True); plt.legend(); plt.show()

Краткий ответ: Согласованность — локальная ошибка →0 при h→0, устойчивость — ошибки не разгоняются итерациями; вместе они дают сходимость. У Эйлера округления и большая ступенька могут накапливаться и уводить решение.
""",

"В физике для моделирования равновесия двух взаимодействующих частиц решается система уравнений x^2 - y = 1, x - y^2 = 0. Используя метод Ньютона с фиксированным якобианом, вычисленным в точке (1.5,1.5), решите систему. Как константа Липшица связана с сходимостью метода функциональной итерации? Объясните, как накопление ошибок округления может повлиять на критерий остановки итерационного процесса.": """
import numpy as np
from numpy import sqrt

def F(x, y):
    return np.array([x*x - y - 1, x - y*y], float)

a, b = 1.5, 1.5
J11, J12 = 2*a, -1.0
J21, J22 = 1.0, -2*b
det = J11*J22 - J12*J21
invJ = np.array([[ J22/det, -J12/det],
                 [-J21/det,  J11/det]], float)

x, y = a, b
tol = 1e-6
for _ in range(100):
    Fx, Fy = F(x, y)
    dx = invJ[0,0]*Fx + invJ[0,1]*Fy
    dy = invJ[1,0]*Fx + invJ[1,1]*Fy
    x -= dx
    y -= dy
    r = sqrt(Fx*Fx + Fy*Fy)
    if r < tol:
        break

print("x ≈", round(x,6), "y ≈", round(y,6))

Краткий ответ: Если у g(x) константа Липшица L<1, то итерации сходятся к фиксированной точке. Округления могут «заморозить» невязку около порога, поэтому критерий остановки лучше сочетать с ограничением числа итераций.
""",

"В обработке изображений для трансформации пикселей используются матричные операции. Реализуйте наивный алгоритм Штрассена для умножения двух квадратных матриц A и B (заданы в условии). Как архитектура памяти компьютера влияет на эффективность алгоритма Штрассена для умножения матриц? Учитывая стандарт IEEE 754, обсудите, как ошибки представления чисел могут повлиять на точность результата.": """
import numpy as np

A = np.array([[4,2,2,0],
              [6,1,9,1],
              [1,3,3,2],
              [2,0,5,4]], float)
B = np.array([[2,5,8,0],
              [3,2,4,1],
              [3,1,5,2],
              [4,6,5,3]], float)

def blk(M,i0,j0):
    return M[i0:i0+2, j0:j0+2]

def mm(A,B):
    r, c = A.shape[0], B.shape[1]
    C = np.zeros((r,c), float)
    for i in range(r):
        for j in range(c):
            s = 0.0
            for k in range(A.shape[1]):
                s += A[i,k]*B[k,j]
            C[i,j] = s
    return C

A11,A12,A21,A22 = blk(A,0,0), blk(A,0,2), blk(A,2,0), blk(A,2,2)
B11,B12,B21,B22 = blk(B,0,0), blk(B,0,2), blk(B,2,0), blk(B,2,2)

M1 = mm(A11 + A22, B11 + B22)
M2 = mm(A21 + A22, B11)
M3 = mm(A11,       B12 - B22)
M4 = mm(A22,       B21 - B11)
M5 = mm(A11 + A12, B22)
M6 = mm(A21 - A11, B11 + B12)
M7 = mm(A12 - A22, B21 + B22)

C = np.zeros((4,4), float)
C[0:2,0:2] = M1 + M4 - M5 + M7
C[0:2,2:4] = M3 + M5
C[2:4,0:2] = M2 + M4
C[2:4,2:4] = M1 + M3 - M2 - M6

for row in C:
    print([round(v,2) for v in row])

Краткий ответ: Рекурсивные блоки Штрассена активно гоняют данные по памяти, и если они не попадают в кэш, время резко растёт. В IEEE 754 суммирования/вычитания многих блоков усиливают округления, поэтому результат может быть менее точным, чем у классики.
""",

"В анализе временных рядов сгенерируйте сигнал длины 16: x[n] = sin(2π·2n/16) + 0.3cos(2π·4n/16) + шум амплитуды 0.05. Вычислите ДПФ и найдите амплитуды компонент 2/16 и 4/16. Постройте амплитудный спектр. Как частота дискретизации влияет на разрешение частотного спектра в ДПФ? Как шум влияет на точность выделения частот? Как можно минимизировать его влияние? Приведите пример применения БПФ в задачах анализа данных.": """
import numpy as np
from numpy import sin, cos, pi, sqrt
import matplotlib.pyplot as plt

N = 16
x = np.zeros(N)
for n in range(N):
    x[n] = sin(2*pi*2*n/N) + 0.3*cos(2*pi*4*n/N) + (np.random.random()-0.5)*0.1

Xr = np.zeros(N)
Xi = np.zeros(N)
for k in range(N):
    sr = 0.0
    si = 0.0
    for n in range(N):
        ang = 2*pi*k*n/N
        sr += x[n]*cos(ang)
        si -= x[n]*sin(ang)
    Xr[k], Xi[k] = sr, si

amp = np.zeros(N)
for k in range(N):
    amp[k] = sqrt(Xr[k]*Xr[k] + Xi[k]*Xi[k])

print("Амплитуда k=2:", round(amp[2],4))
print("Амплитуда k=4:", round(amp[4],4))

plt.stem(range(N), amp, basefmt=" ")
plt.grid(True); plt.title("Амплитудный спектр"); plt.show()

Краткий ответ: Больше отсчётов даёт тоньше шаг по частоте и лучшее разрешение. Шум размывает пики, его гасят окнами, усреднением и увеличением длины записи; БПФ используют для поиска периодичностей/сезонности.
""",

"В анализе данных для восстановления пропущенных значений используются точки (-1,1), (0,0), (1,2). Выполните кубическую сплайн-интерполяцию и найдите значение в точке x=-0.5. Постройте график интерполированной функции. Объясните, как глобальная и локальная интерполяция различаются по подходу и применению. Когда локальная интерполяция предпочтительнее? Объясните влияние переполнения (overflow) на точность.": """
import numpy as np
import matplotlib.pyplot as plt

x = np.array([-1.0, 0.0, 1.0])
y = np.array([ 1.0, 0.0, 2.0])
n = 3
h = np.zeros(n-1)
for i in range(n-1):
    h[i] = x[i+1]-x[i]
alpha = np.zeros(n)
alpha[1] = 3*((y[2]-y[1])/h[1] - (y[1]-y[0])/h[0])
M = np.zeros(n)
M[1] = alpha[1]/(2*(h[0]+h[1]))

def S(xq):
    i = 0 if xq < x[1] else 1
    hi = h[i]
    xi, xi1 = x[i], x[i+1]
    yi, yi1 = y[i], y[i+1]
    Mi, Mi1 = M[i], M[i+1]
    A = xi1 - xq
    B = xq - xi
    return Mi*(A**3)/(6*hi) + Mi1*(B**3)/(6*hi) + (yi/hi - Mi*hi/6)*A + (yi1/hi - Mi1*hi/6)*B

xq = -0.5
yq = S(xq)
print("S(-0.5) =", round(yq,6))

xs = np.linspace(-1,1,200)
ys = np.zeros_like(xs)
for k in range(len(xs)):
    ys[k] = S(xs[k])
plt.plot(xs, ys, label="Сплайн")
plt.scatter(x, y, c="r", label="Узлы")
plt.scatter([xq],[yq], c="k")
plt.grid(True); plt.legend(); plt.show()

Краткий ответ: Глобальный многочлен учитывает все точки сразу и может «колебаться», локальные сплайны гладко сшивают куски и устойчивее. При переполнении операции дают бесконечности и ломают результат, поэтому нужно контролировать масштабы.
""",

"Реализуйте наивный алгоритм умножения двух матриц размером 6×6 A и B, где A и B генерируются случайно с помощью np.random.rand. Выведите результирующую матрицу. Объясните, как алгоритм Штрассена уменьшает количество умножений для матриц. Как это влияет на асимптотическую сложность? Обсудите роль архитектуры памяти в оптимизации матричных операций. Как это влияет на производительность алгоритмов?": """
import numpy as np

A = np.random.random((6,6))
B = np.random.random((6,6))
C = np.zeros((6,6))
for i in range(6):
    for j in range(6):
        s = 0.0
        for k in range(6):
            s += A[i,k]*B[k,j]
        C[i,j] = s
for row in C:
    print(["{0:.4f}".format(v) for v in row])


Краткий ответ: Штрассен заменяет 8 блоковых умножений на 7 и снижает сложность с O(n^3) до около O(n^2.81). Производительность серьёзно зависит от кэша и порядка обхода элементов, поэтому грамотная организация памяти часто решает больше, чем формальная асимптотика.
""",

"В робототехнике высота дрона фиксируется в моменты (0,0), (2,4), (4,10), (6,18). Реализуйте интерполяцию многочленом Лагранжа для оценки высоты в t=3 и постройте график на [0,6]. Как выбор интерполяционных точек влияет на точность многочлена Лагранжа? Объясните, как ошибки представления чисел с плавающей точкой (IEEE 754) могут накапливаться при вычислении многочленов Лагранжа высокой степени. Предложите стратегии для уменьшения этих ошибок.": """
import numpy as np
import matplotlib.pyplot as plt

tx = np.array([0.0, 2.0, 4.0, 6.0])
hy = np.array([0.0, 4.0, 10.0, 18.0])

def lagrange(t):
    n = tx.shape[0]
    s = 0.0
    for i in range(n):
        Li = 1.0
        for j in range(n):
            if j != i:
                Li *= (t - tx[j])/(tx[i]-tx[j])
        s += hy[i]*Li
    return s

tq = 3.0
hq = lagrange(tq)
print("h(3) =", round(hq,6))

ts = np.linspace(0,6,200)
ys = np.zeros_like(ts)
for k in range(ts.shape[0]):
    ys[k] = lagrange(ts[k])

plt.plot(ts, ys, label="Лагранж")
plt.scatter(tx, hy, c="r", label="Узлы")
plt.scatter([tq],[hq], c="k")
plt.grid(True); plt.legend(); plt.show()

Краткий ответ: Неразумные равномерные узлы при большой степени дают осцилляции (эффект Рунге). Ошибки IEEE 754 накапливаются через многие умножения/деления; помогают узлы Чебышева, барицентрическая форма и сплайны.
""",

"В вычислительной линейной алгебре реализуйте наивный алгоритм умножения двух квадратных матриц 8×8 со случайными элементами от 0 до 1. Как алгоритм Штрассена оптимизирует умножение матриц? Как это влияет на сложность в big-O? Почему для малых матриц он может быть менее эффективен? Опишите роль архитектуры памяти в оптимизации матричных операций. Как ошибки представления чисел в формате IEEE 754 влияют на точность умножения матриц?": """
import numpy as np

A = np.random.random((8,8))
B = np.random.random((8,8))
C = np.zeros((8,8))
for i in range(8):
    for j in range(8):
        s = 0.0
        for k in range(8):
            s += A[i,k]*B[k,j]
        C[i,j] = s
for row in C:
    print([round(v,4) for v in row])

Краткий ответ: Штрассен уменьшает число умножений и даёт сложность ≈O(n^2.81), но на малых n проигрывает из-за накладных расходов. Производительность определяется кэшированием и доступом к памяти; из-за IEEE 754 множество операций может немного искажать результат.
""",

"В физике для моделирования затухающего осциллятора решается ОДУ dy/dt = -0.2y + cos(t), y(0)=1 на [0,10] с шагом 0.1. Решите методом Адамса–Мултона и сравните с методом Эйлера. Как порядок точности метода Адамса–Мултона влияет на его точность по сравнению с методом Эйлера? Как локальная ошибка усечения влияет на глобальную ошибку? Опишите понятия слабой и строгой устойчивости численных методов.": """
import numpy as np
import matplotlib.pyplot as plt

f = lambda t,y: -0.2*y + np.cos(t)
t0, tf, h = 0.0, 10.0, 0.1
N = int((tf-t0)/h)+1
t = np.zeros(N)
for i in range(1,N):
    t[i] = t[i-1] + h

ye = np.zeros(N); ye[0] = 1.0
for n in range(N-1):
    ye[n+1] = ye[n] + h*f(t[n], ye[n])

ya = np.zeros(N); ya[0] = 1.0
ya[1] = ya[0] + h*f(t[0], ya[0])
for n in range(1,N-1):
    pred = ya[n] + h*f(t[n], ya[n])
    ya[n+1] = ya[n] + h*(f(t[n], ya[n]) + f(t[n+1], pred))/2

print("y(10) Эйлер:", round(ye[-1],6), "Адамс–Мултон:", round(ya[-1],6))

plt.plot(t, ye, label="Эйлер")
plt.plot(t, ya, label="Адамс–Мултон")
plt.grid(True); plt.legend(); plt.show()

Краткий ответ: У Мултона порядок выше, поэтому глобальная ошибка меньше, чем у Эйлера. Локальная ошибка шаг за шагом суммируется в глобальную; слабая устойчивость — ошибки ограничены, строгая — ошибки затухают при h→0.
""",

"В популяционной динамике для моделирования взаимодействия двух видов решается система x^2 - y = 2, xy = 1. Используя метод Гаусса–Зейделя с начальным приближением (1.5,0.7), найдите решение системы. Сравните метод Гаусса–Зейделя с методом Ньютона для систем по устойчивости к ошибкам округления. Объясните влияние константы Липшица на сходимость.": """
import numpy as np
from numpy import sqrt

x, y = 1.5, 0.7
tol = 1e-6
for k in range(200):
    x_old, y_old = x, y
    x = 1.0/y_old
    y = x*x - 2.0
    if abs(x-x_old)<tol and abs(y-y_old)<tol:
        break
print("x ≈", round(x,6), "y ≈", round(y,6), "итераций:", k+1)

Краткий ответ: Гаусс–Зейдель менее чувствителен к округлениям, но медленнее; Ньютон быстрее, но требовательнее к старту и производным. Если константа Липшица оператора <1, итерации сходятся.
""",

"Для матрицы A = [[15,2,3,4,5],[2,25,6,7,8],[3,6,35,9,10],[4,7,9,45,11],[5,8,10,11,55]] вычислите QR-разложение. Объясните, как разложение по собственным векторам используется в методе главных компонент. Как это связано с вычислением SVD?": """
import numpy as np

A = np.array([
    [15,2,3,4,5],
    [2,25,6,7,8],
    [3,6,35,9,10],
    [4,7,9,45,11],
    [5,8,10,11,55]
], float)
m, n = A.shape
Q = np.zeros((m,n))
R = np.zeros((n,n))
for j in range(n):
    v = np.zeros(m)
    for k in range(m):
        v[k] = A[k,j]
    for i in range(j):
        s = 0.0
        for k in range(m):
            s += Q[k,i]*A[k,j]
        R[i,j] = s
        for k in range(m):
            v[k] -= s*Q[k,i]
    nv = 0.0
    for k in range(m):
        nv += v[k]*v[k]
    nv = nv**0.5
    R[j,j] = nv
    for k in range(m):
        Q[k,j] = v[k]/nv

print("Q:")
for row in Q:
    print(["{0:.4f}".format(v) for v in row])
print("R:")
for row in R:
    print(["{0:.4f}".format(v) for v in row])

Краткий ответ: В PCA собственные векторы ковариации задают оси наибольшей дисперсии, собственные значения — вклад этих осей. Через SVD матрицы данных получают те же направления, поэтому PCA часто делают через SVD.
""",

"Сгенерируйте сигнал длины 64: x[n] = cos(2πn/8) + 0.3sin(2π·2n/8) для n=0..63. Вычислите ДПФ и найдите наиболее значимые компоненты. Постройте амплитудный спектр. Как ограниченная длина сигнала влияет на спектральное разрешение ДПФ? Как можно улучшить разрешение? Объясните, как БПФ применяется в анализе сезонности временных рядов. Как выбор длины сигнала влияет на точность?": """
import numpy as np
from numpy import sin, cos, pi, sqrt
import matplotlib.pyplot as plt

N = 64
x = np.zeros(N)
for n in range(N):
    x[n] = cos(2*pi*n/8) + 0.3*sin(2*pi*2*n/8)

Xr = np.zeros(N); Xi = np.zeros(N)
for k in range(N):
    sr = 0.0; si = 0.0
    for n in range(N):
        a = 2*pi*k*n/N
        sr += x[n]*cos(a)
        si -= x[n]*sin(a)
    Xr[k], Xi[k] = sr, si

amp = np.zeros(N)
for k in range(N):
    amp[k] = sqrt(Xr[k]*Xr[k] + Xi[k]*Xi[k])

print("Пики k=8 и k=16:", round(amp[8],4), round(amp[16],4))

plt.stem(range(N), amp, basefmt=" ")
plt.grid(True); plt.show()

Краткий ответ: Длина записи задаёт шаг по частоте 1/N, так что длиннее сигнал — выше разрешение. Его повышают увеличением N и zero-padding; БПФ применяют для выявления сезонности и циклов, а подходящая длина уменьшает утечки и повышает точность.
""",

"В анализе данных окружающей среды заданы точки (0,15), (2,18), (5,22), (8,20). Реализуйте интерполяцию многочленом Лагранжа для оценки температуры в момент t=4 и постройте график на [0,8]. Объясните, как многочлен Лагранжа обеспечивает точное прохождение через заданные точки. Как степень полинома влияет на точность интерполяции для зашумлённых данных?": """
import numpy as np
import matplotlib.pyplot as plt

x = np.array([0.0,2.0,5.0,8.0])
y = np.array([15.0,18.0,22.0,20.0])
n = len(x)
h = np.zeros(n-1)
for i in range(n-1):
    h[i] = x[i+1]-x[i]
a = np.zeros(n-2)
b = np.zeros(n-2)
c = np.zeros(n-2)
d = np.zeros(n-2)
for i in range(1,n-1):
    a[i-1] = h[i-1]
    b[i-1] = 2*(h[i-1]+h[i])
    c[i-1] = h[i]
    d[i-1] = 6*((y[i+1]-y[i])/h[i] - (y[i]-y[i-1])/h[i-1])

M = np.zeros(n)
if n==4:
    cp0 = c[0]/b[0]
    dp0 = d[0]/b[0]
    b1 = b[1] - a[1]*cp0
    d1 = d[1] - a[1]*dp0
    M[2] = d1/b1
    M[1] = dp0 - cp0*M[2]

def S(xq):
    i = 0
    for k in range(n-1):
        if x[k] <= xq <= x[k+1]:
            i = k
            break
    hi = h[i]
    A = x[i+1]-xq
    B = xq-x[i]
    return M[i]*(A**3)/(6*hi) + M[i+1]*(B**3)/(6*hi) + (y[i]/hi - M[i]*hi/6)*A + (y[i+1]/hi - M[i+1]*hi/6)*B

tq = 4.0
Tq = S(tq)
print("T(4) =", round(Tq,6))
ts = np.linspace(0,8,200)
ys = np.zeros_like(ts)
for k in range(len(ts)):
    ys[k] = S(ts[k])
plt.plot(ts, ys, label="Кубический сплайн")
plt.scatter(x,y,c="r")
plt.scatter([tq],[Tq],c="k")
plt.grid(True); plt.legend(); plt.show()

Краткий ответ: Базисные полиномы равны 1 в своей точке и 0 в остальных, поэтому кривая проходит через все узлы. Высокая степень при шуме переобучается и даёт колебания, поэтому лучше сплайны или меньшая степень.
""",

"Для матриц A=[[3,5,2,1],[6,4,1,5],[1,7,3,2],[3,2,5,4]] и B=[[2,5,3,0],[3,7,4,1],[4,3,5,3],[4,2,3,3]] выполните перемножение методом Штрассена. Задавая матрицы при помощи numpy.random, постройте график зависимости времени выполнения от размера матрицы. Как алгоритм Штрассена уменьшает количество умножений по сравнению с наивным алгоритмом? Как это влияет на асимптотическую сложность? Опишите, как архитектура памяти влияет на производительность алгоритма Штрассена. В каких приложениях он используется?": """
import numpy as np
import time
import matplotlib.pyplot as plt

def mm(A,B):
    r, c = A.shape[0], B.shape[1]
    C = np.zeros((r,c))
    for i in range(r):
        for j in range(c):
            s = 0.0
            for k in range(A.shape[1]):
                s += A[i,k]*B[k,j]
            C[i,j] = s
    return C

def strassen(A,B):
    n = A.shape[0]
    if n==1:
        return A*B
    m = n//2
    A11,A12,A21,A22 = A[:m,:m],A[:m,m:],A[m:,:m],A[m:,m:]
    B11,B12,B21,B22 = B[:m,:m],B[:m,m:],B[m:,:m],B[m:,m:]
    M1 = strassen(A11+A22, B11+B22)
    M2 = strassen(A21+A22, B11)
    M3 = strassen(A11, B12-B22)
    M4 = strassen(A22, B21-B11)
    M5 = strassen(A11+A12, B22)
    M6 = strassen(A21-A11, B11+B12)
    M7 = strassen(A12-A22, B21+B22)
    C = np.zeros((n,n))
    C[:m,:m] = M1 + M4 - M5 + M7
    C[:m,m:] = M3 + M5
    C[m:,:m] = M2 + M4
    C[m:,m:] = M1 - M2 + M3 + M6
    return C

A = np.array([[3,5,2,1],[6,4,1,5],[1,7,3,2],[3,2,5,4]], float)
B = np.array([[2,5,3,0],[3,7,4,1],[4,3,5,3],[4,2,3,3]], float)
print("A·B=\n", strassen(A,B))

sizes = [2,4,8,16]
times = []
for n in sizes:
    M = np.random.random((n,n))
    N = np.random.random((n,n))
    t0 = time.time()
    strassen(M,N)
    times.append(time.time()-t0)

plt.plot(sizes, times, marker="o")
plt.xlabel("Размер n×n"); plt.ylabel("Время (с)")
plt.grid(True); plt.show()

Краткий ответ: За счёт семи умножений вместо восьми сложность падает примерно до O(n^2.81). На малых n накладные расходы сглатывают выгоду. Память и кэш сильно влияют на скорость; применяют при больших матрицах в численных библиотеках и ML.
""",

"Приблизьте вторую производную функции f(x)=x^2 e^{-x} в точке x=1 методом центральной разности с шагом h=0.1 и сравните с точным f''(x)=(4-4x+x^2)e^{-x}. Какова точность центральной разности для аппроксимации второй производной? Как ошибка зависит от шага h? Сравните центральную разность с прямой разностью по точности и вычислительным затратам.": """
import numpy as np
from numpy import exp

f = lambda x: x*x*exp(-x)
f2 = lambda x: (4-4*x+x*x)*exp(-x)
x0 = 1.0
h = 0.1
cent = (f(x0+h)-2*f(x0)+f(x0-h))/(h*h)
fwd  = (f(x0+2*h)-2*f(x0+h)+f(x0))/(h*h)
print("f'' точн =", round(f2(x0),6))
print("центральная =", round(cent,6))
print("прямая =", round(fwd,6))

Краткий ответ: Центральная схема имеет погрешность O(h^2), прямая — O(h). При уменьшении h ошибка центральной падает примерно квадратично, а вычислительная цена сопоставима.
""",

"В физике для моделирования поведения реального газа используется уравнение Ван дер Ваальса (P + a/v^2)(V - b) = R*T. Для газа с a=0.034, b=0.018 при T=300 K и P=1 бар найдите V методом Ньютона с точностью 10^-6. Как выбор начального приближения влияет на сходимость? Сравните метод Ньютона с методом бисекции по скорости сходимости и требованиям к функции. Приведите пример функции, где метод Ньютона может не сойтись.": """
a,b = 0.034, 0.018
R,T,P = 0.08314, 300.0, 1.0
V = 0.03
tol = 1e-6
for _ in range(100):
    f = (P + a/(V*V))*(V-b) - R*T
    fp = (P + a/(V*V)) + (V-b)*(-2.0*a/(V*V*V))
    Vn = V - f/fp
    if abs(Vn-V) < tol:
        V = Vn
        break
    V = Vn
print("V ≈", round(V,6), "л/моль")


Краткий ответ: Старт влияет сильно: плохой выбор ведёт к уходу от корня. Ньютон сходится быстрее (квадратично), но требует производной и удачного старта; бисекция медленнее, зато надёжна при смене знака. Ньютон может срываться, например, для f(x)=x^(1/3) около нуля.
""",

"Для матрицы A=[[2,-1,0],[-1,2,-1],[0,-1,2]] найдите наибольшее собственное значение методом степеней с начальным вектором (1,1,1) и точностью 10^-5. Что такое спектральный радиус матрицы и как он связан с собственными значениями? Как зазор между собственными значениями влияет на сходимость? Как круги Гершгорина помогают оценить собственные значения?": """
A = [[ 2,-1, 0],
     [-1, 2,-1],
     [ 0,-1, 2]]
x = [1.0,1.0,1.0]
s = (x[0]*x[0]+x[1]*x[1]+x[2]*x[2])**0.5
x = [x[i]/s for i in range(3)]
tol = 1e-5
lam = 0.0
for _ in range(2000):
    y = [0.0,0.0,0.0]
    for i in range(3):
        t = 0.0
        for j in range(3):
            t += A[i][j]*x[j]
        y[i] = t
    s = (y[0]*y[0]+y[1]*y[1]+y[2]*y[2])**0.5
    y = [y[i]/s for i in range(3)]
    Ay = [0.0,0.0,0.0]
    for i in range(3):
        t = 0.0
        for j in range(3):
            t += A[i][j]*y[j]
        Ay[i] = t
    lam_new = y[0]*Ay[0] + y[1]*Ay[1] + y[2]*Ay[2]
    d = ((y[0]-x[0])**2 + (y[1]-x[1])**2 + (y[2]-x[2])**2)**0.5
    x = y
    lam = lam_new
    if d < tol:
        break
print("λ_max ≈", round(lam,6))

Краткий ответ: Спектральный радиус — максимум модулей собственных значений; к нему тянется степенной метод. Большой разрыв между первым и вторым значениями ускоряет сходимость; круги Гершгорина дают грубые границы для спектра.
""",

"Решите ОДУ dy/dt = -y + t, y(0)=0 на [0,1] с шагом 0.1 методом Рунге–Кутты 4-го порядка и сравните с точным y(t)=t-1+e^{-t}. Что такое локальная и глобальная ошибки в численных методах для ОДУ? Как порядок точности метода влияет на эти ошибки? Объясните понятие устойчивости численных методов для ОДУ.": """
import numpy as np

def f(t,y):
    return -y + t

h = 0.1
N = int(1.0/h)+1
t = [0.0]*N
y = [0.0]*N
for n in range(N-1):
    tn = t[n]
    yn = y[n]
    k1 = f(tn, yn)
    k2 = f(tn+h/2, yn+h*k1/2)
    k3 = f(tn+h/2, yn+h*k2/2)
    k4 = f(tn+h, yn+h*k3)
    y[n+1] = yn + h*(k1+2*k2+2*k3+k4)/6
    t[n+1] = tn + h

y_ex = [t[i] - 1 + np.exp(-t[i]) for i in range(N)]
print("t=1 RK4:", round(y[-1],6), "точн:", round(y_ex[-1],6))

Краткий ответ: Локальная ошибка — на шаг, глобальная — накопившаяся по интервалу; высокий порядок уменьшает обе при уменьшении h. Устойчивость — способность метода не разгонять ошибки при итерациях.
""",

"В популяционной динамике для системы x^2 - y = 1, x - y^2 = 0 найдите решение методом Гаусса–Зейделя с начальным приближением (1.5,1.5). Сравните метод Гаусса–Зейделя с методом Ньютона по вычислительной сложности и устойчивости к ошибкам округления. В чём разница между абсолютной и относительной погрешностями? Как эти понятия связаны с ошибками округления в арифметике с плавающей точкой?": """
import numpy as np
from numpy import sqrt

x, y = 1.5, 1.5
tol = 1e-6
for it in range(200):
    xn = sqrt(1.0 + y)
    yn = sqrt(xn)
    if abs(xn-x)<tol and abs(yn-y)<tol:
        x, y = xn, yn
        break
    x, y = xn, yn
print("x ≈", round(x,6), "y ≈", round(y,6), "ит:", it+1)

Краткий ответ: Гаусс–Зейдель проще и устойчивее, но медленнее; Ньютон быстрее, но чувствительнее к округлениям и началу. Абсолютная ошибка — разность значений, относительная — доля от истинного значения; округления чаще влияют именно на относительную для маленьких чисел.
""",

"2. Реализуйте наивный алгоритм умножения двух матриц размером 4×4 с элементами np.random.rand. 1) Подсчитайте число скалярных умножений и сложений. 2) Объясните, как изменилось бы число операций при использовании алгоритма Штрассена. 3) Объясните, как архитектура памяти влияет на производительность. 4) Как суммирование по Кахану может улучшить точность накопления при больших суммах?": """
import numpy as np

A = np.random.random((4,4))
B = np.random.random((4,4))
C = np.zeros((4,4))
for i in range(4):
    for j in range(4):
        s = 0.0
        for k in range(4):
            s += A[i,k]*B[k,j]
        C[i,j] = s
for row in C:
    print([round(v,4) for v in row])

mul_ops = 4*4*4
add_ops = 4*4*(4-1)
print("умножений:", mul_ops, "сложений:", add_ops)

Краткий ответ: Наивный метод требует 64 умножения и 48 сложений. У Штрассена рекурсивно 7 умножений блоков вместо 8, но дополнительные сложения дают выгоду лишь на больших размерах. Кэш-локальность и порядок обхода решают многое; Кахан компенсирует потерю младших битов и повышает точность сумм.
""",

"3. Сгенерируйте сигнал длины 32: x[n] = cos(2π·2n/16) + 0.4sin(2π·3n/16), n=0..15. 1) ДПФ и амплитуды для 2/16 и 3/16. 2) Амплитудный спектр. 3) Как fs влияет на разрешение? 4) Как бороться с шумом? 5) Как БПФ снижает сложность?": """
import numpy as np
import matplotlib.pyplot as plt
from numpy import cos, sin, pi, sqrt

N = 16
x = [cos(2*pi*2*n/N) + 0.4*sin(2*pi*3*n/N) for n in range(N)]
Xr = [0.0]*N
Xi = [0.0]*N
for k in range(N):
    sr = 0.0; si = 0.0
    for n in range(N):
        a = 2*pi*k*n/N
        sr += x[n]*cos(a)
        si -= x[n]*sin(a)
    Xr[k], Xi[k] = sr, si
amp = [sqrt(Xr[k]*Xr[k] + Xi[k]*Xi[k]) for k in range(N)]
print("Амплитуды k=2,3:", round(amp[2],4), round(amp[3],4))
plt.stem(range(N), amp, basefmt=" ")
plt.grid(True); plt.show()

Краткий ответ: Частота дискретизации задаёт шаг частотной оси и предел Найквиста. Шум искажает пики; помогают окна и усреднение. БПФ делит сигнал на чётные/нечётные части и переиспользует вычисления, снижая сложность с O(N^2) до O(N log N).
""",

"В экономике для нахождения равновесия в системе x^2 + y^2 = 2, xy = 1 найдите решение методом Ньютона в двумерном случае. Объясните, как ошибка округления может повлиять на сходимость метода Ньютона. Приведите пример функции, где это заметно.": """
x, y = 1.2, 0.8
tol = 1e-6
for it in range(50):
    f1 = x*x + y*y - 2.0
    f2 = x*y - 1.0
    if abs(f1)<tol and abs(f2)<tol:
        break
    J11, J12 = 2*x, 2*y
    J21, J22 = y, x
    det = J11*J22 - J12*J21
    dx = ( J22*f1 - J12*f2)/det
    dy = (-J21*f1 + J11*f2)/det
    x -= dx
    y -= dy
print("x ≈", round(x,6), "y ≈", round(y,6), "итераций:", it+1)

Краткий ответ: Округления искажает шаг и направление, особенно при вырожденном якобиане, что замедляет или срывает сходимость. Например, у f(x)=x^3 рядом с 0 производная мала, и Ньютон становится капризным.
""",

"Реализуйте наивный алгоритм умножения двух квадратных матриц 4×4 (A и B из условия). Какова асимптотическая сложность наивного алгоритма и алгоритма Штрассена для n×n? Как Штрассен уменьшает количество операций? Почему для малых n его преимущества могут быть незначительными? Как архитектура памяти влияет на производительность алгоритма умножения матриц?": """
import numpy as np

A = np.array([[3,5,2,1],
              [6,4,1,5],
              [1,7,3,2],
              [3,2,5,4]], float)
B = np.array([[2,5,3,0],
              [3,7,4,1],
              [4,3,5,3],
              [4,2,3,3]], float)
C = np.zeros((4,4))
for i in range(4):
    for j in range(4):
        s = 0.0
        for k in range(4):
            s += A[i,k]*B[k,j]
        C[i,j] = s
for row in C:
    print([round(v,4) for v in row])
print("умножений:", 64, "сложений:", 48)

Краткий ответ: Наивно O(n^3); у Штрассена ≈O(n^2.81) благодаря 7 блоковым умножениям вместо 8. На малых n накладные расходы и память съедают выигрыш. Память и кэш-локальность критичны для скорости.
""",

"В физике для моделирования гармонического осциллятора решается система dx/dt=y, dy/dt=-x, x(0)=1, y(0)=0 на [0,10] с шагом 0.1 методом Рунге–Кутты 4-го порядка. Постройте фазовый портрет. Как фазовый портрет помогает анализировать системы? Как численные методы могут искажать портрет из-за округления? Сравните явные и неявные методы по устойчивости.": """
import numpy as np
import matplotlib.pyplot as plt

h = 0.1
N = int(10.0/h)+1
t = [0.0]*N
x = [0.0]*N
y = [0.0]*N
x[0], y[0] = 1.0, 0.0
for n in range(N-1):
    xn, yn = x[n], y[n]
    k1x, k1y = yn, -xn
    k2x, k2y = yn + h*k1y/2, -(xn + h*k1x/2)
    k3x, k3y = yn + h*k2y/2, -(xn + h*k2x/2)
    k4x, k4y = yn + h*k3y,   -(xn + h*k3x)
    x[n+1] = xn + h*(k1x+2*k2x+2*k3x+k4x)/6
    y[n+1] = yn + h*(k1y+2*k2y+2*k3y+k4y)/6
    t[n+1] = t[n] + h
plt.plot(x, y)
plt.xlabel("x"); plt.ylabel("y")
plt.grid(True); plt.show()

Краткий ответ: Фазовый портрет наглядно показывает циклы и устойчивость. Из-за округлений идеальный круг может «спиралить»; явные методы менее устойчивы для жёстких задач, неявные стабильнее, но тяжелее вычислительно.
""",

"1. Приблизьте первую производную f(x)=x^3 e^{-x} в x=1 центральной разностью для h=0.1 и h=0.01, сравните с f'(x)=3x^2 e^{-x} - x^3 e^{-x}. Какова точность центральной разности и как она зависит от h? Сравните с прямой разностью.": """
import numpy as np
from numpy import exp

f = lambda x: x**3*exp(-x)
df = lambda x: (3*x*x - x**3)*exp(-x)
x0 = 1.0
for h in [0.1, 0.01]:
    c = (f(x0+h)-f(x0-h))/(2*h)
    print("h =", h, "центральная =", round(c,6), "ошибка =", round(abs(c-df(x0)),8))
print("точное =", round(df(x0),6))

Краткий ответ: Центральная разность точна порядка O(h^2), у прямой — O(h). При меньшем h ошибка центральной падает примерно как h^2, при той же цене она заметно точнее прямой.
""",

"2. Напишите функцию, которая находит собственные векторы методом вращений (Якоби) для матрицы A=[[4,2,2,0],[6,1,9,1],[1,3,3,2],[2,0,5,4]] и проверьте сходимость при ξ=0.001. Почему характеристический многочлен неэффективен? Как зазор между собственными значениями влияет на скорость сходимости?": """
import numpy as np

A = np.array([[4,2,2,0],
              [6,1,9,1],
              [1,3,3,2],
              [2,0,5,4]], float)

def mm(X,Y):
    r, c = X.shape[0], Y.shape[1]
    Z = np.zeros((r,c))
    for i in range(r):
        for j in range(c):
            s = 0.0
            for k in range(X.shape[1]):
                s += X[i,k]*Y[k,j]
            Z[i,j] = s
    return Z

def jacobi(A, eps=1e-3, itmax=200):
    n = A.shape[0]
    V = np.eye(n)
    M = A + 0.0
    for _ in range(itmax):
        off = 0.0; p=0; q=1
        for i in range(n):
            for j in range(i+1,n):
                if abs(M[i,j])>off:
                    off=abs(M[i,j]); p=i; q=j
        if off<eps:
            break
        d = M[q,q]-M[p,p]
        phi = 0.5*np.arctan(2*M[p,q]/(d if d!=0 else 1e-12))
        c,s = np.cos(phi), np.sin(phi)
        J = np.eye(n)
        J[p,p]=c; J[q,q]=c; J[p,q]=s; J[q,p]=-s
        M = mm(mm(J.T,M),J)
        V = mm(V,J)
    vals = np.zeros(n)
    for i in range(n):
        vals[i] = M[i,i]
    return vals, V

vals, vecs = jacobi(A, eps=1e-3)
print("Собственные значения:", vals)
print("Собственные векторы:\n", vecs)


Краткий ответ: Полином нестабилен численно и дорог для больших n. Чем больше разнос собственных значений, тем быстрее сходимость Якоби; близкие значения замедляют вращения.
""",

"3. Решите систему y1'(x)=x^2−y2, y2'(x)=y1+x, y1(0)=1, y2(0)=0 на [1,5] с h=0.1 методом Адамса–Мултона. Постройте фазовый портрет. Сравните с Рунге–Куттой 4-го порядка по точности, сложности, устойчивости и применимости к жёстким системам.": """
import numpy as np
import matplotlib.pyplot as plt

def f(x,y1,y2):
    return np.array([x*x - y2, y1 + x], float)

h = 0.1
x0, xf = 1.0, 5.0
N = int((xf-x0)/h)+1
x = np.linspace(x0, xf, N)
y1 = np.zeros(N); y2 = np.zeros(N)
y1[0], y2[0] = 1.0, 0.0

def rk4_step(x,y1,y2,h):
    k1 = f(x,y1,y2)
    k2 = f(x+h/2, y1+h*k1[0]/2, y2+h*k1[1]/2)
    k3 = f(x+h/2, y1+h*k2[0]/2, y2+h*k2[1]/2)
    k4 = f(x+h, y1+h*k3[0], y2+h*k3[1])
    return np.array([y1,y2]) + h*(k1+2*k2+2*k3+k4)/6

y1[1], y2[1] = rk4_step(x[0], y1[0], y2[0], h)
for n in range(1,N-1):
    pred = np.array([y1[n],y2[n]]) + h*f(x[n], y1[n], y2[n])
    corr = np.array([y1[n],y2[n]]) + h*(f(x[n], y1[n], y2[n]) + f(x[n+1], pred[0], pred[1]))/2
    y1[n+1], y2[n+1] = corr

plt.plot(y1, y2, "-o")
plt.xlabel("y1"); plt.ylabel("y2")
plt.grid(True); plt.show()

Краткий ответ: РК4 даёт O(h^4) глобальную точность, Адамс–Мултон может быть дешевле на шаг и более устойчив при длинных расчётах и жёстких системах, но требует истории и предиктора. Для универсальности берут РК4, для продолжительных и жёстких задач — многошаговые Мултона.
""",

"2. В финансовом моделировании для уравнения f(x)=e^x−x−2=0 найдите корень модифицированным Ньютоном с фиксированным df(1) на [1,2] (точность 10^-4). Сравните с обычным Ньютоном. Когда модифицированный метод предпочтителен? Как ошибки в производной влияют на сходимость?": """
import numpy as np

f = lambda x: np.exp(x) - x - 2.0
df = lambda x: np.exp(x) - 1.0

x = 1.5
tol = 1e-4
jac = df(1.0)
for it in range(1000):
    xn = x - f(x)/jac
    if abs(xn-x) < tol:
        x = xn
        break
    x = xn
print("Модифицированный Ньютон:", round(x,6), "итераций:", it+1)

x = 1.5
for it in range(1000):
    xn = x - f(x)/df(x)
    if abs(xn-x) < tol:
        x = xn
        break
    x = xn
print("Обычный Ньютон:", round(x,6), "итераций:", it+1)

Краткий ответ: Модифицированный Ньютон хорош, когда производную считать дорого или она почти не меняется; скорость линейная. Ошибочная производная замедляет сходимость и может сорвать её.
""",

"2. Для матрицы A=[[5,1,2,3],[1,7,2,9],[2,4,7,10],[3,9,10,8]] вычислите разложение Шура и проверьте нормальность матрицы (без scipy.linalg).": """
import numpy as np

A = np.array([[5,1,2,3],
              [1,7,2,9],
              [2,4,7,10],
              [3,9,10,8]], float)

def mm(X,Y):
    r, c = X.shape[0], Y.shape[1]
    Z = np.zeros((r,c))
    for i in range(r):
        for j in range(c):
            s = 0.0
            for k in range(X.shape[1]):
                s += X[i,k]*Y[k,j]
            Z[i,j] = s
    return Z

def qr_gs(M):
    m, n = M.shape
    Q = np.zeros((m,n))
    R = np.zeros((n,n))
    for j in range(n):
        v = np.zeros(m)
        for k in range(m):
            v[k] = M[k,j]
        for i in range(j):
            s = 0.0
            for k in range(m):
                s += Q[k,i]*M[k,j]
            R[i,j] = s
            for k in range(m):
                v[k] -= s*Q[k,i]
        nv = 0.0
        for k in range(m):
            nv += v[k]*v[k]
        nv = nv**0.5
        R[j,j] = nv
        for k in range(m):
            Q[k,j] = v[k]/nv
    return Q, R

def schur_qr(A, iters=80):
    M = A + 0.0
    Z = np.eye(A.shape[0])
    for _ in range(iters):
        Q,R = qr_gs(M)
        M = mm(R,Q)
        Z = mm(Z,Q)
    return M, Z

T, Z = schur_qr(A, iters=80)
print("T:\n", T)
print("Z:\n", Z)

AA = mm(A, np.transpose(A))
AT = mm(np.transpose(A), A)
diff = 0.0
m, n = AA.shape
for i in range(m):
    for j in range(n):
        diff += abs(AA[i,j]-AT[i,j])
diff = diff/(m*n)
print("Нормальная?", diff < 1e-6)

Краткий ответ: Получили квазитреугольную T и ортогональную Z так, что A≈ZTZ^T; матрица нормальна, если AA^T=A^TA, что мы проверили по средней разности. Большинство матриц не нормальны.
""",

"В экономике для моделирования равновесной цены решается уравнение f(x)=x^3 - 2x - 2=0. Используя метод бисекции, найдите корень на [1,2] с точностью 10^-5. Как минимизировать число итераций? Что такое верные цифры в строгом и широком смысле и как это связано с округлением и значащими цифрами?": """
def f(x): return x**3 - 3*x + 2
def df(x): return 3*x*x - 3

def mod_newton(x0, tol=1e-5, itmax=1000):
    j = df(x0)
    x = x0
    for _ in range(itmax):
        xn = x - f(x)/j
        if abs(xn-x) < tol:
            return xn
        x = xn
    return x

r1 = mod_newton(0.5)
r2 = mod_newton(-1.5)
print("Корни ≈", round(r1,6), round(r2,6))

Краткий ответ: Лучше выбирать как можно более узкий стартовый интервал со сменой знака и останавливать по длине отрезка. Верные цифры в строгом смысле полностью совпадают с истинными, в широком — допускают небольшое отклонение из-за округления; это и есть связь со значащими цифрами.
""",

"Реализуйте степенной метод со сдвигом для нахождения наибольшего собственного значения матрицы A=[[5,1,5,3],[1,5,2,4],[5,2,4,-2],[3,4,-2,5]] с точностью 10^-4. Как зазор между собственными значениями влияет на скорость сходимости? Как округления IEEE754 влияют на точность и как снизить их влияние?": """
import numpy as np

A = np.array([[5,1,5,3],
              [1,5,2,4],
              [5,2,4,-2],
              [3,4,-2,5]], float)

x = np.zeros(4)
for i in range(4):
    x[i] = 1.0
mu = 0.0
tol = 1e-4

def mv(A,v):
    n = A.shape[0]
    y = np.zeros(n)
    for i in range(n):
        s = 0.0
        for j in range(n):
            s += A[i,j]*v[j]
        y[i] = s
    return y

for it in range(2000):
    y = mv(A,x) - mu*x
    s = 0.0
    for i in range(4):
        s += y[i]*y[i]
    s = s**0.5
    y = y/s
    Ay = mv(A,y)
    lam = 0.0
    for i in range(4):
        lam += y[i]*Ay[i]
    d = 0.0
    for i in range(4):
        d += (y[i]-x[i])*(y[i]-x[i])
    if d**0.5 < tol:
        break
    x = y
print("λ_max ≈", round(lam,6), "итераций:", it+1)

Краткий ответ: Чем больше разрыв между доминирующими собственными значениями, тем быстрее сходится метод. Округления смещают направление; помогает нормировка каждый шаг, двойная точность и ортогонализация при поиске нескольких векторов.
""",

"Решите ОДУ dy/dt = -y + sin(t), y(0)=0 на [0,2π] с шагом h=π/20 методом Эйлера, постройте график решения. Как фазовые портреты помогают в анализе ОДУ? Может ли адаптивный шаг улучшить точность и как это связано с устойчивостью и округлением?": """
import numpy as np
import matplotlib.pyplot as plt

f = lambda t,y: -y + np.sin(t)
h = np.pi/20
N = int(2*np.pi/h)+1
t = np.linspace(0, 2*np.pi, N)
y = np.zeros(N)
for n in range(N-1):
    y[n+1] = y[n] + h*f(t[n], y[n])
plt.plot(t, y)
plt.grid(True); plt.show()

Краткий ответ: Фазовый портрет показывает устойчивость и характер движения. Адаптивный шаг уменьшает ошибку на крутых участках и экономит вычисления на гладких, что повышает устойчивость и снижает накопление округлений.
""",

"1. Реализуйте кубическую сплайн-интерполяцию: точки (0,15),(2,18),(5,22),(8,20). Найдите T(4) и постройте график. Объясните про Лагранжа и шум.": """
import numpy as np
import matplotlib.pyplot as plt

x = np.array([0.0,2.0,5.0,8.0])
y = np.array([15.0,18.0,22.0,20.0])
n = len(x)
h = np.zeros(n-1)
for i in range(n-1):
    h[i] = x[i+1]-x[i]
a = np.zeros(n-2)
b = np.zeros(n-2)
c = np.zeros(n-2)
d = np.zeros(n-2)
for i in range(1,n-1):
    a[i-1] = h[i-1]
    b[i-1] = 2*(h[i-1]+h[i])
    c[i-1] = h[i]
    d[i-1] = 6*((y[i+1]-y[i])/h[i] - (y[i]-y[i-1])/h[i-1])
for i in range(1,n-2):
    w = a[i]/b[i-1]
    b[i] -= w*c[i-1]
    d[i] -= w*d[i-1]
M = np.zeros(n)
if n>2:
    M[1] = d[0]/b[0]
for i in range(n-3,0,-1):
    M[i] = (d[i-1]-c[i-1]*M[i+1])/b[i-1]

def S(xq):
    i = 0
    for k in range(n-1):
        if x[k] <= xq <= x[k+1]:
            i = k
            break
    hi = h[i]
    A = (x[i+1]-xq)
    B = (xq-x[i])
    return M[i]*(A**3)/(6*hi) + M[i+1]*(B**3)/(6*hi) + (y[i]/hi - M[i]*hi/6)*A + (y[i+1]/hi - M[i+1]*hi/6)*B

tq = 4.0
Tq = S(tq)
print("T(4) =", round(Tq,6))
ts = np.linspace(0,8,200)
ys = np.zeros_like(ts)
for k in range(len(ts)):
    ys[k] = S(ts[k])
plt.plot(ts, ys, label="Кубический сплайн")
plt.scatter(x,y,c="r")
plt.scatter([tq],[Tq],c="k")
plt.grid(True); plt.legend(); plt.show()

Краткий ответ: Полином Лагранжа проходит точно через узлы, потому что его базисы построены так. Для шумных данных высокая степень даёт колебания, поэтому лучше сплайны или аппроксимацию по МНК.
""",

"2. Реализуйте функцию Якоби для нахождения собственного значения на квадратной матрице A с tolerance=1e-6": """
import numpy as np

def mm(X,Y):
    r, c = X.shape[0], Y.shape[1]
    Z = np.zeros((r,c))
    for i in range(r):
        for j in range(c):
            s = 0.0
            for k in range(X.shape[1]):
                s += X[i,k]*Y[k,j]
            Z[i,j] = s
    return Z

def jacobi_eigen(A, tol=1e-6, max_iter=500):
    n = A.shape[0]
    V = np.eye(n)
    M = A + 0.0
    for _ in range(max_iter):
        off = 0.0; p=0; q=1
        for i in range(n):
            for j in range(i+1,n):
                if abs(M[i,j])>off:
                    off=abs(M[i,j]); p=i; q=j
        if off<tol:
            break
        d = M[q,q]-M[p,p]
        phi = 0.5*np.arctan(2*M[p,q]/(d if d!=0 else 1e-12))
        c,s = np.cos(phi), np.sin(phi)
        J = np.eye(n)
        J[p,p]=c; J[q,q]=c; J[p,q]=s; J[q,p]=-s
        M = mm(mm(J.T,M),J)
        V = mm(V,J)
    vals = np.zeros(n)
    for i in range(n):
        vals[i] = M[i,i]
    return vals, V

A = np.array([[4,1,1],[1,3,0],[1,0,2]], float)
vals, vecs = jacobi_eigen(A, tol=1e-6)
print("Eigenvalues:", vals)
print("Vectors:\n", vecs)
""",

"Для матриц A=[[3,5,2,1],[6,1,7,5],[1,7,3,2],[3,2,5,4]] и B=[[2,5,3,0],[3,7,4,1],[4,3,5,3],[4,2,3,3]] выполните Штрассена и постройте время vs размер. Как уменьшаются умножения и где применяется?": """
import numpy as np
import time
import matplotlib.pyplot as plt

def mm(A,B):
    r,c = A.shape[0], B.shape[1]
    C = np.zeros((r,c))
    for i in range(r):
        for j in range(c):
            s = 0.0
            for k in range(A.shape[1]):
                s += A[i,k]*B[k,j]
            C[i,j] = s
    return C

def strassen(A,B):
    n = A.shape[0]
    if n==1:
        return A*B
    m = n//2
    A11,A12,A21,A22 = A[:m,:m],A[:m,m:],A[m:,:m],A[m:,m:]
    B11,B12,B21,B22 = B[:m,:m],B[:m,m:],B[m:,:m],B[m:,m:]
    M1 = strassen(A11+A22, B11+B22)
    M2 = strassen(A21+A22, B11)
    M3 = strassen(A11, B12-B22)
    M4 = strassen(A22, B21-B11)
    M5 = strassen(A11+A12, B22)
    M6 = strassen(A21-A11, B11+B12)
    M7 = strassen(A12-A22, B21+B22)
    C = np.zeros((n,n))
    C[:m,:m] = M1+M4-M5+M7
    C[:m,m:] = M3+M5
    C[m:,:m] = M2+M4
    C[m:,m:] = M1-M2+M3+M6
    return C

A = np.array([[3,5,2,1],[6,1,7,5],[1,7,3,2],[3,2,5,4]], float)
B = np.array([[2,5,3,0],[3,7,4,1],[4,3,5,3],[4,2,3,3]], float)
print("A·B=\n", strassen(A,B))

sizes=[2,4,8,16]
times=[]
for n in sizes:
    M1=np.random.random((n,n))
    M2=np.random.random((n,n))
    t0=time.time(); strassen(M1,M2); times.append(time.time()-t0)
plt.plot(sizes,times,marker="o"); plt.grid(True); plt.show()

Краткий ответ: 7 умножений против 8 и сложность ≈O(n^2.81); ускорение видно на больших матрицах и хорошей кэш-локальности. Используется в больших численных пакетах и ML.
""",

"1. В физике для нахождения точки равновесия системы решается уравнение f(x)=x^3-3x+2=0. Используя модифицированный метод Ньютона найдите корни с точностью 10^-5. Почему метод Ньютона может не сойтись и как влияет IEEE 754?": """
def f(x): return x**3 - 3*x + 2
def df(x): return 3*x*x - 3
x = 0.5
tol = 1e-5
for _ in range(1000):
    xn = x - f(x)/df(1.0)
    if abs(xn-x) < tol:
        x = xn
        break
    x = xn
print("Корень ≈", round(x,6))

Краткий ответ: Ньютон срывается при малой производной или неудачном старте. Округления IEEE 754 добавляют шум в шаг, что может замедлять сходимость или уводить от корня.
""",

"2. Реализуйте алгоритм Штрассена для умножения двух матриц 8×8, где A и B генерируются случайно; постройте график времени vs размер. Как он уменьшает умножения и где полезен?": """
import numpy as np
import time
import matplotlib.pyplot as plt

def strassen(A,B):
    n = A.shape[0]
    if n==1: return A*B
    m = n//2
    A11,A12,A21,A22 = A[:m,:m],A[:m,m:],A[m:,:m],A[m:,m:]
    B11,B12,B21,B22 = B[:m,:m],B[:m,m:],B[m:,:m],B[m:,m:]
    M1 = strassen(A11+A22, B11+B22)
    M2 = strassen(A21+A22, B11)
    M3 = strassen(A11, B12-B22)
    M4 = strassen(A22, B21-B11)
    M5 = strassen(A11+A12, B22)
    M6 = strassen(A21-A11, B11+B12)
    M7 = strassen(A12-A22, B21+B22)
    C = np.zeros((n,n))
    C[:m,:m] = M1+M4-M5+M7
    C[:m,m:] = M3+M5
    C[m:,:m] = M2+M4
    C[m:,m:] = M1-M2+M3+M6
    return C

A = np.random.rand(8,8)
B = np.random.rand(8,8)
C = strassen(A,B)
for i in range(C.shape[0]):
    print([round(C[i,j],3) for j in range(C.shape[1])])

sizes=[2,4,8,16]
times=[]
for n in sizes:
    M=np.random.rand(n,n); N=np.random.rand(n,n)
    t0=time.time(); strassen(M,N); times.append(time.time()-t0)
plt.plot(sizes,times,marker="o"); plt.grid(True); plt.show()

Краткий ответ: За счёт 7 умножений вместо 8 и рекурсии сложность ≈O(n^2.81). Полезен для очень больших матриц и хорошей памяти; на малых размерностях выигрыш невелик.
"""
}
