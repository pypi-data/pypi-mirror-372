{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646ff709-849c-4236-a5c9-e83596d21333",
   "metadata": {},
   "source": [
    "# Manipulating an HDF5 file with python. \n",
    "\n",
    "The aim of this notebook is to teach how to do basic manipulation (read / write) on an HDF5 file. This uses the SWAXSanalysis.utils module and some basic h5py functions. This is very base level, even the functions in the utils module use h5py only.\n",
    "\n",
    "An example HDF5 file is provided in the **.\\Data Treatment Center\\Jupyter notebooks\\NoteBook\\Example HDF5** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ff96d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:silx.opencl.common:Unable to import pyOpenCl. Please install it from: https://pypi.org/project/pyopencl\n",
      "WARNING:pyFAI.DEPRECATION:Module pyFAI.azimuthalIntegrator is deprecated since pyFAI version 2024.10. Use 'pyFAI.integrator.azimuthal' instead.\n",
      "  File \"C:\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 752, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\AT280565\\AppData\\Local\\Temp\\ipykernel_4364\\1544120339.py\", line 10, in <module>\n",
      "    from SWAXSanalysis.class_nexus_file import NexusFile\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\SWAXSanalysis\\class_nexus_file.py\", line 17, in <module>\n",
      "    from smi_analysis import SMI_beamline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\AT280565\\Desktop\\Data Treatment Center\\Jupyter notebooks\\.venv\\lib\\site-packages\\smi_analysis\\SMI_beamline.py\", line 2, in <module>\n",
      "    from pyFAI import azimuthalIntegrator\n",
      "  File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "from SWAXSanalysis.utils import explore_file, extract_from_h5, replace_h5_dataset\n",
    "from SWAXSanalysis.class_nexus_file import NexusFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de75a51",
   "metadata": {},
   "source": [
    "# Inspecting your h5 file\n",
    "\n",
    "The function `explore_file` can be used to visualize the structure of the HDF5 file you want to treat. Thanks to this function you'll be able to know precisely where everything is. alternatively, you can use HDFView to visualize your HDF5 file.\n",
    "There are three types of element :\n",
    "\n",
    "    - GROUPS : you can view them as a directory, it can contain other groups, dataset or attributes\n",
    "    - data_sets : you can view them as a file, it can only contain attributes\n",
    "    - @attributes : you can view them as metadata, it cannot contain anything and gives additional information\n",
    "    \n",
    "To open an HDF5 file in a python script you need to use the `h5py` library and use the `with h5py.File(\"path\", \"r\") as file:`, indent to get access to the file variable (which is an h5py object) and unindent when you're done. This ensures that the file is properly opened and closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52cf1e59-cfa3-4eb6-8e9c-8409c1d34576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring HDF5 structure...\n",
      "\n",
      "├──Group : ENTRY\n",
      "|  ├──Group : ENTRY/COLLECTION\n",
      "|  |  ├──Dataset : ENTRY/COLLECTION/do_absolute_intensity\n",
      "|  |  ├──Dataset : ENTRY/COLLECTION/experiment_type\n",
      "|  |  ├──Dataset : ENTRY/COLLECTION/exposition_time\n",
      "|  |  ├──Dataset : ENTRY/COLLECTION/geometry\n",
      "|  |  ├──Dataset : ENTRY/COLLECTION/sample_fixture\n",
      "|  |  ├──Dataset : ENTRY/COLLECTION/username\n",
      "|  ├──Group : ENTRY/DATA\n",
      "|  |  ├──Dataset : ENTRY/DATA/I\n",
      "|  |  ├──Dataset : ENTRY/DATA/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA/Qmean\n",
      "|  |  ├──Dataset : ENTRY/DATA/mask\n",
      "|  ├──Group : ENTRY/DATA_ABS\n",
      "|  |  ├──Dataset : ENTRY/DATA_ABS/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_ABS/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_ABS/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA_ABS/Qmean\n",
      "|  |  ├──Dataset : ENTRY/DATA_ABS/mask\n",
      "|  ├──Group : ENTRY/DATA_AZI_AVG\n",
      "|  |  ├──Dataset : ENTRY/DATA_AZI_AVG/Chi\n",
      "|  |  ├──Dataset : ENTRY/DATA_AZI_AVG/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_AZI_AVG/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_AZI_AVG/Qdev\n",
      "|  |  ├──Dataset : ENTRY/DATA_AZI_AVG/Qmean\n",
      "|  ├──Group : ENTRY/DATA_CAKED\n",
      "|  |  ├──Dataset : ENTRY/DATA_CAKED/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_CAKED/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_CAKED/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA_CAKED/Qmean\n",
      "|  |  ├──Dataset : ENTRY/DATA_CAKED/mask\n",
      "|  ├──Group : ENTRY/DATA_HOR_INT\n",
      "|  |  ├──Dataset : ENTRY/DATA_HOR_INT/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_HOR_INT/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_HOR_INT/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA_HOR_INT/Qdev\n",
      "|  |  ├──Dataset : ENTRY/DATA_HOR_INT/Qmean\n",
      "|  ├──Group : ENTRY/DATA_Q_SPACE\n",
      "|  |  ├──Dataset : ENTRY/DATA_Q_SPACE/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_Q_SPACE/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_Q_SPACE/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA_Q_SPACE/Qmean\n",
      "|  |  ├──Dataset : ENTRY/DATA_Q_SPACE/mask\n",
      "|  ├──Group : ENTRY/DATA_RAD_AVG\n",
      "|  |  ├──Dataset : ENTRY/DATA_RAD_AVG/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_RAD_AVG/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_RAD_AVG/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA_RAD_AVG/Qdev\n",
      "|  |  ├──Dataset : ENTRY/DATA_RAD_AVG/Qmean\n",
      "|  ├──Group : ENTRY/DATA_VER_INT\n",
      "|  |  ├──Dataset : ENTRY/DATA_VER_INT/I\n",
      "|  |  ├──Dataset : ENTRY/DATA_VER_INT/Idev\n",
      "|  |  ├──Dataset : ENTRY/DATA_VER_INT/Q\n",
      "|  |  ├──Dataset : ENTRY/DATA_VER_INT/Qdev\n",
      "|  |  ├──Dataset : ENTRY/DATA_VER_INT/Qmean\n",
      "|  ├──Group : ENTRY/INSTRUMENT\n",
      "|  |  ├──Group : ENTRY/INSTRUMENT/APERTURE\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/APERTURE/shape\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/APERTURE/x_gap\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/APERTURE/y_gap\n",
      "|  |  ├──Group : ENTRY/INSTRUMENT/COLLIMATOR\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/COLLIMATOR/distance\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/COLLIMATOR/length\n",
      "|  |  ├──Group : ENTRY/INSTRUMENT/DETECTOR\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/SDD\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/beam_center_x\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/beam_center_y\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/name\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/pitch\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/roll\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/slit_length\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/x_pixel_size\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/x_position\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/y_pixel_size\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/y_position\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/DETECTOR/yaw\n",
      "|  |  ├──Group : ENTRY/INSTRUMENT/SOURCE\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/beam_size_x\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/beam_size_y\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/incident_wavelength\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/probe\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/type\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/wavelength_max\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/wavelength_min\n",
      "|  |  |  ├──Dataset : ENTRY/INSTRUMENT/SOURCE/wavelength_spread\n",
      "|  ├──Group : ENTRY/PROCESS\n",
      "|  |  ├──Dataset : ENTRY/PROCESS/date\n",
      "|  |  ├──Dataset : ENTRY/PROCESS/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS/name\n",
      "|  ├──Group : ENTRY/PROCESS_ABS\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_ABS/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_ABS/name\n",
      "|  ├──Group : ENTRY/PROCESS_AZI_AVG\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_AZI_AVG/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_AZI_AVG/name\n",
      "|  ├──Group : ENTRY/PROCESS_CAKED\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_CAKED/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_CAKED/name\n",
      "|  ├──Group : ENTRY/PROCESS_HOR_INT\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_HOR_INT/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_HOR_INT/name\n",
      "|  ├──Group : ENTRY/PROCESS_Q_SPACE\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_Q_SPACE/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_Q_SPACE/name\n",
      "|  ├──Group : ENTRY/PROCESS_RAD_AVG\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_RAD_AVG/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_RAD_AVG/name\n",
      "|  ├──Group : ENTRY/PROCESS_VER_INT\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_VER_INT/description\n",
      "|  |  ├──Dataset : ENTRY/PROCESS_VER_INT/name\n",
      "|  ├──Group : ENTRY/SAMPLE\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/pitch\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/roll\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/temperature\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/thickness\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/transmission\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/x_position\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/y_position\n",
      "|  |  ├──Dataset : ENTRY/SAMPLE/yaw\n",
      "|  ├──Dataset : ENTRY/definition\n",
      "|  ├──Dataset : ENTRY/run\n",
      "|  ├──Dataset : ENTRY/title\n"
     ]
    }
   ],
   "source": [
    "example_hdf5_path =  Path(r\".\\Example HDF5\\testSample_SAXS_00001.h5\")\n",
    "\n",
    "with h5py.File(example_hdf5_path, \"r\") as file_object:\n",
    "    explore_file(file_object, explore_group=True, explore_attribute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fa21a-6a9b-41ee-b076-a3bc50ab2413",
   "metadata": {},
   "source": [
    "# Getting the value of a dataset\n",
    "We can now see what's inside the HDF5 file. In case we want to change something in this file, we need to get it's path in the HDF5 file.\n",
    "\n",
    "As an example, let's get the value of the source's wavelength. This parameter, \"incident_wavelength\", is present in the \"SOURCE\" group, itslef inside the \"Instrument\" group, itself inside the \"ENTRY\" group.\n",
    "\n",
    "The path of the \"wavelength\" element is thus :\\\n",
    "**ENTRY/INSTRUMENT/SOURCE/incident_wavelength**\n",
    "\n",
    "### Warning\n",
    "If the value you're extracting is a string of characters, you need to decode it via the `.decode()` method !\n",
    "\n",
    "Now that you have the path you can use the `extract_from_h5` function to get the value. here is how to do it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c4cad3-a13c-4379-8bb7-2f22bc60c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident wavelength = 0.9999999999999999 nm\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(example_hdf5_path, \"r\") as file_object:\n",
    "    wavelength_value = extract_from_h5(\n",
    "        file_object, \n",
    "        h5path=\"ENTRY/INSTRUMENT/SOURCE/incident_wavelength\", \n",
    "        data_type=\"dataset\"\n",
    "    )\n",
    "    wavelength_unit = extract_from_h5(\n",
    "        file_object,\n",
    "        h5path=\"ENTRY/INSTRUMENT/SOURCE/incident_wavelength\", \n",
    "        data_type=\"attribute\", \n",
    "        attribute_name=\"units\"\n",
    "    )\n",
    "\n",
    "# We can then use those values\n",
    "print(f\"Incident wavelength = {wavelength_value} {wavelength_unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76026ca4-06f3-4cc2-87b3-9104e3679ffb",
   "metadata": {},
   "source": [
    "# Changing the value of a Dataset or Attribute.\n",
    "Let's say a mistake has been made during calibration and it affected the results. If you want to change the faulty value you can use the function `replace_h5_dataset`. \n",
    "\n",
    "To use it, you need to prive a file opened as an h5py object, using the same command as before, except this time we do not use `\"r\"`, which stands for read, but `\"r+\"` which stands for read/write. Then, we provide the arguments to the function and let it do it's thing.\n",
    "\n",
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e505a3-267e-4880-90d1-1ccc305a1df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDD before change : 1.0 arbitrary\n",
      "SDD after change : 0.975 m\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(example_hdf5_path, \"r+\") as file_object:\n",
    "    # We check the value\n",
    "    sdd_value = extract_from_h5(\n",
    "        file_object, \n",
    "        h5path=\"ENTRY/INSTRUMENT/DETECTOR/SDD\", \n",
    "        data_type=\"dataset\"\n",
    "    )\n",
    "    sdd_unit = extract_from_h5(\n",
    "        file_object,\n",
    "        h5path=\"ENTRY/INSTRUMENT/DETECTOR/SDD\", \n",
    "        data_type=\"attribute\", \n",
    "        attribute_name=\"units\"\n",
    "    )\n",
    "    print(f\"SDD before change : {sdd_value} {sdd_unit}\")\n",
    "    \n",
    "\n",
    "    # We change the value\n",
    "    replace_h5_dataset(\n",
    "        file_object,\n",
    "        old_h5path=\"ENTRY/INSTRUMENT/DETECTOR/SDD\",\n",
    "        new_dataset=0.975\n",
    "    )\n",
    "\n",
    "    # We change the unit\n",
    "    attributes_dict = file_object[\"ENTRY/INSTRUMENT/DETECTOR/SDD\"].attrs\n",
    "    attributes_dict[\"units\"] = \"m\"\n",
    "\n",
    "\n",
    "    # We check the value again\n",
    "    sdd_value = extract_from_h5(\n",
    "        file_object, \n",
    "        h5path=\"ENTRY/INSTRUMENT/DETECTOR/SDD\", \n",
    "        data_type=\"dataset\"\n",
    "    )\n",
    "    sdd_unit = extract_from_h5(\n",
    "        file_object,\n",
    "        h5path=\"ENTRY/INSTRUMENT/DETECTOR/SDD\", \n",
    "        data_type=\"attribute\", \n",
    "        attribute_name=\"units\"\n",
    "    )\n",
    "    print(f\"SDD after change : {sdd_value} {sdd_unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44068b26",
   "metadata": {},
   "source": [
    "# Troubleshooting\n",
    "\n",
    "1. In case you forgot to use the try: / finally: environement and you're having some error saying a file is already used by a process : Close the notebook and the command invite and go to the directory where your files are, there should be a .tmp file, you can delete it and reopen the notebook. To avoid this error please use the try: / environement: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c93fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
