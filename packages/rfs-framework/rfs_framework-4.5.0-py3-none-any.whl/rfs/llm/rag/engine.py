"""
RAG Engine

ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval Augmented Generation) ì—”ì§„ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.
ë²¡í„° ìŠ¤í† ì–´ì™€ LLM Manager, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í†µí•©í•˜ì—¬ 
ì§€ì‹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

from typing import List, Dict, Any, Optional, Union
from rfs.core.result import Result, Success, Failure
from rfs.core.annotations import Service
from rfs.hof.core import pipe
from .vector_store import VectorStore


@Service("llm_service")
class RAGEngine:
    """RAG (Retrieval Augmented Generation) ì—”ì§„
    
    ë²¡í„° ê²€ìƒ‰ê³¼ LLM ìƒì„±ì„ ê²°í•©í•˜ì—¬ ì§€ì‹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
    Result Patternê³¼ HOF íŒ¨í„´ì„ ì™„ì „íˆ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        llm_manager: 'LLMManager',
        vector_store: VectorStore,
        template_manager: Optional['PromptTemplateManager'] = None
    ):
        self.llm_manager = llm_manager
        self.vector_store = vector_store
        self.template_manager = template_manager
        
        # ê¸°ë³¸ RAG í…œí”Œë¦¿ ë“±ë¡
        if self.template_manager:
            self._register_default_templates()
    
    def _register_default_templates(self):
        """ê¸°ë³¸ RAG í…œí”Œë¦¿ë“¤ì„ ë“±ë¡í•©ë‹ˆë‹¤"""
        try:
            # ê¸°ë³¸ RAG í…œí”Œë¦¿
            basic_rag_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{% for doc in context_documents %}
{{ doc.content }}
---
{% endfor %}

ì§ˆë¬¸: {{ question }}

ë‹µë³€: ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤."""

            # ìƒì„¸ RAG í…œí”Œë¦¿
            detailed_rag_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{% for doc in context_documents %}
ğŸ“„ ë¬¸ì„œ {{ loop.index }}: {{ doc.metadata.get('title', 'ì œëª© ì—†ìŒ') }}
{{ doc.content }}
{% if doc.metadata %}
ë©”íƒ€ì •ë³´: {{ doc.metadata }}
{% endif %}

---
{% endfor %}

ì§ˆë¬¸: {{ question }}

{% if instruction %}
ë‹µë³€ ì§€ì¹¨: {{ instruction }}
{% endif %}

ë‹µë³€:"""

            # ìš”ì•½ RAG í…œí”Œë¦¿
            summary_rag_template = """ë‹¤ìŒ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ {{ question }}ì— ëŒ€í•´ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ê´€ë ¨ ì •ë³´:
{% for doc in context_documents %}
â€¢ {{ doc.content[:200] }}{% if doc.content|length > 200 %}...{% endif %}
{% endfor %}

ìš”ì•½ ë‹µë³€ ({{ max_words | default(100) }}ë‹¨ì–´ ì´ë‚´):"""

            # ë¹„êµ ë¶„ì„ RAG í…œí”Œë¦¿
            comparative_rag_template = """ì—¬ëŸ¬ ê´€ì ì—ì„œ {{ question }}ì— ëŒ€í•´ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì°¸ê³  ìë£Œ:
{% for doc in context_documents %}
ê´€ì  {{ loop.index }}:
{{ doc.content }}
{% if doc.metadata.source %}
ì¶œì²˜: {{ doc.metadata.source }}
{% endif %}

{% endfor %}

ë¹„êµ ë¶„ì„:
1. ê³µí†µì :
2. ì°¨ì´ì :
3. ê²°ë¡ :"""

            # í…œí”Œë¦¿ ë“±ë¡
            templates = {
                "rag_basic": basic_rag_template,
                "rag_detailed": detailed_rag_template,
                "rag_summary": summary_rag_template,
                "rag_comparative": comparative_rag_template
            }
            
            for name, template in templates.items():
                self.template_manager.register_template(name, template)
                
        except Exception as e:
            # í…œí”Œë¦¿ ë“±ë¡ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ë¡œê·¸ë§Œ ë‚¨ê¹€
            pass
    
    async def query(
        self,
        question: str,
        model: str,
        k: int = 5,
        template_name: str = "rag_basic",
        provider: Optional[str] = None,
        similarity_threshold: Optional[float] = None,
        filter_metadata: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Result[Dict[str, Any], str]:
        """RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
        
        Args:
            question: ì§ˆë¬¸
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            template_name: ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            provider: LLM Provider ì´ë¦„
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            filter_metadata: ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„° í•„í„°
            **kwargs: LLM ìƒì„± ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            Result[Dict[str, Any], str]: RAG ì‘ë‹µ ê²°ê³¼
        """
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            if similarity_threshold:
                search_result = await self.vector_store.search_with_threshold(
                    query=question,
                    similarity_threshold=similarity_threshold,
                    max_results=k
                )
            else:
                search_result = await self.vector_store.similarity_search(
                    query=question,
                    k=k,
                    filter_metadata=filter_metadata
                )
            
            if search_result.is_failure():
                return search_result
            
            context_documents = search_result.unwrap()
            
            # 2. ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if not context_documents:
                return Success({
                    "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”.",
                    "context_documents": [],
                    "question": question,
                    "context_count": 0
                })
            
            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
            if self.template_manager:
                # í…œí”Œë¦¿ ì‚¬ìš©
                generation_result = await self.template_manager.render_and_generate(
                    template_name=template_name,
                    llm_manager=self.llm_manager,
                    model=model,
                    variables={
                        "question": question,
                        "context_documents": context_documents,
                        **kwargs
                    },
                    provider=provider
                )
            else:
                # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                context_text = "\n---\n".join([doc["content"] for doc in context_documents])
                prompt = f"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸:\n{context_text}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"
                
                generation_result = await self.llm_manager.generate(
                    prompt=prompt,
                    model=model,
                    provider=provider,
                    **kwargs
                )
            
            if generation_result.is_failure():
                return generation_result
            
            answer = generation_result.unwrap()
            
            return Success({
                "answer": answer,
                "context_documents": context_documents,
                "question": question,
                "context_count": len(context_documents),
                "template_used": template_name,
                "model_used": model,
                "provider_used": provider
            })
            
        except Exception as e:
            return Failure(f"RAG ì§ˆì˜ì‘ë‹µ ì‹¤íŒ¨: {str(e)}")
    
    async def add_knowledge(
        self,
        documents: List[Dict[str, Any]]
    ) -> Result[List[str], str]:
        """ì§€ì‹ ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€
        
        Args:
            documents: ì¶”ê°€í•  ë¬¸ì„œ ëª©ë¡
            
        Returns:
            Result[List[str], str]: ì¶”ê°€ëœ ë¬¸ì„œ ID ëª©ë¡ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
        """
        return await self.vector_store.add_documents(documents)
    
    async def add_knowledge_from_texts(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        auto_chunk: bool = True
    ) -> Result[List[str], str]:
        """í…ìŠ¤íŠ¸ë“¤ì„ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€ (ìë™ ì²­í‚¹ ì§€ì›)
        
        Args:
            texts: ì¶”ê°€í•  í…ìŠ¤íŠ¸ ëª©ë¡
            metadatas: ê° í…ìŠ¤íŠ¸ì˜ ë©”íƒ€ë°ì´í„°
            chunk_size: ì²­í‚¹ í¬ê¸° (auto_chunk=Trueì¸ ê²½ìš°)
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨
            auto_chunk: ìë™ ì²­í‚¹ ì—¬ë¶€
            
        Returns:
            Result[List[str], str]: ì¶”ê°€ëœ ë¬¸ì„œ ID ëª©ë¡
        """
        try:
            processed_texts = []
            processed_metadatas = []
            
            for i, text in enumerate(texts):
                base_metadata = metadatas[i] if metadatas and i < len(metadatas) else {}
                
                if auto_chunk and len(text) > chunk_size:
                    # í…ìŠ¤íŠ¸ ì²­í‚¹
                    chunks = self.vector_store.chunk_text(
                        text=text,
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    
                    for j, chunk in enumerate(chunks):
                        processed_texts.append(chunk)
                        chunk_metadata = base_metadata.copy()
                        chunk_metadata.update({
                            "chunk_index": j,
                            "total_chunks": len(chunks),
                            "original_text_index": i,
                            "is_chunked": True
                        })
                        processed_metadatas.append(chunk_metadata)
                else:
                    processed_texts.append(text)
                    processed_metadatas.append(base_metadata)
            
            return await self.vector_store.add_text_documents(
                texts=processed_texts,
                metadatas=processed_metadatas
            )
            
        except Exception as e:
            return Failure(f"í…ìŠ¤íŠ¸ ì§€ì‹ ë² ì´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
    
    async def update_knowledge(
        self,
        documents: List[Dict[str, Any]]
    ) -> Result[List[str], str]:
        """ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ ì—…ë°ì´íŠ¸
        
        Args:
            documents: ì—…ë°ì´íŠ¸í•  ë¬¸ì„œ ëª©ë¡ (ID í¬í•¨ í•„ìš”)
            
        Returns:
            Result[List[str], str]: ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ID ëª©ë¡
        """
        return await self.vector_store.update_documents(documents)
    
    async def remove_knowledge(
        self,
        document_ids: List[str]
    ) -> Result[None, str]:
        """ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œ ì‚­ì œ
        
        Args:
            document_ids: ì‚­ì œí•  ë¬¸ì„œ ID ëª©ë¡
            
        Returns:
            Result[None, str]: ì‚­ì œ ê²°ê³¼
        """
        return await self.vector_store.delete_documents(document_ids)
    
    async def search_knowledge(
        self,
        query: str,
        k: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None,
        similarity_threshold: Optional[float] = None
    ) -> Result[List[Dict[str, Any]], str]:
        """ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ (LLM ì—†ì´)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            filter_metadata: ë©”íƒ€ë°ì´í„° í•„í„°
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            Result[List[Dict[str, Any]], str]: ê²€ìƒ‰ ê²°ê³¼
        """
        if similarity_threshold:
            return await self.vector_store.search_with_threshold(
                query=query,
                similarity_threshold=similarity_threshold,
                max_results=k
            )
        else:
            return await self.vector_store.similarity_search(
                query=query,
                k=k,
                filter_metadata=filter_metadata
            )
    
    async def get_knowledge_stats(self) -> Result[Dict[str, Any], str]:
        """ì§€ì‹ ë² ì´ìŠ¤ í†µê³„ ì •ë³´
        
        Returns:
            Result[Dict[str, Any], str]: í†µê³„ ì •ë³´
        """
        try:
            collection_info = self.vector_store.get_collection_info()
            
            # ChromaDBì˜ ê²½ìš° ìƒì„¸ í†µê³„ ì¡°íšŒ
            if hasattr(self.vector_store, 'get_collection_stats'):
                stats_result = await self.vector_store.get_collection_stats()
                if stats_result.is_success():
                    stats = stats_result.unwrap()
                    collection_info.update(stats)
            
            return Success(collection_info)
            
        except Exception as e:
            return Failure(f"ì§€ì‹ ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    
    def create_qa_pipeline(
        self,
        model: str,
        template_name: str = "rag_basic",
        k: int = 5,
        provider: Optional[str] = None,
        **default_kwargs
    ):
        """ì§ˆì˜ì‘ë‹µ íŒŒì´í”„ë¼ì¸ ìƒì„± (HOF íŒ¨í„´)
        
        Args:
            model: ì‚¬ìš©í•  ëª¨ë¸
            template_name: í…œí”Œë¦¿ ì´ë¦„
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            provider: Provider ì´ë¦„
            **default_kwargs: ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            
        Returns:
            Callable: ì§ˆë¬¸ì„ ë°›ì•„ ë‹µë³€ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        """
        async def qa_pipeline(question: str, **kwargs) -> Result[Dict[str, Any], str]:
            merged_kwargs = {**default_kwargs, **kwargs}
            return await self.query(
                question=question,
                model=model,
                template_name=template_name,
                k=k,
                provider=provider,
                **merged_kwargs
            )
        
        return qa_pipeline
    
    def create_knowledge_ingestion_pipeline(
        self,
        auto_chunk: bool = True,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """ì§€ì‹ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ìƒì„± (HOF íŒ¨í„´)
        
        Args:
            auto_chunk: ìë™ ì²­í‚¹ ì—¬ë¶€
            chunk_size: ì²­í‚¹ í¬ê¸°
            chunk_overlap: ì²­í¬ ê²¹ì¹¨
            
        Returns:
            Callable: í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
        """
        async def ingestion_pipeline(
            texts: List[str],
            metadatas: Optional[List[Dict[str, Any]]] = None
        ) -> Result[List[str], str]:
            return await self.add_knowledge_from_texts(
                texts=texts,
                metadatas=metadatas,
                auto_chunk=auto_chunk,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
        
        return ingestion_pipeline