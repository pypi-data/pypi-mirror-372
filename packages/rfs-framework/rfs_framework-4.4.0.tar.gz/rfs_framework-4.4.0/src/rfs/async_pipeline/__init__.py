"""
RFS Framework Async Pipeline

ë¹„ë™ê¸° í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì„ ìœ„í•œ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ë„êµ¬

ì´ íŒ¨í‚¤ì§€ëŠ” RFS Frameworkì˜ ë¹„ë™ê¸° ì²˜ë¦¬ ëŠ¥ë ¥ì„ ëŒ€í­ ê°•í™”í•˜ì—¬ 
ìš°ì•„í•˜ê³  íƒ€ì… ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
- AsyncResult: ë¹„ë™ê¸° ì „ìš© Result ëª¨ë‚˜ë“œ
- AsyncPipeline: ë™ê¸°/ë¹„ë™ê¸° í•¨ìˆ˜ í˜¼ì¬ íŒŒì´í”„ë¼ì¸
- ê³ ê¸‰ ì—ëŸ¬ ì²˜ë¦¬: ì¬ì‹œë„, í´ë°±, ì„œí‚·ë¸Œë ˆì´ì»¤
- ì„±ëŠ¥ ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬, ë°±í”„ë ˆì…”, ìºì‹±

Example:
    >>> from rfs.async_pipeline import AsyncResult, async_pipe
    >>> 
    >>> # AsyncResult ê¸°ë³¸ ì‚¬ìš©
    >>> result = await (
    ...     AsyncResult.from_async(fetch_user_data)
    ...     .bind_async(validate_user)
    ...     .map_async(format_response)
    ... )
    >>> 
    >>> # í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
    >>> pipeline = async_pipe(
    ...     validate_input,    # ë™ê¸° í•¨ìˆ˜
    ...     fetch_data,        # ë¹„ë™ê¸° í•¨ìˆ˜
    ...     transform_data,    # ë™ê¸° í•¨ìˆ˜
    ...     save_result        # ë¹„ë™ê¸° í•¨ìˆ˜
    ... )
    >>> result = await pipeline.execute(input_data)
"""

# === Core Components ===
from .async_result import (
    AsyncResult,
    async_success,
    async_failure,
    from_awaitable,
    sequence_async_results,
    parallel_map_async
)

from .core import (
    AsyncPipeline,
    AsyncPipelineBuilder,
    async_pipe,
    execute_async_pipeline,
    parallel_pipeline_execution
)

# === Error Handling ===
from .error_handling import (
    AsyncErrorContext,
    AsyncRetryWrapper,
    AsyncFallbackWrapper,
    AsyncCircuitBreaker,
    AsyncErrorStrategy,
    AsyncErrorMonitor,
    ErrorSeverity,
    with_retry,
    with_fallback,
    with_circuit_breaker
)

# === Performance Tools ===
from .performance import (
    AsyncPerformanceMonitor,
    AsyncBackpressureController,
    AsyncStreamProcessor,
    AsyncCache,
    PerformanceMetrics,
    parallel_map,
    async_cached,
    async_rate_limited,
    get_global_cache,
    get_global_monitor
)

# === ë²„ì „ ì •ë³´ ===
__version__ = '1.0.0'
__author__ = 'RFS Framework Team'
__email__ = 'team@rfs-framework.dev'

# === íŒ¨í‚¤ì§€ ë©”íƒ€ë°ì´í„° ===
__title__ = 'RFS Async Pipeline'
__description__ = 'ë¹„ë™ê¸° í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì„ ìœ„í•œ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ë„êµ¬'
__url__ = 'https://github.com/rfs-framework/async-pipeline'
__license__ = 'MIT'

# === Export ëª©ë¡ ===
__all__ = [
    # === Core Components ===
    'AsyncResult',
    'AsyncPipeline', 
    'AsyncPipelineBuilder',
    'async_pipe',
    'execute_async_pipeline',
    'parallel_pipeline_execution',
    
    # === AsyncResult í¸ì˜ í•¨ìˆ˜ ===
    'async_success',
    'async_failure',
    'from_awaitable',
    'sequence_async_results',
    'parallel_map_async',
    
    # === Error Handling ===
    'AsyncErrorContext',
    'AsyncRetryWrapper',
    'AsyncFallbackWrapper', 
    'AsyncCircuitBreaker',
    'AsyncErrorStrategy',
    'AsyncErrorMonitor',
    'ErrorSeverity',
    'with_retry',
    'with_fallback',
    'with_circuit_breaker',
    
    # === Performance Tools ===
    'AsyncPerformanceMonitor',
    'AsyncBackpressureController',
    'AsyncStreamProcessor',
    'AsyncCache',
    'PerformanceMetrics',
    'parallel_map',
    'async_cached',
    'async_rate_limited',
    'get_global_cache',
    'get_global_monitor',
]


# === íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ë©”ì‹œì§€ ===
import logging

logger = logging.getLogger(__name__)

def _show_init_message():
    """íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ë©”ì‹œì§€ í‘œì‹œ"""
    try:
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ë©”ì‹œì§€ í‘œì‹œ
        import os
        if os.getenv('RFS_ENV') == 'development':
            print(f"ğŸš€ RFS Async Pipeline v{__version__} ë¡œë”© ì™„ë£Œ")
            print("   ë¹„ë™ê¸° í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ë„êµ¬ í™œì„±í™”")
    except:
        pass  # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ íŒ¨í‚¤ì§€ ë¡œë”©ì—ëŠ” ì˜í–¥ ì—†ìŒ

# ì´ˆê¸°í™” ë©”ì‹œì§€ í‘œì‹œ
_show_init_message()


# === ì‚¬ìš© ì˜ˆì‹œ ë° í€µ ìŠ¤íƒ€íŠ¸ ê°€ì´ë“œ ===

def get_quick_start_examples():
    """
    í€µ ìŠ¤íƒ€íŠ¸ ì˜ˆì‹œ ì½”ë“œ ë°˜í™˜
    
    Returns:
        dict: ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆì‹œ ì½”ë“œ
    """
    return {
        'basic_async_result': '''
from rfs.async_pipeline import AsyncResult

# ê¸°ë³¸ AsyncResult ì‚¬ìš©
async def example_basic():
    result = await (
        AsyncResult.from_async(lambda: fetch_data("user123"))
        .bind_async(lambda data: AsyncResult.from_async(lambda: validate_data(data)))
        .map_sync(lambda data: {"processed": data, "timestamp": time.time()})
    )
    return await result.unwrap_async()
        ''',
        
        'pipeline_mixed_functions': '''
from rfs.async_pipeline import async_pipe

# ë™ê¸°/ë¹„ë™ê¸° í•¨ìˆ˜ í˜¼ì¬ íŒŒì´í”„ë¼ì¸
def validate_input(data: str) -> Result[str, str]:
    return Success(data) if data else Failure("Empty input")

async def fetch_data(query: str) -> dict:
    # ë¹„ë™ê¸° ë°ì´í„° ì¡°íšŒ
    return {"result": query.upper()}

def format_output(data: dict) -> str:
    return f"Result: {data['result']}"

async def example_pipeline():
    pipeline = async_pipe(validate_input, fetch_data, format_output)
    result = await pipeline.execute("hello world")
    return await result.unwrap_async()  # "Result: HELLO WORLD"
        ''',
        
        'error_handling_retry': '''
from rfs.async_pipeline import with_retry, AsyncResult

# ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
@with_retry(max_attempts=3, base_delay=1.0, backoff_factor=2.0)
async def unreliable_api_call():
    # ê°€ë” ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ” API í˜¸ì¶œ
    import random
    if random.random() < 0.7:
        raise ConnectionError("Network error")
    return {"data": "success"}

async def example_retry():
    retry_wrapper = with_retry(max_attempts=3)
    result = await retry_wrapper(unreliable_api_call)
    return await result.unwrap_async()
        ''',
        
        'performance_parallel_processing': '''
from rfs.async_pipeline import parallel_map

# ë³‘ë ¬ ì²˜ë¦¬
async def process_item(item: int) -> int:
    await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
    return item * 2

async def example_parallel():
    items = list(range(100))
    results = await parallel_map(
        process_item, 
        items, 
        max_concurrency=10
    )
    
    # AsyncResultë“¤ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
    processed = []
    for async_result in results:
        value = await async_result.unwrap_async()
        processed.append(value)
    
    return processed
        ''',
        
        'caching_optimization': '''
from rfs.async_pipeline import async_cached, AsyncCache

# ìºì‹± ìµœì í™”
@async_cached(ttl=3600)  # 1ì‹œê°„ ìºì‹±
async def expensive_computation(x: int) -> int:
    await asyncio.sleep(2)  # ì‹œë®¬ë ˆì´ì…˜
    return x ** 2

async def example_caching():
    result1 = await expensive_computation(10)  # ê³„ì‚°ë¨ (2ì´ˆ ì†Œìš”)
    result2 = await expensive_computation(10)  # ìºì‹œì—ì„œ ë°˜í™˜ (ì¦‰ì‹œ)
    return result1, result2  # (100, 100)
        '''
    }


def print_quick_start_guide():
    """í€µ ìŠ¤íƒ€íŠ¸ ê°€ì´ë“œ ì¶œë ¥"""
    examples = get_quick_start_examples()
    
    print("\n" + "="*60)
    print("ğŸš€ RFS Async Pipeline í€µ ìŠ¤íƒ€íŠ¸ ê°€ì´ë“œ")
    print("="*60)
    
    for category, code in examples.items():
        print(f"\nğŸ“Œ {category.replace('_', ' ').title()}")
        print("-" * 40)
        print(code.strip())
        print()
    
    print("ìì„¸í•œ ë¬¸ì„œ: https://rfs-framework.readthedocs.io/async-pipeline")
    print("="*60)


# === ê³ ê¸‰ ì‚¬ìš© íŒ¨í„´ ===

class AsyncPipelinePatterns:
    """
    ìì£¼ ì‚¬ìš©ë˜ëŠ” ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ íŒ¨í„´ë“¤
    
    ê°œë°œìë“¤ì´ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” ê²€ì¦ëœ íŒ¨í„´ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    @staticmethod
    def data_processing_pipeline(
        validation_func,
        fetch_func, 
        transform_func,
        save_func,
        with_retry_config=None,
        with_fallback=None
    ):
        """
        ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ íŒ¨í„´
        
        Args:
            validation_func: ë°ì´í„° ê²€ì¦ í•¨ìˆ˜
            fetch_func: ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜ 
            transform_func: ë°ì´í„° ë³€í™˜ í•¨ìˆ˜
            save_func: ë°ì´í„° ì €ì¥ í•¨ìˆ˜
            with_retry_config: ì¬ì‹œë„ ì„¤ì •
            with_fallback: í´ë°± í•¨ìˆ˜
        """
        operations = [validation_func, fetch_func, transform_func, save_func]
        
        pipeline = AsyncPipeline(operations)
        
        if with_retry_config:
            # ì¬ì‹œë„ ë˜í•‘ (ì˜ˆì œ - ì‹¤ì œë¡œëŠ” ê° ì—°ì‚°ì— ê°œë³„ ì ìš© í•„ìš”)
            pass
            
        if with_fallback:
            # í´ë°± ë˜í•‘ (ì˜ˆì œ - ì‹¤ì œë¡œëŠ” ê° ì—°ì‚°ì— ê°œë³„ ì ìš© í•„ìš”)
            pass
        
        return pipeline
    
    @staticmethod
    def api_aggregation_pipeline(api_calls: list, aggregator_func):
        """
        API ì§‘ê³„ íŒŒì´í”„ë¼ì¸ íŒ¨í„´
        
        ì—¬ëŸ¬ API í˜¸ì¶œ ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì§‘ê³„í•©ë‹ˆë‹¤.
        """
        async def execute_aggregation(initial_data):
            # ë³‘ë ¬ API í˜¸ì¶œ
            api_results = await parallel_map_async(
                lambda call: call(initial_data),
                api_calls,
                max_concurrency=len(api_calls)
            )
            
            # ì„±ê³µí•œ ê²°ê³¼ë“¤ë§Œ ì¶”ì¶œ
            successful_results = []
            for result in api_results:
                if await result.is_success():
                    successful_results.append(await result.unwrap_async())
            
            # ì§‘ê³„
            aggregated = aggregator_func(successful_results)
            return AsyncResult.from_value(aggregated)
        
        return execute_aggregation
    
    @staticmethod
    def streaming_processor_pipeline(
        stream_source,
        batch_size=100,
        max_concurrency=10,
        error_strategy=None
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ íŒ¨í„´
        
        ëŒ€ìš©ëŸ‰ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        processor = AsyncStreamProcessor(
            processor_func=stream_source,
            batch_size=batch_size,
            max_concurrency=max_concurrency
        )
        
        return processor


# === ê°œë°œì ìœ í‹¸ë¦¬í‹° ===

def validate_async_pipeline_environment():
    """
    AsyncPipeline í™˜ê²½ ê²€ì¦
    
    Returns:
        dict: í™˜ê²½ ê²€ì¦ ê²°ê³¼
    """
    import sys
    import asyncio
    
    validation_results = {
        'python_version': sys.version_info >= (3, 8),
        'asyncio_available': True,
        'typing_support': True,
        'performance_features': True
    }
    
    try:
        # asyncio ì •ì±… í™•ì¸
        loop_policy = asyncio.get_event_loop_policy()
        validation_results['event_loop_policy'] = type(loop_policy).__name__
    except:
        validation_results['asyncio_available'] = False
    
    try:
        # typing ëª¨ë“ˆ ê¸°ëŠ¥ í™•ì¸
        from typing import Generic, TypeVar, Awaitable
        validation_results['typing_support'] = True
    except ImportError:
        validation_results['typing_support'] = False
    
    # ì „ì²´ ê²€ì¦ ê²°ê³¼
    validation_results['all_checks_passed'] = all([
        validation_results['python_version'],
        validation_results['asyncio_available'], 
        validation_results['typing_support']
    ])
    
    return validation_results


def get_async_pipeline_info():
    """
    AsyncPipeline íŒ¨í‚¤ì§€ ì •ë³´ ë°˜í™˜
    
    Returns:
        dict: íŒ¨í‚¤ì§€ ì •ë³´
    """
    return {
        'version': __version__,
        'components': len(__all__),
        'core_features': [
            'AsyncResult ëª¨ë‚˜ë“œ',
            'AsyncPipeline ì‹œìŠ¤í…œ', 
            'ê³ ê¸‰ ì—ëŸ¬ ì²˜ë¦¬',
            'ì„±ëŠ¥ ìµœì í™” ë„êµ¬'
        ],
        'compatibility': {
            'rfs_framework': '>= 4.3.0',
            'python': '>= 3.8',
            'asyncio': 'native'
        },
        'documentation': 'https://rfs-framework.readthedocs.io/async-pipeline',
        'repository': __url__
    }


# === ë””ë²„ê¹… ë° ê°œë°œ ë„êµ¬ ===

async def debug_async_pipeline(pipeline: AsyncPipeline, input_data):
    """
    AsyncPipeline ë””ë²„ê¹… ì‹¤í–‰
    
    íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê³¼ì •ì„ ìƒì„¸íˆ ì¶”ì í•˜ì—¬ ë””ë²„ê¹… ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    print(f"ğŸ” AsyncPipeline ë””ë²„ê¹… ì‹œì‘")
    print(f"   íŒŒì´í”„ë¼ì¸: {pipeline}")
    print(f"   ì…ë ¥ ë°ì´í„°: {input_data}")
    
    result, execution_info = await pipeline.execute_with_context(input_data)
    
    print(f"\nğŸ“Š ì‹¤í–‰ ì •ë³´:")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {execution_info['total_duration']:.3f}ì´ˆ")
    print(f"   ì™„ë£Œëœ ë‹¨ê³„: {execution_info['steps_completed']}/{execution_info['operations_count']}")
    print(f"   ì„±ê³µ ì—¬ë¶€: {execution_info['success']}")
    
    print(f"\nğŸ“‹ ë‹¨ê³„ë³„ ìƒì„¸:")
    for step_info in execution_info['steps_details']:
        status = "âœ…" if step_info['success'] else "âŒ"
        print(f"   {status} ë‹¨ê³„ {step_info['step']}: {step_info['operation']} "
              f"({step_info['duration']:.3f}ì´ˆ)")
        
        if not step_info['success'] and 'error' in step_info:
            print(f"      ì—ëŸ¬: {step_info['error']}")
    
    return result, execution_info


# === ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ===

async def benchmark_async_pipeline_performance():
    """
    AsyncPipeline ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
    
    ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì—¬ ìµœì í™” í¬ì¸íŠ¸ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
    """
    import time
    import asyncio
    
    print("ğŸ AsyncPipeline ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ ì •ì˜
    def sync_operation(x):
        return x * 2
    
    async def async_operation(x):
        await asyncio.sleep(0.001)  # 1ms ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
        return x + 1
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤ë“¤
    scenarios = [
        {
            'name': 'ìˆœìˆ˜ ë™ê¸° íŒŒì´í”„ë¼ì¸',
            'operations': [sync_operation, sync_operation, sync_operation],
            'data_size': 1000
        },
        {
            'name': 'ìˆœìˆ˜ ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸', 
            'operations': [async_operation, async_operation, async_operation],
            'data_size': 100
        },
        {
            'name': 'í˜¼ì¬ íŒŒì´í”„ë¼ì¸',
            'operations': [sync_operation, async_operation, sync_operation],
            'data_size': 500
        }
    ]
    
    results = {}
    
    for scenario in scenarios:
        print(f"\nğŸ“Š {scenario['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        pipeline = AsyncPipeline(scenario['operations'])
        start_time = time.time()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for i in range(scenario['data_size']):
            await pipeline.execute(i)
        
        end_time = time.time()
        total_time = end_time - start_time
        throughput = scenario['data_size'] / total_time
        
        results[scenario['name']] = {
            'total_time': total_time,
            'throughput': throughput,
            'avg_time_per_item': total_time / scenario['data_size']
        }
        
        print(f"   ì´ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {throughput:.1f} items/sec")
        print(f"   í•­ëª©ë‹¹ í‰ê· : {total_time * 1000 / scenario['data_size']:.3f}ms")
    
    return results


# === ëª¨ë“ˆ ì „ì—­ ì„¤ì • ===

# ì „ì—­ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í™œì„±í™”
_PERFORMANCE_MONITORING_ENABLED = True

# ì „ì—­ ì—ëŸ¬ ëª¨ë‹ˆí„°ë§ í™œì„±í™”
_ERROR_MONITORING_ENABLED = True

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
_DEBUG_MODE = False


def set_async_pipeline_config(
    performance_monitoring: bool = True,
    error_monitoring: bool = True,
    debug_mode: bool = False
):
    """
    AsyncPipeline ì „ì—­ ì„¤ì •
    
    Args:
        performance_monitoring: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í™œì„±í™” ì—¬ë¶€
        error_monitoring: ì—ëŸ¬ ëª¨ë‹ˆí„°ë§ í™œì„±í™” ì—¬ë¶€
        debug_mode: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
    """
    global _PERFORMANCE_MONITORING_ENABLED, _ERROR_MONITORING_ENABLED, _DEBUG_MODE
    
    _PERFORMANCE_MONITORING_ENABLED = performance_monitoring
    _ERROR_MONITORING_ENABLED = error_monitoring
    _DEBUG_MODE = debug_mode
    
    if debug_mode:
        logging.getLogger(__name__).setLevel(logging.DEBUG)
        print("ğŸ› AsyncPipeline ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    else:
        logging.getLogger(__name__).setLevel(logging.INFO)