"""
RFS Framework AsyncResult ì „ìš© ë¡œê¹… ìœ í‹¸ë¦¬í‹°

AsyncResult ì²´ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ìë™ìœ¼ë¡œ ë¡œê¹…í•˜ê³  ì¶”ì í•˜ëŠ” ê³ ê¸‰ ë¡œê¹… ì‹œìŠ¤í…œ.
ë¯¼ê°í•œ ì •ë³´ ìë™ ë§ˆìŠ¤í‚¹, êµ¬ì¡°í™”ëœ ë¡œê¹…, ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import time
import traceback
from contextlib import asynccontextmanager
from dataclasses import asdict, dataclass, field
from enum import Enum
from functools import wraps
from typing import Any, Callable, Dict, List, Optional, Set, TypeVar, Union

from ..async_pipeline import AsyncResult
from ..core.result import Result, Success, Failure
from ..hof.core import pipe, curry
from ..hof.collections import compact_map, partition

T = TypeVar('T')
E = TypeVar('E')

# ë¯¼ê°í•œ í‚¤ì›Œë“œ ëª©ë¡ (í™•ì¥ ê°€ëŠ¥)
SENSITIVE_KEYS = {
    'password', 'passwd', 'pwd', 'secret', 'token', 'key', 'auth', 'authorization',
    'credential', 'credentials', 'api_key', 'access_key', 'private_key', 'session',
    'cookie', 'csrf', 'jwt', 'bearer', 'oauth', 'refresh_token', 'client_secret',
    'signature', 'hash', 'salt', 'pin', 'ssn', 'social_security', 'credit_card',
    'card_number', 'cvv', 'cvc', 'account_number', 'routing_number', 'bank_account'
}


class LogLevel(Enum):
    """ë¡œê¹… ë ˆë²¨"""
    TRACE = "TRACE"
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


@dataclass
class AsyncResultLogContext:
    """AsyncResult ë¡œê¹… ì»¨í…ìŠ¤íŠ¸"""
    operation_name: str
    operation_id: str
    start_time: float
    chain_depth: int = 0
    parent_operation: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, float] = field(default_factory=dict)


@dataclass
class AsyncResultLogEntry:
    """AsyncResult ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    timestamp: float
    operation_name: str
    operation_id: str
    level: LogLevel
    message: str
    data: Optional[Any] = None
    error: Optional[str] = None
    duration: Optional[float] = None
    chain_depth: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(self.to_dict(), default=str, ensure_ascii=False)


class AsyncResultLogger:
    """AsyncResult ì²´ì¸ ìë™ ë¡œê¹… í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        logger: logging.Logger,
        enable_sensitive_masking: bool = True,
        enable_performance_tracking: bool = True,
        enable_chain_tracking: bool = True,
        max_data_length: int = 1000,
        sensitive_keys: Optional[Set[str]] = None
    ):
        """
        AsyncResult Logger ì´ˆê¸°í™”
        
        Args:
            logger: Python í‘œì¤€ ë¡œê±°
            enable_sensitive_masking: ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹ í™œì„±í™”
            enable_performance_tracking: ì„±ëŠ¥ ì¶”ì  í™œì„±í™”  
            enable_chain_tracking: ì²´ì¸ ì¶”ì  í™œì„±í™”
            max_data_length: ìµœëŒ€ ë°ì´í„° ê¸¸ì´ (ë¡œê¹…ìš©)
            sensitive_keys: ì¶”ê°€ ë¯¼ê°í•œ í‚¤ì›Œë“œ ëª©ë¡
        """
        self.logger = logger
        self.enable_sensitive_masking = enable_sensitive_masking
        self.enable_performance_tracking = enable_performance_tracking
        self.enable_chain_tracking = enable_chain_tracking
        self.max_data_length = max_data_length
        
        # ë¯¼ê°í•œ í‚¤ì›Œë“œ ì„¤ì •
        self.sensitive_keys = SENSITIVE_KEYS.copy()
        if sensitive_keys:
            self.sensitive_keys.update(sensitive_keys)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self._performance_metrics: Dict[str, List[float]] = {}
        self._operation_contexts: Dict[str, AsyncResultLogContext] = {}
    
    # === í•µì‹¬ ë¡œê¹… ë©”ì„œë“œ ===
    
    def log_chain(
        self,
        operation_name: str,
        log_level: LogLevel = LogLevel.INFO,
        include_performance: bool = None,
        include_chain_info: bool = None,
        custom_metadata: Optional[Dict[str, Any]] = None
    ):
        """
        AsyncResult ì²´ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ìë™ ë¡œê¹…
        
        Args:
            operation_name: ì—°ì‚° ì´ë¦„ (ë¡œê·¸ì— í‘œì‹œë  ì‹ë³„ì)
            log_level: ë¡œê¹… ë ˆë²¨
            include_performance: ì„±ëŠ¥ ì •ë³´ í¬í•¨ ì—¬ë¶€
            include_chain_info: ì²´ì¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            custom_metadata: ì»¤ìŠ¤í…€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            Callable: AsyncResultë¥¼ ë˜í•‘í•˜ëŠ” ë°ì½”ë ˆì´í„° í•¨ìˆ˜
            
        Example:
            >>> logger = AsyncResultLogger(logging.getLogger(__name__))
            >>> result = await (
            ...     logger.log_chain("user_fetch")(
            ...         AsyncResult.from_async(fetch_user)
            ...     )
            ...     .bind_async(lambda user: 
            ...         logger.log_chain("user_validation")(
            ...             validate_user_async(user)
            ...         )
            ...     )
            ... )
        """
        # ì„¤ì • ê¸°ë³¸ê°’ ì ìš©
        include_performance = include_performance if include_performance is not None else self.enable_performance_tracking
        include_chain_info = include_chain_info if include_chain_info is not None else self.enable_chain_tracking
        
        def decorator(async_result: AsyncResult[T, E]) -> AsyncResult[T, E]:
            return self._wrap_async_result(
                async_result,
                operation_name,
                log_level,
                include_performance,
                include_chain_info,
                custom_metadata or {}
            )
        
        return decorator
    
    def _wrap_async_result(
        self,
        async_result: AsyncResult[T, E],
        operation_name: str,
        log_level: LogLevel,
        include_performance: bool,
        include_chain_info: bool,
        custom_metadata: Dict[str, Any]
    ) -> AsyncResult[T, E]:
        """AsyncResult ë˜í•‘ ë° ë¡œê¹… ì¶”ê°€"""
        
        async def logged_execution():
            operation_id = f"{operation_name}_{int(time.time() * 1000000)}"
            start_time = time.time()
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = AsyncResultLogContext(
                operation_name=operation_name,
                operation_id=operation_id,
                start_time=start_time,
                metadata=custom_metadata
            )
            
            if include_chain_info:
                self._operation_contexts[operation_id] = context
            
            try:
                # ì‹œì‘ ë¡œê·¸
                self._log_operation_start(context, log_level)
                
                # ì‹¤ì œ AsyncResult ì‹¤í–‰
                result = await async_result.to_result()
                
                # ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
                end_time = time.time()
                duration = end_time - start_time
                
                # ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¥¸ ë¡œê¹…
                if result.is_success():
                    value = result.unwrap()
                    self._log_operation_success(
                        context, value, duration, log_level, include_performance
                    )
                else:
                    error = result.unwrap_error()
                    self._log_operation_failure(
                        context, error, duration, log_level, include_performance
                    )
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
                if include_performance:
                    self._record_performance_metric(operation_name, duration)
                
                return result
                
            except Exception as unexpected_error:
                end_time = time.time()
                duration = end_time - start_time
                
                self._log_operation_exception(
                    context, unexpected_error, duration, log_level
                )
                
                # ì—ëŸ¬ë¥¼ Failureë¡œ ë˜í•‘í•˜ì—¬ ë°˜í™˜
                return Failure(unexpected_error)
            
            finally:
                # ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
                if include_chain_info and operation_id in self._operation_contexts:
                    del self._operation_contexts[operation_id]
        
        return AsyncResult(logged_execution())
    
    # === ê°œë³„ ë¡œê¹… ë©”ì„œë“œ ===
    
    def _log_operation_start(
        self,
        context: AsyncResultLogContext,
        log_level: LogLevel
    ):
        """ì—°ì‚° ì‹œì‘ ë¡œê·¸"""
        entry = AsyncResultLogEntry(
            timestamp=context.start_time,
            operation_name=context.operation_name,
            operation_id=context.operation_id,
            level=log_level,
            message=f"ğŸš€ {context.operation_name}: ì‹œì‘",
            chain_depth=context.chain_depth,
            metadata=context.metadata
        )
        
        self._emit_log(entry)
    
    def _log_operation_success(
        self,
        context: AsyncResultLogContext,
        value: Any,
        duration: float,
        log_level: LogLevel,
        include_performance: bool
    ):
        """ì—°ì‚° ì„±ê³µ ë¡œê·¸"""
        formatted_value = self._format_value(value)
        
        message_parts = [f"âœ… {context.operation_name}: ì„±ê³µ"]
        if include_performance:
            message_parts.append(f"({duration:.3f}ì´ˆ)")
        
        entry = AsyncResultLogEntry(
            timestamp=time.time(),
            operation_name=context.operation_name,
            operation_id=context.operation_id,
            level=log_level,
            message=" ".join(message_parts),
            data=formatted_value,
            duration=duration if include_performance else None,
            chain_depth=context.chain_depth,
            metadata=context.metadata
        )
        
        self._emit_log(entry)
    
    def _log_operation_failure(
        self,
        context: AsyncResultLogContext,
        error: Any,
        duration: float,
        log_level: LogLevel,
        include_performance: bool
    ):
        """ì—°ì‚° ì‹¤íŒ¨ ë¡œê·¸"""
        error_str = str(error)
        
        message_parts = [f"âŒ {context.operation_name}: ì‹¤íŒ¨ - {error_str}"]
        if include_performance:
            message_parts.append(f"({duration:.3f}ì´ˆ)")
        
        entry = AsyncResultLogEntry(
            timestamp=time.time(),
            operation_name=context.operation_name,
            operation_id=context.operation_id,
            level=LogLevel.ERROR,  # ì‹¤íŒ¨ëŠ” í•­ìƒ ERROR ë ˆë²¨
            message=" ".join(message_parts),
            error=error_str,
            duration=duration if include_performance else None,
            chain_depth=context.chain_depth,
            metadata=context.metadata
        )
        
        self._emit_log(entry)
    
    def _log_operation_exception(
        self,
        context: AsyncResultLogContext,
        exception: Exception,
        duration: float,
        log_level: LogLevel
    ):
        """ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë¡œê·¸"""
        error_details = {
            "type": type(exception).__name__,
            "message": str(exception),
            "traceback": traceback.format_exc()
        }
        
        entry = AsyncResultLogEntry(
            timestamp=time.time(),
            operation_name=context.operation_name,
            operation_id=context.operation_id,
            level=LogLevel.CRITICAL,
            message=f"ğŸ’¥ {context.operation_name}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë°œìƒ",
            error=json.dumps(error_details, ensure_ascii=False),
            duration=duration,
            chain_depth=context.chain_depth,
            metadata=context.metadata
        )
        
        self._emit_log(entry)
    
    # === ë°ì´í„° í¬ë§·íŒ… ë° ë§ˆìŠ¤í‚¹ ===
    
    def _format_value(self, value: Any) -> str:
        """ë¡œê¹…ìš© ê°’ í¬ë§·íŒ… (ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹)"""
        try:
            if value is None:
                return "None"
            
            # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
            if isinstance(value, dict):
                return self._format_dict(value)
            
            # ë¦¬ìŠ¤íŠ¸/íŠœí”Œ ì²˜ë¦¬
            elif isinstance(value, (list, tuple)):
                return self._format_collection(value)
            
            # ë¬¸ìì—´ ì²˜ë¦¬
            elif isinstance(value, str):
                # ê¸¸ì´ ì œí•œ ì ìš©
                if len(value) > self.max_data_length:
                    return f"{value[:self.max_data_length]}... (ê¸¸ì´: {len(value)})"
                return value
            
            # ê¸°íƒ€ íƒ€ì…
            else:
                value_str = str(value)
                if len(value_str) > self.max_data_length:
                    return f"{value_str[:self.max_data_length]}... (íƒ€ì…: {type(value).__name__})"
                return value_str
                
        except Exception as format_error:
            return f"<í¬ë§·íŒ… ì—ëŸ¬: {str(format_error)}>"
    
    def _format_dict(self, data: dict) -> str:
        """ë”•ì…”ë„ˆë¦¬ í¬ë§·íŒ… (ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹)"""
        if not self.enable_sensitive_masking:
            return str(data)
        
        masked = {}
        
        for key, value in data.items():
            key_lower = str(key).lower()
            
            # ë¯¼ê°í•œ í‚¤ ì²´í¬
            if any(sensitive in key_lower for sensitive in self.sensitive_keys):
                masked[key] = "***MASKED***"
            else:
                # ê°’ë„ ì¬ê·€ì ìœ¼ë¡œ í¬ë§·íŒ…
                if isinstance(value, dict):
                    masked[key] = self._format_dict(value)
                elif isinstance(value, (list, tuple)):
                    masked[key] = self._format_collection(value)
                else:
                    value_str = str(value)
                    if len(value_str) > 100:  # ê°œë³„ ê°’ ê¸¸ì´ ì œí•œ
                        masked[key] = f"{value_str[:100]}..."
                    else:
                        masked[key] = value
        
        formatted = str(masked)
        if len(formatted) > self.max_data_length:
            return f"{formatted[:self.max_data_length]}... (ì´ í‚¤ ìˆ˜: {len(data)})"
        
        return formatted
    
    def _format_collection(self, collection) -> str:
        """ì»¬ë ‰ì…˜ í¬ë§·íŒ…"""
        try:
            if len(collection) == 0:
                return str(collection)
            
            # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ì€ ê²½ìš° ì¼ë¶€ë§Œ)
            if len(collection) > 10:
                sample = list(collection)[:10]
                return f"{str(sample)[:-1]}, ... (ì´ {len(collection)}ê°œ í•­ëª©)]"
            
            # ê° í•­ëª© í¬ë§·íŒ…
            formatted_items = []
            for item in collection:
                if isinstance(item, dict):
                    formatted_items.append(self._format_dict(item))
                else:
                    item_str = str(item)
                    if len(item_str) > 50:
                        formatted_items.append(f"{item_str[:50]}...")
                    else:
                        formatted_items.append(item_str)
            
            result = str(formatted_items)
            if len(result) > self.max_data_length:
                return f"{result[:self.max_data_length]}... (ì´ {len(collection)}ê°œ í•­ëª©)"
            
            return result
            
        except Exception:
            return f"<ì»¬ë ‰ì…˜ (ê¸¸ì´: {len(collection)})>"
    
    # === ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê´€ë¦¬ ===
    
    def _record_performance_metric(self, operation_name: str, duration: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if operation_name not in self._performance_metrics:
            self._performance_metrics[operation_name] = []
        
        metrics = self._performance_metrics[operation_name]
        metrics.append(duration)
        
        # ìµœê·¼ 100ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(metrics) > 100:
            self._performance_metrics[operation_name] = metrics[-100:]
    
    def get_performance_summary(self, operation_name: Optional[str] = None) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if operation_name:
            if operation_name not in self._performance_metrics:
                return {"error": f"ì—°ì‚° '{operation_name}'ì— ëŒ€í•œ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤"}
            
            metrics = self._performance_metrics[operation_name]
            return self._calculate_performance_stats(operation_name, metrics)
        
        # ì „ì²´ ì—°ì‚° ìš”ì•½
        summary = {}
        for op_name, metrics in self._performance_metrics.items():
            summary[op_name] = self._calculate_performance_stats(op_name, metrics)
        
        return summary
    
    def _calculate_performance_stats(self, operation_name: str, metrics: List[float]) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ê³„ì‚°"""
        if not metrics:
            return {"count": 0}
        
        sorted_metrics = sorted(metrics)
        count = len(metrics)
        
        return {
            "count": count,
            "min": min(metrics),
            "max": max(metrics),
            "avg": sum(metrics) / count,
            "p50": sorted_metrics[count // 2],
            "p90": sorted_metrics[int(count * 0.9)],
            "p99": sorted_metrics[int(count * 0.99)] if count >= 100 else sorted_metrics[-1],
            "total_time": sum(metrics)
        }
    
    # === ë¡œê·¸ ì¶œë ¥ ===
    
    def _emit_log(self, entry: AsyncResultLogEntry):
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ì¶œë ¥"""
        # Python ë¡œê¹… ë ˆë²¨ ë§¤í•‘
        level_mapping = {
            LogLevel.TRACE: logging.DEBUG,
            LogLevel.DEBUG: logging.DEBUG,
            LogLevel.INFO: logging.INFO,
            LogLevel.WARNING: logging.WARNING,
            LogLevel.ERROR: logging.ERROR,
            LogLevel.CRITICAL: logging.CRITICAL,
        }
        
        python_level = level_mapping.get(entry.level, logging.INFO)
        
        # ì¶”ê°€ ì •ë³´ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±
        extra_info = {
            "operation_id": entry.operation_id,
            "chain_depth": entry.chain_depth,
            "duration": entry.duration,
            "async_result_data": entry.data,
            "async_result_error": entry.error,
            "async_result_metadata": entry.metadata
        }
        
        # ë¡œê·¸ ì¶œë ¥
        self.logger.log(python_level, entry.message, extra=extra_info)


# === ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ ===

_global_async_loggers: Dict[str, AsyncResultLogger] = {}


def get_async_result_logger(
    logger_name: str = "rfs.async_result",
    **logger_kwargs
) -> AsyncResultLogger:
    """
    ê¸€ë¡œë²Œ AsyncResult ë¡œê±° ë°˜í™˜
    
    Args:
        logger_name: ë¡œê±° ì´ë¦„
        **logger_kwargs: AsyncResultLogger ì´ˆê¸°í™” ì¸ìë“¤
        
    Returns:
        AsyncResultLogger: ì„¤ì •ëœ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
    """
    if logger_name not in _global_async_loggers:
        python_logger = logging.getLogger(logger_name)
        _global_async_loggers[logger_name] = AsyncResultLogger(
            python_logger, **logger_kwargs
        )
    
    return _global_async_loggers[logger_name]


def configure_async_result_logging(
    logger_name: str = "rfs.async_result",
    log_level: str = "INFO",
    log_format: Optional[str] = None,
    enable_json_logging: bool = False,
    **logger_kwargs
) -> AsyncResultLogger:
    """
    AsyncResult ë¡œê¹… ì„¤ì •
    
    Args:
        logger_name: ë¡œê±° ì´ë¦„
        log_level: ë¡œê¹… ë ˆë²¨
        log_format: ë¡œê·¸ í¬ë§· (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        enable_json_logging: JSON ë¡œê¹… í™œì„±í™”
        **logger_kwargs: AsyncResultLogger ì¶”ê°€ ì„¤ì •
        
    Returns:
        AsyncResultLogger: ì„¤ì •ëœ ë¡œê±°
    """
    python_logger = logging.getLogger(logger_name)
    python_logger.setLevel(getattr(logging, log_level.upper()))
    
    # í•¸ë“¤ëŸ¬ ì„¤ì •
    if not python_logger.handlers:
        handler = logging.StreamHandler()
        
        if enable_json_logging:
            # JSON í¬ë§·í„° (êµ¬ì¡°í™”ëœ ë¡œê¹…)
            from .formatters import JsonFormatter
            formatter = JsonFormatter()
        else:
            # ê¸°ë³¸ í¬ë§·í„°
            if log_format is None:
                log_format = (
                    "%(asctime)s - %(name)s - %(levelname)s - "
                    "%(message)s [%(operation_id)s]"
                )
            formatter = logging.Formatter(log_format)
        
        handler.setFormatter(formatter)
        python_logger.addHandler(handler)
    
    # AsyncResultLogger ìƒì„± ë° ìºì‹œ
    async_logger = AsyncResultLogger(python_logger, **logger_kwargs)
    _global_async_loggers[logger_name] = async_logger
    
    return async_logger


# === ë°ì½”ë ˆì´í„° ë‹¨ì¶• í•¨ìˆ˜ ===

@curry
def log_async_chain(
    operation_name: str,
    log_level: LogLevel = LogLevel.INFO,
    logger_name: str = "rfs.async_result"
):
    """
    AsyncResult ì²´ì¸ ë¡œê¹… ë°ì½”ë ˆì´í„° (ì»¤ë§ëœ í•¨ìˆ˜)
    
    Args:
        operation_name: ì—°ì‚° ì´ë¦„
        log_level: ë¡œê¹… ë ˆë²¨
        logger_name: ë¡œê±° ì´ë¦„
        
    Returns:
        Callable: ë°ì½”ë ˆì´í„° í•¨ìˆ˜
        
    Example:
        >>> # ì»¤ë§ëœ ì‚¬ìš©
        >>> log_user_fetch = log_async_chain("user_fetch")
        >>> result = await log_user_fetch(
        ...     AsyncResult.from_async(fetch_user)
        ... )
        >>> 
        >>> # ì²´ì´ë‹ ì‚¬ìš©
        >>> result = await (
        ...     AsyncResult.from_async(fetch_user)
        ...     .bind_async(log_async_chain("user_validation")(validate_user))
        ... )
    """
    logger = get_async_result_logger(logger_name)
    return logger.log_chain(operation_name, log_level)


# === HOF íŒ¨í„´ í†µí•© ===

def create_logged_pipeline(*operations, pipeline_name: str = "pipeline"):
    """
    ë¡œê¹…ì´ í†µí•©ëœ íŒŒì´í”„ë¼ì¸ ìƒì„± (HOF íŒ¨í„´)
    
    Args:
        *operations: íŒŒì´í”„ë¼ì¸ ì—°ì‚°ë“¤
        pipeline_name: íŒŒì´í”„ë¼ì¸ ì´ë¦„
        
    Returns:
        Callable: ë¡œê¹…ì´ í¬í•¨ëœ íŒŒì´í”„ë¼ì¸
        
    Example:
        >>> logged_pipeline = create_logged_pipeline(
        ...     validate_input,
        ...     fetch_data,
        ...     transform_data,
        ...     pipeline_name="data_processing"
        ... )
        >>> result = await logged_pipeline(input_data)
    """
    logger = get_async_result_logger()
    
    # ê° ì—°ì‚°ì„ ë¡œê¹…ìœ¼ë¡œ ë˜í•‘
    logged_operations = []
    for i, operation in enumerate(operations):
        operation_name = f"{pipeline_name}_step_{i+1}"
        if hasattr(operation, '__name__'):
            operation_name = f"{pipeline_name}_{operation.__name__}"
        
        logged_op = logger.log_chain(operation_name)(operation)
        logged_operations.append(logged_op)
    
    return pipe(*logged_operations)


@asynccontextmanager
async def async_result_log_context(
    operation_name: str,
    logger_name: str = "rfs.async_result",
    **context_metadata
):
    """
    AsyncResult ë¡œê¹… ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    Args:
        operation_name: ì—°ì‚° ì´ë¦„
        logger_name: ë¡œê±° ì´ë¦„
        **context_metadata: ì»¨í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
        
    Example:
        >>> async with async_result_log_context("batch_processing", user_id="123"):
        ...     result1 = await AsyncResult.from_async(operation1)
        ...     result2 = await AsyncResult.from_async(operation2)
    """
    logger = get_async_result_logger(logger_name)
    operation_id = f"{operation_name}_{int(time.time() * 1000000)}"
    start_time = time.time()
    
    # ì‹œì‘ ë¡œê·¸
    logger.logger.info(
        f"ğŸ {operation_name} ì»¨í…ìŠ¤íŠ¸ ì‹œì‘",
        extra={
            "operation_id": operation_id,
            "context_metadata": context_metadata
        }
    )
    
    try:
        yield operation_id
    except Exception as e:
        # ì»¨í…ìŠ¤íŠ¸ ë‚´ ì—ëŸ¬ ë¡œê¹…
        logger.logger.error(
            f"ğŸ’¥ {operation_name} ì»¨í…ìŠ¤íŠ¸ ì—ëŸ¬: {str(e)}",
            extra={
                "operation_id": operation_id,
                "error": str(e),
                "context_metadata": context_metadata
            }
        )
        raise
    finally:
        # ì¢…ë£Œ ë¡œê·¸
        duration = time.time() - start_time
        logger.logger.info(
            f"ğŸ {operation_name} ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ({duration:.3f}ì´ˆ)",
            extra={
                "operation_id": operation_id,
                "duration": duration,
                "context_metadata": context_metadata
            }
        )


# === ì‚¬ìš© ì˜ˆì‹œ ===

def get_usage_examples():
    """ì‚¬ìš© ì˜ˆì‹œ ë°˜í™˜"""
    return {
        "basic_logging": '''
from rfs.logging.async_logging import get_async_result_logger
from rfs.async_pipeline import AsyncResult

logger = get_async_result_logger()

# ê¸°ë³¸ ì²´ì¸ ë¡œê¹…
result = await (
    logger.log_chain("user_fetch")(
        AsyncResult.from_async(fetch_user)
    )
    .bind_async(lambda user: 
        logger.log_chain("user_validation")(
            validate_user_async(user)
        )
    )
)
        ''',
        
        "curried_logging": '''
from rfs.logging.async_logging import log_async_chain

# ì»¤ë§ëœ ë¡œê¹… í•¨ìˆ˜ ìƒì„±
log_user_fetch = log_async_chain("user_fetch")
log_user_validation = log_async_chain("user_validation")

result = await (
    log_user_fetch(AsyncResult.from_async(fetch_user))
    .bind_async(lambda user: log_user_validation(validate_user_async(user)))
)
        ''',
        
        "pipeline_logging": '''
from rfs.logging.async_logging import create_logged_pipeline

# ë¡œê¹…ì´ í†µí•©ëœ íŒŒì´í”„ë¼ì¸
user_processing_pipeline = create_logged_pipeline(
    validate_input,
    fetch_user_data,
    enrich_user_profile,
    format_response,
    pipeline_name="user_processing"
)

result = await user_processing_pipeline(user_input)
        ''',
        
        "context_logging": '''
from rfs.logging.async_logging import async_result_log_context

async with async_result_log_context("batch_processing", batch_id="batch_123"):
    users = await AsyncResult.from_async(lambda: fetch_users())
    processed = await AsyncResult.from_async(lambda: process_users(users))
    saved = await AsyncResult.from_async(lambda: save_results(processed))
        ''',
        
        "performance_monitoring": '''
from rfs.logging.async_logging import configure_async_result_logging

# ì„±ëŠ¥ ì¶”ì ì´ í™œì„±í™”ëœ ë¡œê±° ì„¤ì •
logger = configure_async_result_logging(
    logger_name="app.performance",
    log_level="INFO",
    enable_performance_tracking=True,
    enable_json_logging=True
)

# ì‚¬ìš© í›„ ì„±ëŠ¥ ìš”ì•½ í™•ì¸
performance_summary = logger.get_performance_summary("user_fetch")
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {performance_summary['avg']:.3f}ì´ˆ")
        '''
    }


# === ëª¨ë“ˆ ì •ë³´ ===

__version__ = "1.0.0"
__author__ = "RFS Framework Team"

def get_module_info():
    """ëª¨ë“ˆ ì •ë³´ ë°˜í™˜"""
    return {
        "name": "RFS AsyncResult Logging",
        "version": __version__,
        "features": [
            "AsyncResult ì²´ì¸ ìë™ ë¡œê¹…",
            "ë¯¼ê°í•œ ì •ë³´ ìë™ ë§ˆìŠ¤í‚¹",
            "ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì ",
            "êµ¬ì¡°í™”ëœ ë¡œê¹… ì§€ì›",
            "HOF íŒ¨í„´ í†µí•©",
            "ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›"
        ],
        "dependencies": {
            "rfs_framework": ">= 4.3.0",
            "python": ">= 3.8"
        }
    }