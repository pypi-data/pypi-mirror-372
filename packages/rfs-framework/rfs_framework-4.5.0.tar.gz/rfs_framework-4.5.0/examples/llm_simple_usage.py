#!/usr/bin/env python3
"""
RFS Framework LLM í†µí•© ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ

ê¸°ë³¸ì ì¸ LLM ì‚¬ìš©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤.
"""

import asyncio
import os
from rfs.core.result import Result, Success, Failure


async def simple_openai_example():
    """OpenAI ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ"""
    print("ğŸ¤– OpenAI ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ")
    
    try:
        from rfs.llm import OpenAIProvider, LLMManager
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # 1. ì§ì ‘ ì œê³µì ì‚¬ìš©
        provider = OpenAIProvider(api_key=api_key)
        
        result = await provider.generate(
            prompt="Pythonì—ì„œ í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì˜ í•µì‹¬ ê°œë… 3ê°€ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            model="gpt-3.5-turbo",
            max_tokens=200
        )
        
        if result.is_success():
            response = result.unwrap()
            print(f"ì‘ë‹µ: {response['response']}")
            print(f"ì‚¬ìš©ëœ í† í°: {response.get('usage', {}).get('total_tokens', 'N/A')}")
        else:
            print(f"ì˜¤ë¥˜: {result.unwrap_error()}")
        
        # 2. LLM Manager ì‚¬ìš©
        print("\nğŸ¯ LLM Managerë¥¼ í†µí•œ ì‚¬ìš©")
        
        manager = LLMManager()
        await manager.register_provider("openai", provider)
        
        result2 = await manager.generate(
            provider="openai",
            prompt="FastAPIì˜ ì¥ì ì„ 3ê°€ì§€ ë‚˜ì—´í•´ì£¼ì„¸ìš”.",
            model="gpt-3.5-turbo",
            max_tokens=150
        )
        
        if result2.is_success():
            response2 = result2.unwrap()
            print(f"Manager ì‘ë‹µ: {response2['response']}")
        
    except ImportError:
        print("âŒ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def simple_template_example():
    """í…œí”Œë¦¿ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ"""
    print("\nğŸ“ í…œí”Œë¦¿ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ")
    
    try:
        from rfs.llm import PromptTemplateManager, LLMManager, OpenAIProvider
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¤ì •
        manager = LLMManager()
        provider = OpenAIProvider(api_key=api_key)
        await manager.register_provider("openai", provider)
        
        template_manager = PromptTemplateManager()
        await template_manager.register_common_templates()
        
        # ì½”ë“œ ë¦¬ë·° í…œí”Œë¦¿ ì‚¬ìš©
        code_sample = '''
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
'''
        
        result = await template_manager.render_and_generate(
            template_name="code_review",
            provider="openai",
            manager=manager,
            variables={
                "code": code_sample,
                "language": "Python",
                "focus_areas": ["ì„±ëŠ¥", "ë©”ëª¨ë¦¬ ì‚¬ìš©"]
            },
            model="gpt-3.5-turbo"
        )
        
        if result.is_success():
            review = result.unwrap()
            print(f"ì½”ë“œ ë¦¬ë·° ê²°ê³¼:\n{review['response']}")
        else:
            print(f"ì˜¤ë¥˜: {result.unwrap_error()}")
            
    except ImportError:
        print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai jinja2")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def simple_chain_example():
    """ì²´ì¸ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ"""
    print("\nğŸ”— ì²´ì¸ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ")
    
    try:
        from rfs.llm import LLMManager, OpenAIProvider
        from rfs.llm.chains.base import SimpleLLMChain
        from rfs.llm.chains.sequential import SequentialChain
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¤ì •
        manager = LLMManager()
        provider = OpenAIProvider(api_key=api_key)
        await manager.register_provider("openai", provider)
        
        # ì²´ì¸ ìƒì„±
        brainstorm_chain = SimpleLLMChain(
            manager=manager,
            provider="openai",
            model="gpt-3.5-turbo",
            name="ì•„ì´ë””ì–´_ë„ì¶œ"
        )
        
        refine_chain = SimpleLLMChain(
            manager=manager,
            provider="openai",
            model="gpt-3.5-turbo",
            name="ì•„ì´ë””ì–´_ì •ì œ"
        )
        
        # ìˆœì°¨ ì²´ì¸ êµ¬ì„±
        sequential_chain = SequentialChain([brainstorm_chain, refine_chain])
        
        # ì‹¤í–‰
        result = await sequential_chain.run({
            "topic": "ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼",
            "prompt": "ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼ì˜ í•µì‹¬ ê¸°ëŠ¥ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”."
        })
        
        if result.is_success():
            output = result.unwrap()
            print("ì²´ì¸ ì‹¤í–‰ ê²°ê³¼:")
            print(f"ìµœì¢… ì‘ë‹µ: {output.get('response', 'N/A')}")
            
            # ì‹¤í–‰ íˆìŠ¤í† ë¦¬ í™•ì¸
            execution_info = output.get('_sequential_execution', {})
            print(f"ì‹¤í–‰ëœ ì²´ì¸ ìˆ˜: {execution_info.get('chain_count', 0)}")
        else:
            print(f"ì˜¤ë¥˜: {result.unwrap_error()}")
            
    except ImportError:
        print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def simple_rag_example():
    """RAG ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ"""
    print("\nğŸ“š RAG ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ")
    
    try:
        from rfs.llm import LLMManager, OpenAIProvider
        from rfs.llm.rag.chroma import ChromaVectorStore
        from rfs.llm.rag.engine import RAGEngine
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¤ì •
        manager = LLMManager()
        provider = OpenAIProvider(api_key=api_key)
        await manager.register_provider("openai", provider)
        
        # ë²¡í„° ìŠ¤í† ì–´ì™€ RAG ì—”ì§„
        vector_store = ChromaVectorStore(
            collection_name="simple_demo",
            persist_directory="./simple_chroma_data"
        )
        
        rag_engine = RAGEngine(
            vector_store=vector_store,
            llm_manager=manager
        )
        
        # ìƒ˜í”Œ ë¬¸ì„œ ì¶”ê°€
        documents = [
            "FastAPIëŠ” í˜„ëŒ€ì ì¸ Python ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë¹ ë¥¸ ì„±ëŠ¥ê³¼ ìë™ ë¬¸ì„œ ìƒì„±ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.",
            "DjangoëŠ” ë°°í„°ë¦¬ í¬í•¨ ì² í•™ì˜ Python ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ORMê³¼ ê´€ë¦¬ì ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            "FlaskëŠ” ë§ˆì´í¬ë¡œ ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìµœì†Œí•œì˜ ê¸°ëŠ¥ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        ]
        
        for i, doc in enumerate(documents):
            await rag_engine.add_to_knowledge_base(
                text=doc,
                doc_id=f"doc_{i}",
                metadata={"source": f"document_{i}"}
            )
        
        print("âœ… ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
        
        # ì§ˆë¬¸í•˜ê¸°
        question = "FastAPIì™€ Djangoì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        
        result = await rag_engine.generate_answer(
            question=question,
            provider="openai",
            model="gpt-3.5-turbo",
            template="basic"
        )
        
        if result.is_success():
            answer_data = result.unwrap()
            print(f"\nì§ˆë¬¸: {question}")
            print(f"ë‹µë³€: {answer_data['answer']}")
            print(f"ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(answer_data.get('sources', []))}")
        else:
            print(f"ì˜¤ë¥˜: {result.unwrap_error()}")
            
    except ImportError:
        print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai chromadb")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def simple_monitoring_example():
    """ëª¨ë‹ˆí„°ë§ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ"""
    print("\nğŸ“Š ëª¨ë‹ˆí„°ë§ ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œ")
    
    try:
        from rfs.llm import LLMManager, OpenAIProvider
        from rfs.llm.monitoring import get_metrics_collector, get_response_cache
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¤ì •
        manager = LLMManager()
        provider = OpenAIProvider(api_key=api_key)
        await manager.register_provider("openai", provider)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        metrics_collector = get_metrics_collector()
        response_cache = get_response_cache()
        
        # ëª‡ ë²ˆì˜ LLM í˜¸ì¶œ
        prompts = [
            "Hello World",
            "Python í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì´ë€?",
            "FastAPI ì‚¬ìš©ë²•"
        ]
        
        print("LLM í˜¸ì¶œ ì¤‘...")
        for prompt in prompts:
            result = await manager.generate(
                provider="openai",
                prompt=prompt,
                model="gpt-3.5-turbo",
                max_tokens=50
            )
            
            if result.is_success():
                print(f"âœ… '{prompt[:20]}...' ì™„ë£Œ")
            else:
                print(f"âŒ '{prompt[:20]}...' ì‹¤íŒ¨")
        
        # ë©”íŠ¸ë¦­ í™•ì¸
        metrics_summary = metrics_collector.get_summary()
        if metrics_summary.is_success():
            summary = metrics_summary.unwrap()
            print(f"\nğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½:")
            print(f"  - ì´ í˜¸ì¶œ ìˆ˜: {summary.total_calls}")
            print(f"  - ì„±ê³µë¥ : {summary.success_rate:.1f}%")
            print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {summary.avg_duration_ms:.1f}ms")
            print(f"  - ì´ í† í° ì‚¬ìš©: {summary.total_tokens}")
            print(f"  - ì´ ë¹„ìš©: ${summary.total_cost:.4f}")
        
        # ìºì‹œ í†µê³„
        cache_stats = await response_cache.get_cache_stats()
        if cache_stats.is_success():
            stats = cache_stats.unwrap()
            print(f"\nğŸ’¾ ìºì‹œ í†µê³„:")
            print(f"  - íˆíŠ¸: {stats['hits']}")
            print(f"  - ë¯¸ìŠ¤: {stats['misses']}")
            print(f"  - íˆíŠ¸ìœ¨: {stats['hit_rate']:.1f}%")
            print(f"  - ìºì‹œëœ í‚¤ ìˆ˜: {stats['total_keys']}")
        
    except ImportError:
        print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def main():
    """ëª¨ë“  ê°„ë‹¨ ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸš€ RFS Framework LLM ê°„ë‹¨ ì‚¬ìš© ì˜ˆì œë“¤\n")
    
    examples = [
        ("OpenAI ê¸°ë³¸ ì‚¬ìš©", simple_openai_example),
        ("í…œí”Œë¦¿ ì‚¬ìš©", simple_template_example),
        ("ì²´ì¸ ì›Œí¬í”Œë¡œìš°", simple_chain_example),
        ("RAG ì‹œìŠ¤í…œ", simple_rag_example),
        ("ëª¨ë‹ˆí„°ë§", simple_monitoring_example),
    ]
    
    for name, example_func in examples:
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ“– {name}")
            print('='*60)
            await example_func()
            
        except Exception as e:
            print(f"âŒ {name} ì˜ˆì œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")
    print("\nğŸ’¡ ë” ìì„¸í•œ ì˜ˆì œëŠ” 'llm_integration_example.py'ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")


if __name__ == "__main__":
    # í™˜ê²½ë³€ìˆ˜ ì•ˆë‚´
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì•ˆë‚´:")
        print("   export OPENAI_API_KEY='your-api-key'")
        print("   export ANTHROPIC_API_KEY='your-api-key'  # ì˜µì…˜")
        print("\nğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:")
        print("   pip install openai anthropic chromadb jinja2")
        print("\n" + "="*60)
    
    asyncio.run(main())