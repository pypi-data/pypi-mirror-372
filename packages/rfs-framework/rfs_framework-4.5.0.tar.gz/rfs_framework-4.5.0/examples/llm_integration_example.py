#!/usr/bin/env python3
"""
RFS Framework LLM í†µí•© ì‚¬ìš© ì˜ˆì œ

LLM í†µí•© ëª¨ë“ˆì˜ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì¢…í•©ì ì¸ ì˜ˆì œì…ë‹ˆë‹¤.
"""

import asyncio
from datetime import datetime
from typing import Dict, Any

from rfs.core.result import Result, Success, Failure
from rfs.core.config import Config, get_config
from rfs.hof.core import pipe, curry
from rfs.hof.collections import compact_map, first


# LLM ê´€ë ¨ import (ì¡°ê±´ë¶€)
try:
    from rfs.llm import (
        LLMManager, 
        OpenAIProvider, 
        AnthropicProvider,
        PromptTemplateManager,
        RAGEngine,
        SequentialChain,
        ParallelChain,
        ConditionalChain,
        get_metrics_collector,
        get_response_cache
    )
    from rfs.llm.rag.chroma import ChromaVectorStore
    HAS_LLM = True
except ImportError:
    print("LLM ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë ¤ë©´ ì¶”ê°€ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("pip install openai anthropic chromadb jinja2")
    HAS_LLM = False


class LLMIntegrationDemo:
    """LLM í†µí•© ë°ëª¨ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.llm_manager = None
        self.template_manager = None
        self.rag_engine = None
        self.metrics_collector = None
        self.response_cache = None
    
    async def setup(self) -> Result[None, str]:
        """ë°ëª¨ í™˜ê²½ ì„¤ì •"""
        if not HAS_LLM:
            return Failure("LLM ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # 1. LLM Manager ì„¤ì •
            self.llm_manager = LLMManager()
            
            # OpenAI ì œê³µì ë“±ë¡ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            openai_api_key = get_config("llm.providers.openai.api_key", None)
            if openai_api_key:
                openai_provider = OpenAIProvider(api_key=openai_api_key)
                await self.llm_manager.register_provider("openai", openai_provider)
                print("âœ… OpenAI ì œê³µì ë“±ë¡ ì™„ë£Œ")
            
            # Anthropic ì œê³µì ë“±ë¡ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            anthropic_api_key = get_config("llm.providers.anthropic.api_key", None)
            if anthropic_api_key:
                anthropic_provider = AnthropicProvider(api_key=anthropic_api_key)
                await self.llm_manager.register_provider("anthropic", anthropic_provider)
                print("âœ… Anthropic ì œê³µì ë“±ë¡ ì™„ë£Œ")
            
            # 2. í…œí”Œë¦¿ ë§¤ë‹ˆì € ì„¤ì •
            self.template_manager = PromptTemplateManager()
            await self.template_manager.register_common_templates()
            print("âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë“±ë¡ ì™„ë£Œ")
            
            # 3. RAG ì—”ì§„ ì„¤ì •
            vector_store = ChromaVectorStore(
                collection_name="demo_collection",
                persist_directory="./chroma_data"
            )
            self.rag_engine = RAGEngine(
                vector_store=vector_store,
                llm_manager=self.llm_manager
            )
            print("âœ… RAG ì—”ì§„ ì„¤ì • ì™„ë£Œ")
            
            # 4. ëª¨ë‹ˆí„°ë§ ì„¤ì •
            self.metrics_collector = get_metrics_collector()
            self.response_cache = get_response_cache()
            print("âœ… ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            
            return Success(None)
            
        except Exception as e:
            return Failure(f"ì„¤ì • ì‹¤íŒ¨: {str(e)}")
    
    async def demo_basic_llm_usage(self) -> Result[Dict[str, Any], str]:
        """ê¸°ë³¸ LLM ì‚¬ìš©ë²• ë°ëª¨"""
        print("\nğŸ”¥ ê¸°ë³¸ LLM ì‚¬ìš©ë²• ë°ëª¨")
        
        try:
            results = {}
            
            # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„±
            prompt = "Pythonì—ì„œ í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì„ 3ê°€ì§€ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
            
            # OpenAI ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
            if await self.llm_manager.has_provider("openai"):
                openai_result = await self.llm_manager.generate(
                    "openai", 
                    prompt,
                    model="gpt-3.5-turbo",
                    max_tokens=200
                )
                
                if openai_result.is_success():
                    results["openai_response"] = openai_result.unwrap()["response"]
                    print(f"OpenAI ì‘ë‹µ: {results['openai_response'][:100]}...")
            
            # Anthropic ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
            if await self.llm_manager.has_provider("anthropic"):
                anthropic_result = await self.llm_manager.generate(
                    "anthropic",
                    prompt,
                    model="claude-3-haiku-20240307",
                    max_tokens=200
                )
                
                if anthropic_result.is_success():
                    results["anthropic_response"] = anthropic_result.unwrap()["response"]
                    print(f"Anthropic ì‘ë‹µ: {results['anthropic_response'][:100]}...")
            
            return Success(results)
            
        except Exception as e:
            return Failure(f"ê¸°ë³¸ LLM ì‚¬ìš© ë°ëª¨ ì‹¤íŒ¨: {str(e)}")
    
    async def demo_template_usage(self) -> Result[Dict[str, Any], str]:
        """í…œí”Œë¦¿ ì‚¬ìš©ë²• ë°ëª¨"""
        print("\nğŸ“ í…œí”Œë¦¿ ì‚¬ìš©ë²• ë°ëª¨")
        
        try:
            results = {}
            
            # ì½”ë“œ ë¦¬ë·° í…œí”Œë¦¿ ì‚¬ìš©
            code_to_review = """
def calculate_fibonacci(n):
    if n <= 1:
        return n
    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)
"""
            
            # í…œí”Œë¦¿ ë Œë”ë§ ë° ìƒì„±
            if await self.llm_manager.has_provider("openai"):
                review_result = await self.template_manager.render_and_generate(
                    template_name="code_review",
                    provider="openai",
                    manager=self.llm_manager,
                    variables={
                        "code": code_to_review,
                        "language": "Python",
                        "focus_areas": ["ì„±ëŠ¥", "ê°€ë…ì„±", "ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"]
                    },
                    model="gpt-3.5-turbo"
                )
                
                if review_result.is_success():
                    results["code_review"] = review_result.unwrap()["response"]
                    print(f"ì½”ë“œ ë¦¬ë·° ê²°ê³¼: {results['code_review'][:150]}...")
            
            # ìš”ì•½ í…œí”Œë¦¿ ì‚¬ìš©
            text_to_summarize = """
            í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì€ ìˆ˜í•™ì  í•¨ìˆ˜ì˜ ê°œë…ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤.
            ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œëŠ” ë¶ˆë³€ì„±, ìˆœìˆ˜ í•¨ìˆ˜, ê³ ì°¨ í•¨ìˆ˜, í•¨ìˆ˜ í•©ì„± ë“±ì´ ìˆìŠµë‹ˆë‹¤.
            Pythonì—ì„œëŠ” map, filter, reduce ê°™ì€ ë‚´ì¥ í•¨ìˆ˜ì™€ í•¨ê»˜ lambdaë¥¼ ì‚¬ìš©í•˜ì—¬
            í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ìŠ¤íƒ€ì¼ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ itertools ëª¨ë“ˆì€
            í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì— ìœ ìš©í•œ ë§ì€ ë„êµ¬ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
            """
            
            if await self.llm_manager.has_provider("openai"):
                summary_result = await self.template_manager.render_and_generate(
                    template_name="summarization",
                    provider="openai",
                    manager=self.llm_manager,
                    variables={
                        "text": text_to_summarize,
                        "length": "ì§§ê²Œ",
                        "style": "ê¸°ìˆ ì "
                    },
                    model="gpt-3.5-turbo"
                )
                
                if summary_result.is_success():
                    results["summary"] = summary_result.unwrap()["response"]
                    print(f"ìš”ì•½ ê²°ê³¼: {results['summary']}")
            
            return Success(results)
            
        except Exception as e:
            return Failure(f"í…œí”Œë¦¿ ì‚¬ìš© ë°ëª¨ ì‹¤íŒ¨: {str(e)}")
    
    async def demo_rag_usage(self) -> Result[Dict[str, Any], str]:
        """RAG ì‚¬ìš©ë²• ë°ëª¨"""
        print("\nğŸ“š RAG (Retrieval Augmented Generation) ì‚¬ìš©ë²• ë°ëª¨")
        
        try:
            results = {}
            
            # ìƒ˜í”Œ ë¬¸ì„œë“¤ì„ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€
            sample_docs = [
                {
                    "content": "RFS FrameworkëŠ” í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° íŒ¨í„´ì„ ì§€ì›í•˜ëŠ” Python í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Result íŒ¨í„´ì„ í†µí•´ ëª…ì‹œì  ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                    "metadata": {"source": "rfs_overview", "type": "documentation"}
                },
                {
                    "content": "Result íŒ¨í„´ì—ì„œ Successì™€ Failureë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ì˜ ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜ˆì™¸ ì²˜ë¦¬ë³´ë‹¤ ì•ˆì „í•œ ë°©ì‹ì…ë‹ˆë‹¤.",
                    "metadata": {"source": "result_pattern", "type": "documentation"}
                },
                {
                    "content": "HOF(Higher-Order Functions) ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ í•©ì„±, ì»¤ë§, íŒŒì´í”„ë¼ì¸ ë“±ì˜ í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                    "metadata": {"source": "hof_library", "type": "documentation"}
                }
            ]
            
            # ë¬¸ì„œë“¤ì„ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€
            for i, doc in enumerate(sample_docs):
                add_result = await self.rag_engine.add_to_knowledge_base(
                    text=doc["content"],
                    doc_id=f"doc_{i}",
                    metadata=doc["metadata"]
                )
                
                if add_result.is_failure():
                    print(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {add_result.unwrap_error()}")
            
            print("âœ… ìƒ˜í”Œ ë¬¸ì„œë“¤ì„ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            
            # RAG ì§ˆì˜ ì‘ë‹µ
            questions = [
                "RFS Frameworkì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "Result íŒ¨í„´ì´ ì˜ˆì™¸ ì²˜ë¦¬ë³´ë‹¤ ì¢‹ì€ ì´ìœ ëŠ”?",
                "HOF ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” ì–´ë–¤ ê¸°ëŠ¥ë“¤ì´ ìˆë‚˜ìš”?"
            ]
            
            if await self.llm_manager.has_provider("openai"):
                for question in questions:
                    rag_result = await self.rag_engine.generate_answer(
                        question=question,
                        provider="openai",
                        model="gpt-3.5-turbo",
                        template="basic"
                    )
                    
                    if rag_result.is_success():
                        answer_data = rag_result.unwrap()
                        results[f"question_{len(results)}"] = {
                            "question": question,
                            "answer": answer_data["answer"],
                            "sources": [doc["metadata"] for doc in answer_data.get("sources", [])]
                        }
                        print(f"\nì§ˆë¬¸: {question}")
                        print(f"ë‹µë³€: {answer_data['answer'][:200]}...")
            
            return Success(results)
            
        except Exception as e:
            return Failure(f"RAG ì‚¬ìš© ë°ëª¨ ì‹¤íŒ¨: {str(e)}")
    
    async def demo_chain_workflows(self) -> Result[Dict[str, Any], str]:
        """ì²´ì¸ ì›Œí¬í”Œë¡œìš° ë°ëª¨"""
        print("\nğŸ”— ì²´ì¸ ì›Œí¬í”Œë¡œìš° ë°ëª¨")
        
        try:
            results = {}
            
            if not await self.llm_manager.has_provider("openai"):
                return Success({"message": "OpenAI ì œê³µìê°€ í•„ìš”í•©ë‹ˆë‹¤"})
            
            # 1. ìˆœì°¨ ì²´ì¸ ì˜ˆì œ
            from rfs.llm.chains.base import SimpleLLMChain
            
            # ì•„ì´ë””ì–´ ìƒì„± ì²´ì¸
            idea_chain = SimpleLLMChain(
                manager=self.llm_manager,
                provider="openai",
                model="gpt-3.5-turbo",
                name="ì•„ì´ë””ì–´_ìƒì„±"
            )
            
            # ì•„ì´ë””ì–´ í‰ê°€ ì²´ì¸
            evaluation_chain = SimpleLLMChain(
                manager=self.llm_manager,
                provider="openai", 
                model="gpt-3.5-turbo",
                name="ì•„ì´ë””ì–´_í‰ê°€"
            )
            
            # ê°œì„  ì œì•ˆ ì²´ì¸
            improvement_chain = SimpleLLMChain(
                manager=self.llm_manager,
                provider="openai",
                model="gpt-3.5-turbo", 
                name="ê°œì„ _ì œì•ˆ"
            )
            
            # ìˆœì°¨ ì²´ì¸ êµ¬ì„±
            sequential_chain = SequentialChain([
                idea_chain,
                evaluation_chain, 
                improvement_chain
            ])
            
            # ìˆœì°¨ ì²´ì¸ ì‹¤í–‰
            sequential_result = await sequential_chain.run({
                "topic": "Pythonìœ¼ë¡œ ì›¹ APIë¥¼ ê°œë°œí•  ë•Œ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•",
                "prompt": "ì£¼ì œì— ëŒ€í•œ 3ê°€ì§€ ì‹¤ìš©ì ì¸ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”."
            })
            
            if sequential_result.is_success():
                results["sequential_chain"] = sequential_result.unwrap()
                print("âœ… ìˆœì°¨ ì²´ì¸ ì‹¤í–‰ ì™„ë£Œ")
            
            # 2. ë³‘ë ¬ ì²´ì¸ ì˜ˆì œ
            pros_chain = SimpleLLMChain(
                manager=self.llm_manager,
                provider="openai",
                model="gpt-3.5-turbo",
                name="ì¥ì _ë¶„ì„"
            )
            
            cons_chain = SimpleLLMChain(
                manager=self.llm_manager,
                provider="openai",
                model="gpt-3.5-turbo",
                name="ë‹¨ì _ë¶„ì„"
            )
            
            # ë³‘ë ¬ ì²´ì¸ êµ¬ì„±
            parallel_chain = ParallelChain([pros_chain, cons_chain])
            
            # ë³‘ë ¬ ì²´ì¸ ì‹¤í–‰
            parallel_result = await parallel_chain.run({
                "topic": "FastAPI vs Django",
                "prompt": "FastAPIì™€ Djangoì˜ íŠ¹ì§•ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
            })
            
            if parallel_result.is_success():
                results["parallel_chain"] = parallel_result.unwrap()
                print("âœ… ë³‘ë ¬ ì²´ì¸ ì‹¤í–‰ ì™„ë£Œ")
            
            return Success(results)
            
        except Exception as e:
            return Failure(f"ì²´ì¸ ì›Œí¬í”Œë¡œìš° ë°ëª¨ ì‹¤íŒ¨: {str(e)}")
    
    async def demo_monitoring_and_caching(self) -> Result[Dict[str, Any], str]:
        """ëª¨ë‹ˆí„°ë§ ë° ìºì‹± ë°ëª¨"""
        print("\nğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ìºì‹± ë°ëª¨")
        
        try:
            results = {}
            
            # 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë°ëª¨
            if await self.llm_manager.has_provider("openai"):
                # ëª‡ ë²ˆì˜ LLM í˜¸ì¶œë¡œ ë©”íŠ¸ë¦­ ìƒì„±
                test_prompts = [
                    "Hello, world!",
                    "Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì´ë€?",
                    "í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì˜ í•µì‹¬ ê°œë… ì„¤ëª…"
                ]
                
                for prompt in test_prompts:
                    await self.llm_manager.generate(
                        "openai",
                        prompt,
                        model="gpt-3.5-turbo",
                        max_tokens=50
                    )
                
                # ë©”íŠ¸ë¦­ ìš”ì•½ ì¡°íšŒ
                metrics_summary = self.metrics_collector.get_summary()
                if metrics_summary.is_success():
                    results["metrics_summary"] = {
                        "total_calls": metrics_summary.unwrap().total_calls,
                        "success_rate": metrics_summary.unwrap().success_rate,
                        "avg_duration_ms": metrics_summary.unwrap().avg_duration_ms
                    }
                    print(f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ: {results['metrics_summary']}")
            
            # 2. ìºì‹œ ë°ëª¨
            cache_stats_before = await self.response_cache.get_cache_stats()
            
            # ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë‘ ë²ˆ í˜¸ì¶œ (ì²« ë²ˆì§¸ëŠ” ë¯¸ìŠ¤, ë‘ ë²ˆì§¸ëŠ” íˆíŠ¸)
            test_prompt = "ìºì‹œ í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤."
            
            if await self.llm_manager.has_provider("openai"):
                # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
                first_call = await self.llm_manager.generate(
                    "openai",
                    test_prompt,
                    model="gpt-3.5-turbo",
                    max_tokens=30
                )
                
                # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸ - ì‹¤ì œë¡œëŠ” êµ¬í˜„ì— ë”°ë¼ ë‹¤ë¦„)
                second_call = await self.llm_manager.generate(
                    "openai", 
                    test_prompt,
                    model="gpt-3.5-turbo",
                    max_tokens=30
                )
            
            cache_stats_after = await self.response_cache.get_cache_stats()
            
            if cache_stats_after.is_success():
                results["cache_stats"] = cache_stats_after.unwrap()
                print(f"âœ… ìºì‹œ í†µê³„: {results['cache_stats']}")
            
            return Success(results)
            
        except Exception as e:
            return Failure(f"ëª¨ë‹ˆí„°ë§ ë° ìºì‹± ë°ëª¨ ì‹¤íŒ¨: {str(e)}")
    
    async def run_all_demos(self) -> Result[Dict[str, Any], str]:
        """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
        print("ğŸš€ RFS Framework LLM í†µí•© ë°ëª¨ ì‹œì‘\n")
        
        # ì„¤ì •
        setup_result = await self.setup()
        if setup_result.is_failure():
            return setup_result
        
        all_results = {}
        
        # ê° ë°ëª¨ ì‹¤í–‰
        demos = [
            ("basic_llm", self.demo_basic_llm_usage),
            ("templates", self.demo_template_usage), 
            ("rag", self.demo_rag_usage),
            ("chains", self.demo_chain_workflows),
            ("monitoring", self.demo_monitoring_and_caching)
        ]
        
        for demo_name, demo_func in demos:
            try:
                print(f"\n{'='*50}")
                result = await demo_func()
                
                if result.is_success():
                    all_results[demo_name] = result.unwrap()
                else:
                    all_results[demo_name] = {"error": result.unwrap_error()}
                    print(f"âŒ {demo_name} ë°ëª¨ ì‹¤íŒ¨: {result.unwrap_error()}")
                    
            except Exception as e:
                all_results[demo_name] = {"error": str(e)}
                print(f"âŒ {demo_name} ë°ëª¨ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        
        print(f"\n{'='*50}")
        print("ğŸ‰ ëª¨ë“  ë°ëª¨ ì™„ë£Œ!")
        
        return Success(all_results)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    demo = LLMIntegrationDemo()
    result = await demo.run_all_demos()
    
    if result.is_success():
        print("\nâœ… ë°ëª¨ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
        import json
        with open("llm_demo_results.json", "w", encoding="utf-8") as f:
            json.dump(result.unwrap(), f, ensure_ascii=False, indent=2, default=str)
        print("ğŸ“ ê²°ê³¼ê°€ 'llm_demo_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {result.unwrap_error()}")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    asyncio.run(main())