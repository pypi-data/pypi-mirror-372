"""
Unit tests for panel judge scorers.
"""

import json
from unittest.mock import AsyncMock, patch

import pytest

from novaeval.models.base import BaseModel as LLMModel
from novaeval.scorers.panel_judge import (
    AggregationMethod,
    JudgeConfig,
    PanelOfJudgesScorer,
    PanelResult,
    SpecializedPanelScorer,
)


@pytest.mark.unit
class MockLLMModel(LLMModel):
    """Mock LLM model for testing."""

    def __init__(self, mock_responses=None, name="MockModel"):
        super().__init__(
            name=name, model_name=name, api_key="mock_key", base_url="mock_url"
        )
        self.mock_responses = mock_responses or {}
        self.call_count = 0
        self.temperature = 0.0

    def generate(self, prompt, **kwargs):
        """Mock generate method."""
        self.call_count += 1
        if isinstance(self.mock_responses, dict):
            # Find matching prompt patterns for more sophisticated mocking
            for pattern, response in self.mock_responses.items():
                if pattern.lower() in prompt.lower():
                    return response
            return self._get_default_response()
        elif isinstance(self.mock_responses, list):
            if self.call_count <= len(self.mock_responses):
                return self.mock_responses[self.call_count - 1]
            return self._get_default_response()
        else:
            return (
                str(self.mock_responses)
                if self.mock_responses
                else self._get_default_response()
            )

    def generate_batch(self, prompts, **kwargs):
        """Mock batch generate method."""
        return [self.generate(prompt, **kwargs) for prompt in prompts]

    def get_provider(self) -> str:
        """Mock provider method."""
        return "mock_provider"

    async def generate_async(self, prompt, **kwargs):
        """Async wrapper for generate method."""
        return self.generate(prompt, **kwargs)

    def _get_default_response(self):
        """Get default mock response."""
        return json.dumps(
            {
                "score": 4,
                "reasoning": "Good response with minor issues",
                "strengths": "Clear and relevant",
                "weaknesses": "Could be more detailed",
                "confidence": 4,
            }
        )


@pytest.mark.unit
class TestAggregationMethod:
    """Test cases for AggregationMethod enum."""

    def test_enum_values(self):
        """Test that all aggregation methods are defined."""
        assert AggregationMethod.MEAN == "mean"
        assert AggregationMethod.MEDIAN == "median"
        assert AggregationMethod.WEIGHTED_MEAN == "weighted_mean"
        assert AggregationMethod.MAJORITY_VOTE == "majority_vote"
        assert AggregationMethod.CONSENSUS == "consensus"
        assert AggregationMethod.MIN == "min"
        assert AggregationMethod.MAX == "max"


@pytest.mark.unit
class TestJudgeConfig:
    """Test cases for JudgeConfig class."""

    def test_init_default(self):
        """Test initialization with default parameters."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)

        assert judge.model == model
        assert judge.weight == 1.0
        assert judge.name is None
        assert judge.specialty is None
        assert judge.temperature == 0.0

    def test_init_with_params(self):
        """Test initialization with custom parameters."""
        model = MockLLMModel()
        judge = JudgeConfig(
            model=model,
            weight=2.5,
            name="ExpertJudge",
            specialty="accuracy",
            temperature=0.3,
        )

        assert judge.model == model
        assert judge.weight == 2.5
        assert judge.name == "ExpertJudge"
        assert judge.specialty == "accuracy"
        assert judge.temperature == 0.3

    def test_weight_validation_positive(self):
        """Test weight validation with positive value."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model, weight=5.0)
        assert judge.weight == 5.0

    def test_weight_validation_zero(self):
        """Test weight validation with zero value."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model, weight=0.0)
        assert judge.weight == 0.0

    def test_weight_validation_negative(self):
        """Test weight validation with negative value."""
        model = MockLLMModel()
        with pytest.raises(ValueError, match="Judge weight must be non-negative"):
            JudgeConfig(model=model, weight=-1.0)


@pytest.mark.unit
class TestPanelResult:
    """Test cases for PanelResult class."""

    def test_init_default(self):
        """Test initialization with default parameters."""
        result = PanelResult(
            individual_scores=[0.8, 0.9],
            individual_reasonings=["Good", "Excellent"],
            judge_names=["Judge1", "Judge2"],
            aggregated_score=0.85,
            aggregation_method=AggregationMethod.MEAN,
            consensus_level=0.7,
        )

        assert result.individual_scores == [0.8, 0.9]
        assert result.individual_reasonings == ["Good", "Excellent"]
        assert result.judge_names == ["Judge1", "Judge2"]
        assert result.aggregated_score == 0.85
        assert result.aggregation_method == AggregationMethod.MEAN
        assert result.consensus_level == 0.7
        assert result.metadata == {}

    def test_init_with_metadata(self):
        """Test initialization with custom metadata."""
        metadata = {"test_key": "test_value"}
        result = PanelResult(
            individual_scores=[0.8],
            individual_reasonings=["Good"],
            judge_names=["Judge1"],
            aggregated_score=0.8,
            aggregation_method=AggregationMethod.MEAN,
            consensus_level=1.0,
            metadata=metadata,
        )

        assert result.metadata == metadata


@pytest.mark.unit
class TestPanelOfJudgesScorer:
    """Test cases for PanelOfJudgesScorer class."""

    def test_init_default(self):
        """Test initialization with default parameters."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        assert scorer.name == "panel_judge"
        assert scorer.threshold == 0.7
        assert scorer.judges == [judge]
        assert scorer.aggregation_method == AggregationMethod.WEIGHTED_MEAN
        assert scorer.require_consensus is False
        assert scorer.consensus_threshold == 0.8
        assert scorer.evaluation_criteria == "overall quality and correctness"

    def test_init_with_params(self):
        """Test initialization with custom parameters."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model, weight=2.0)
        scorer = PanelOfJudgesScorer(
            judges=[judge],
            aggregation_method=AggregationMethod.MEDIAN,
            threshold=0.8,
            require_consensus=True,
            consensus_threshold=0.9,
            evaluation_criteria="accuracy and completeness",
            name="custom_panel",
        )

        assert scorer.name == "custom_panel"
        assert scorer.threshold == 0.8
        assert scorer.aggregation_method == AggregationMethod.MEDIAN
        assert scorer.require_consensus is True
        assert scorer.consensus_threshold == 0.9
        assert scorer.evaluation_criteria == "accuracy and completeness"

    def test_init_no_judges(self):
        """Test initialization with no judges."""
        with pytest.raises(ValueError, match="At least one judge must be provided"):
            PanelOfJudgesScorer(judges=[])

    def test_init_weight_normalization(self):
        """Test weight normalization for weighted mean aggregation."""
        model1 = MockLLMModel()
        model2 = MockLLMModel()
        judge1 = JudgeConfig(model=model1, weight=1.0)
        judge2 = JudgeConfig(model=model2, weight=2.0)

        scorer = PanelOfJudgesScorer(
            judges=[judge1, judge2],
            aggregation_method=AggregationMethod.WEIGHTED_MEAN,
        )

        # Weights should be normalized (1.0 + 2.0 = 3.0, so 1/3 and 2/3)
        assert abs(scorer.judges[0].weight - 1 / 3) < 1e-10
        assert abs(scorer.judges[1].weight - 2 / 3) < 1e-10

    def test_build_evaluation_prompt_basic(self):
        """Test building evaluation prompt with basic inputs."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        prompt = scorer._build_evaluation_prompt(
            input_text="What is AI?",
            output_text="AI is artificial intelligence",
        )

        assert "What is AI?" in prompt
        assert "AI is artificial intelligence" in prompt
        assert "overall quality and correctness" in prompt
        assert "JSON format" in prompt
        assert "score" in prompt

    def test_build_evaluation_prompt_with_expected_output(self):
        """Test building evaluation prompt with expected output."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        prompt = scorer._build_evaluation_prompt(
            input_text="What is AI?",
            output_text="AI is artificial intelligence",
            expected_output="Artificial Intelligence is a field of computer science",
        )

        assert "Expected/Reference Answer" in prompt
        assert "Artificial Intelligence is a field of computer science" in prompt

    def test_build_evaluation_prompt_with_context(self):
        """Test building evaluation prompt with context."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        prompt = scorer._build_evaluation_prompt(
            input_text="What is AI?",
            output_text="AI is artificial intelligence",
            context="Focus on technical accuracy",
        )

        assert "Additional Context" in prompt
        assert "Focus on technical accuracy" in prompt

    def test_calculate_consensus_single_judge(self):
        """Test consensus calculation with single judge."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        consensus = scorer._calculate_consensus([0.8])
        assert consensus == 1.0

    def test_calculate_consensus_multiple_judges_high_agreement(self):
        """Test consensus calculation with high agreement."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        consensus = scorer._calculate_consensus([0.8, 0.82, 0.79])
        assert consensus > 0.9  # High consensus for similar scores

    def test_calculate_consensus_multiple_judges_low_agreement(self):
        """Test consensus calculation with low agreement."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        consensus = scorer._calculate_consensus([0.2, 0.8, 0.9])
        assert consensus < 0.5  # Low consensus for divergent scores

    def test_aggregate_scores_mean(self):
        """Test score aggregation using mean method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.MEAN
        )

        scores = [0.6, 0.8, 1.0]
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, AggregationMethod.MEAN)
        assert abs(result - 0.8) < 1e-10

    def test_aggregate_scores_median(self):
        """Test score aggregation using median method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.MEDIAN
        )

        scores = [0.6, 0.8, 1.0]
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, AggregationMethod.MEDIAN)
        assert result == 0.8

    def test_aggregate_scores_weighted_mean(self):
        """Test score aggregation using weighted mean method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.WEIGHTED_MEAN
        )

        scores = [0.6, 0.8]
        weights = [0.3, 0.7]
        result = scorer._aggregate_scores(
            scores, weights, AggregationMethod.WEIGHTED_MEAN
        )
        expected = 0.6 * 0.3 + 0.8 * 0.7
        assert abs(result - expected) < 1e-10

    def test_aggregate_scores_weighted_mean_mismatch(self):
        """Test weighted mean with mismatched scores and weights."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.WEIGHTED_MEAN
        )

        scores = [0.6, 0.8]
        weights = [0.3]  # Mismatched
        result = scorer._aggregate_scores(
            scores, weights, AggregationMethod.WEIGHTED_MEAN
        )
        assert abs(result - 0.7) < 1e-10  # Should fallback to mean

    def test_aggregate_scores_majority_vote_pass(self):
        """Test majority vote aggregation with passing scores."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.MAJORITY_VOTE
        )

        scores = [0.8, 0.9, 0.6]  # 2 above threshold (0.7), 1 below
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(
            scores, weights, AggregationMethod.MAJORITY_VOTE
        )
        assert result == 1.0

    def test_aggregate_scores_majority_vote_fail(self):
        """Test majority vote aggregation with failing scores."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.MAJORITY_VOTE
        )

        scores = [0.6, 0.5, 0.8]  # 1 above threshold (0.7), 2 below
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(
            scores, weights, AggregationMethod.MAJORITY_VOTE
        )
        assert result == 0.0

    def test_aggregate_scores_consensus_all_pass(self):
        """Test consensus aggregation with all passing scores."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.CONSENSUS
        )

        scores = [0.8, 0.9, 0.85]  # All above threshold (0.7)
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, AggregationMethod.CONSENSUS)
        assert result == 0.8  # Should return minimum score

    def test_aggregate_scores_consensus_some_fail(self):
        """Test consensus aggregation with some failing scores."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.CONSENSUS
        )

        scores = [0.8, 0.6, 0.9]  # 1 below threshold (0.7)
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, AggregationMethod.CONSENSUS)
        assert result == 0.0

    def test_aggregate_scores_min(self):
        """Test score aggregation using min method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.MIN
        )

        scores = [0.6, 0.8, 1.0]
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, AggregationMethod.MIN)
        assert result == 0.6

    def test_aggregate_scores_max(self):
        """Test score aggregation using max method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(
            judges=[judge], aggregation_method=AggregationMethod.MAX
        )

        scores = [0.6, 0.8, 1.0]
        weights = [1.0, 1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, AggregationMethod.MAX)
        assert result == 1.0

    def test_aggregate_scores_unknown_method(self):
        """Test score aggregation with unknown method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        scores = [0.6, 0.8]
        weights = [1.0, 1.0]
        result = scorer._aggregate_scores(scores, weights, "unknown_method")
        assert abs(result - 0.7) < 1e-10  # Should fallback to mean

    def test_generate_panel_reasoning(self):
        """Test panel reasoning generation."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        panel_result = PanelResult(
            individual_scores=[0.8, 0.9],
            individual_reasonings=["Good response", "Excellent work"],
            judge_names=["Judge1", "Judge2"],
            aggregated_score=0.85,
            aggregation_method=AggregationMethod.MEAN,
            consensus_level=0.8,
        )

        reasoning = scorer._generate_panel_reasoning(panel_result)

        assert "Panel of 2 LLM Judges Evaluation" in reasoning
        assert "Aggregation Method: mean" in reasoning
        assert "Final Score: 0.850" in reasoning
        assert "Consensus Level: 0.800" in reasoning
        assert "Judge1: 0.800" in reasoning
        assert "Judge2: 0.900" in reasoning
        assert "Mean Score: 0.850" in reasoning
        assert "High consensus among judges" in reasoning

    @pytest.mark.asyncio
    async def test_evaluate_with_judge_success(self):
        """Test successful evaluation with a single judge."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the async generate method
        with patch.object(model, "generate", new_callable=AsyncMock) as mock_generate:
            mock_generate.return_value = model._get_default_response()

            prompt = "Evaluate this response"
            result = await scorer._evaluate_with_judge(judge, prompt)

        assert "score" in result
        assert "reasoning" in result
        assert "raw_score" in result
        assert "strengths" in result
        assert "weaknesses" in result
        assert "confidence" in result
        assert 0.0 <= result["score"] <= 1.0
        assert result["raw_score"] == 4

    @pytest.mark.asyncio
    async def test_evaluate_with_judge_no_json(self):
        """Test evaluation with judge response containing no JSON."""
        model = MockLLMModel("This is not JSON")
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the async generate method
        with patch.object(model, "generate", new_callable=AsyncMock) as mock_generate:
            mock_generate.return_value = "This is not JSON"

            prompt = "Evaluate this response"
            with pytest.raises(Exception, match="No JSON found in judge response"):
                await scorer._evaluate_with_judge(judge, prompt)

    @pytest.mark.asyncio
    async def test_evaluate_with_judge_missing_fields(self):
        """Test evaluation with judge response missing required fields."""
        model = MockLLMModel('{"score": 4}')  # Missing reasoning
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the async generate method
        with patch.object(model, "generate", new_callable=AsyncMock) as mock_generate:
            mock_generate.return_value = '{"score": 4}'

            prompt = "Evaluate this response"
            with pytest.raises(
                Exception, match="Judge response missing required fields"
            ):
                await scorer._evaluate_with_judge(judge, prompt)

    @pytest.mark.asyncio
    async def test_evaluate_with_judge_invalid_score(self):
        """Test evaluation with judge response containing invalid score."""
        model = MockLLMModel('{"score": 6, "reasoning": "Good"}')  # Score > 5
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the async generate method
        with patch.object(model, "generate", new_callable=AsyncMock) as mock_generate:
            mock_generate.return_value = '{"score": 6, "reasoning": "Good"}'

            prompt = "Evaluate this response"
            with pytest.raises(Exception, match="Score must be between 1 and 5"):
                await scorer._evaluate_with_judge(judge, prompt)

    @pytest.mark.asyncio
    async def test_evaluate_success(self):
        """Test successful panel evaluation."""
        # Mock responses for judges
        mock_responses = [
            json.dumps(
                {
                    "score": 4,
                    "reasoning": "Good response",
                    "strengths": "Clear",
                    "weaknesses": "Could be more detailed",
                    "confidence": 4,
                }
            ),
            json.dumps(
                {
                    "score": 5,
                    "reasoning": "Excellent response",
                    "strengths": "Comprehensive",
                    "weaknesses": "None",
                    "confidence": 5,
                }
            ),
        ]

        model1 = MockLLMModel()
        model2 = MockLLMModel()
        judge1 = JudgeConfig(model=model1, name="Judge1")
        judge2 = JudgeConfig(model=model2, name="Judge2")

        scorer = PanelOfJudgesScorer(judges=[judge1, judge2])

        # Mock the async generate methods
        with (
            patch.object(model1, "generate", new_callable=AsyncMock) as mock_generate1,
            patch.object(model2, "generate", new_callable=AsyncMock) as mock_generate2,
        ):
            mock_generate1.return_value = mock_responses[0]
            mock_generate2.return_value = mock_responses[1]

            result = await scorer.evaluate(
                input_text="What is AI?",
                output_text="AI is artificial intelligence",
            )

        assert result.score > 0.0
        assert result.passed is True
        assert "Panel of 2 LLM Judges Evaluation" in result.reasoning
        assert "Judge1" in result.reasoning
        assert "Judge2" in result.reasoning

    @pytest.mark.asyncio
    async def test_evaluate_all_judges_fail(self):
        """Test evaluation when all judges fail."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the judge to raise an exception
        with patch.object(
            model,
            "generate",
            new_callable=AsyncMock,
            side_effect=Exception("Judge failed"),
        ):
            result = await scorer.evaluate(
                input_text="What is AI?",
                output_text="AI is artificial intelligence",
            )

        assert result.score == 0.0
        assert result.passed is False
        assert "All judges failed to evaluate" in result.reasoning

    @pytest.mark.asyncio
    async def test_evaluate_consensus_requirement_fail(self):
        """Test evaluation with consensus requirement that fails."""
        mock_responses = [
            json.dumps(
                {
                    "score": 2,
                    "reasoning": "Poor",
                    "strengths": "",
                    "weaknesses": "Many",
                    "confidence": 2,
                }
            ),
            json.dumps(
                {
                    "score": 5,
                    "reasoning": "Excellent",
                    "strengths": "Great",
                    "weaknesses": "",
                    "confidence": 5,
                }
            ),
        ]

        model1 = MockLLMModel()
        model2 = MockLLMModel()
        judge1 = JudgeConfig(model=model1, name="Judge1")
        judge2 = JudgeConfig(model=model2, name="Judge2")

        scorer = PanelOfJudgesScorer(
            judges=[judge1, judge2],
            require_consensus=True,
            consensus_threshold=0.8,
        )

        # Mock the async generate methods
        with (
            patch.object(model1, "generate", new_callable=AsyncMock) as mock_generate1,
            patch.object(model2, "generate", new_callable=AsyncMock) as mock_generate2,
        ):
            mock_generate1.return_value = mock_responses[0]
            mock_generate2.return_value = mock_responses[1]

            result = await scorer.evaluate(
                input_text="What is AI?",
                output_text="AI is artificial intelligence",
            )

        assert result.score == 0.0
        assert result.passed is False
        assert "Insufficient consensus" in result.reasoning

    @pytest.mark.asyncio
    async def test_evaluate_exception_handling(self):
        """Test evaluation exception handling."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock _build_evaluation_prompt to raise an exception
        with patch.object(
            scorer,
            "_build_evaluation_prompt",
            side_effect=Exception("Prompt building failed"),
        ):
            result = await scorer.evaluate(
                input_text="What is AI?",
                output_text="AI is artificial intelligence",
            )

        assert result.score == 0.0
        assert result.passed is False
        assert "Panel evaluation failed" in result.reasoning

    def test_score_synchronous_wrapper(self):
        """Test synchronous score method."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the async evaluate method
        with patch.object(scorer, "evaluate", new_callable=AsyncMock) as mock_evaluate:
            mock_evaluate.return_value.score = 0.85
            mock_evaluate.return_value.passed = True

            result = scorer.score("AI response", "Expected response")

            assert result == 0.85
            mock_evaluate.assert_called_once()

    def test_score_with_context(self):
        """Test synchronous score method with context."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        context = {"input": "What is AI?", "context": "Technical discussion"}

        with patch.object(scorer, "evaluate", new_callable=AsyncMock) as mock_evaluate:
            mock_evaluate.return_value.score = 0.85
            scorer.score("AI response", "Expected response", context)

            # Check that context was properly extracted
            call_args = mock_evaluate.call_args
            assert call_args[1]["input_text"] == "What is AI?"
            assert call_args[1]["context"] == "Technical discussion"

    def test_score_exception_handling(self):
        """Test synchronous score method exception handling."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        with patch.object(
            scorer, "evaluate", side_effect=Exception("Evaluation failed")
        ):
            result = scorer.score("AI response", "Expected response")
            assert result == 0.0


@pytest.mark.unit
class TestSpecializedPanelScorer:
    """Test cases for SpecializedPanelScorer class."""

    def test_create_diverse_panel(self):
        """Test creating a diverse panel."""
        model1 = MockLLMModel(name="Model1")
        model2 = MockLLMModel(name="Model2")
        model3 = MockLLMModel(name="Model3")

        scorer = SpecializedPanelScorer.create_diverse_panel(
            models=[model1, model2, model3],
            evaluation_criteria="technical accuracy",
        )

        assert len(scorer.judges) == 3
        assert scorer.evaluation_criteria == "technical accuracy"
        assert scorer.judges[0].specialty == "accuracy"
        assert scorer.judges[1].specialty == "clarity"
        assert scorer.judges[2].specialty == "completeness"
        assert "MockLLMModel_accuracy" in scorer.judges[0].name
        assert "MockLLMModel_clarity" in scorer.judges[1].name
        assert "MockLLMModel_completeness" in scorer.judges[2].name

    def test_create_consensus_panel(self):
        """Test creating a consensus panel."""
        model1 = MockLLMModel(name="Model1")
        model2 = MockLLMModel(name="Model2")

        scorer = SpecializedPanelScorer.create_consensus_panel(
            models=[model1, model2],
            consensus_threshold=0.9,
        )

        assert len(scorer.judges) == 2
        assert scorer.aggregation_method == AggregationMethod.CONSENSUS
        assert scorer.require_consensus is True
        assert scorer.consensus_threshold == 0.9
        assert "ConsensusJudge_1" in scorer.judges[0].name
        assert "ConsensusJudge_2" in scorer.judges[1].name

    def test_create_weighted_expert_panel(self):
        """Test creating a weighted expert panel."""
        model1 = MockLLMModel(name="Model1")
        model2 = MockLLMModel(name="Model2")

        expert_models = [(model1, 2.0), (model2, 1.0)]
        scorer = SpecializedPanelScorer.create_weighted_expert_panel(
            expert_models=expert_models,
            evaluation_criteria="domain expertise",
        )

        assert len(scorer.judges) == 2
        assert scorer.aggregation_method == AggregationMethod.WEIGHTED_MEAN
        assert scorer.evaluation_criteria == "domain expertise"
        # Weights are normalized, so 2.0 and 1.0 become 2/3 and 1/3
        assert abs(scorer.judges[0].weight - 2 / 3) < 1e-10
        assert abs(scorer.judges[1].weight - 1 / 3) < 1e-10
        assert scorer.judges[0].specialty == "domain_expert"
        assert scorer.judges[1].specialty == "domain_expert"
        assert "Expert_1" in scorer.judges[0].name
        assert "Expert_2" in scorer.judges[1].name


@pytest.mark.integration
class TestPanelJudgeIntegration:
    """Integration tests for panel judge scorers."""

    @pytest.mark.asyncio
    async def test_complete_panel_evaluation_flow(self):
        """Test a complete panel evaluation flow."""
        # Create diverse panel
        mock_responses = [
            json.dumps(
                {
                    "score": 4,
                    "reasoning": "Good technical accuracy",
                    "strengths": "Precise definitions",
                    "weaknesses": "Could use examples",
                    "confidence": 4,
                }
            ),
            json.dumps(
                {
                    "score": 5,
                    "reasoning": "Excellent clarity and completeness",
                    "strengths": "Clear explanations",
                    "weaknesses": "None",
                    "confidence": 5,
                }
            ),
        ]

        model1 = MockLLMModel()
        model2 = MockLLMModel()

        scorer = SpecializedPanelScorer.create_diverse_panel(
            models=[model1, model2],
            evaluation_criteria="technical accuracy and clarity",
        )

        # Mock the async generate methods
        with (
            patch.object(model1, "generate", new_callable=AsyncMock) as mock_generate1,
            patch.object(model2, "generate", new_callable=AsyncMock) as mock_generate2,
        ):
            mock_generate1.return_value = mock_responses[0]
            mock_generate2.return_value = mock_responses[1]

            result = await scorer.evaluate(
                input_text="Explain machine learning",
                output_text="Machine learning is a subset of AI that enables computers to learn from data",
            )

        assert result.score > 0.0
        assert result.passed is True
        assert "Panel of 2 LLM Judges Evaluation" in result.reasoning
        # The evaluation criteria is used in the prompt, not in the reasoning output
        assert "technical accuracy and clarity" in scorer.evaluation_criteria

    def test_panel_scoring_statistics(self):
        """Test that panel scoring tracks statistics correctly."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Mock the evaluate method to return a specific score
        with patch.object(scorer, "evaluate", new_callable=AsyncMock) as mock_evaluate:
            mock_evaluate.return_value.score = 0.85
            mock_evaluate.return_value.passed = True

            # Score multiple predictions
            result1 = scorer.score("Response 1", "Expected 1")
            result2 = scorer.score("Response 2", "Expected 2")

            # The score method returns the score but doesn't track statistics automatically
            # We need to manually track them for testing
            scorer._track_score(result1)
            scorer._track_score(result2)

            # Now check the statistics
            assert scorer.total_scores == 2
            assert abs(scorer.score_sum - 1.7) < 1e-10
            assert len(scorer.scores_history) == 2

    def test_panel_input_validation(self):
        """Test input validation for panel scorer."""
        model = MockLLMModel()
        judge = JudgeConfig(model=model)
        scorer = PanelOfJudgesScorer(judges=[judge])

        # Test empty prediction
        assert scorer.score("", "ground_truth") == 0.0

        # Test empty ground truth
        assert scorer.score("prediction", "") == 0.0

        # Test None inputs
        assert scorer.score(None, "ground_truth") == 0.0
        assert scorer.score("prediction", None) == 0.0
