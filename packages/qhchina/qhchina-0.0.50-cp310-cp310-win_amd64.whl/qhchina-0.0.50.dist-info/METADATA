Metadata-Version: 2.4
Name: qhchina
Version: 0.0.50
Summary: A Python package for NLP tasks related to Chinese text.
Author-email: Maciej Kurzynski <makurz@gmail.com>
License: Copyright 2025 Maciej Kurzynski
        
        Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Project-URL: Homepage, https://www.qhchina.org/
Project-URL: Documentation, https://www.qhchina.org/pkg_docs
Project-URL: Bug Tracker, https://github.com/mcjkurz/qhchina/issues
Keywords: digital humanities,nlp,Chinese,text analysis,corpus linguistics,topic modeling
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=2.0.2
Requires-Dist: scipy>=1.14.1
Requires-Dist: matplotlib>=3.10.0
Requires-Dist: scikit-learn>=1.6.1
Requires-Dist: tqdm
Requires-Dist: pandas
Dynamic: license-file

# qhChina Lab

**Quantitative Humanities China Lab** - A research lab and Python package for NLP tasks related to Chinese text analysis.

## Repository Structure

This repository contains:

1. **Python Package**: The `qhchina` package for Chinese text analysis in humanities research
2. **Lab Website**: The qhChina Lab website located in the `/docs` folder with information about our research, projects, resources, and documentation

## Python Package Features

- **Collocation Analysis**: Find significant word co-occurrences in text
- **Corpus Comparison**: Statistically compare different corpora
- **Word Embeddings**: Work with Word2Vec and other embedding models
- **Topic Modeling**: Fast LDA implementation with Cython acceleration

## Installation

```bash
pip install qhchina
```

## Usage Examples

### Topic Modeling with LDA

```python
from qhchina.analytics import LDAGibbsSampler

# Each document is a list of tokens
documents = [
    ["word1", "word2", "word3"],
    ["word2", "word4", "word5"],
    # ...
]

# Initialize and train the model
lda = LDAGibbsSampler(
    n_topics=10,
    iterations=500
)
lda.fit(documents)

# Get top words for each topic
for i, topic in enumerate(lda.get_topic_words(10)):
    print(f"Topic {i}: {[word for word, _ in topic]}")
```

For more examples, see the module documentation.

## Website

The lab website is built with Jekyll and includes:

- Information about the qhChina Lab and our research
- Project descriptions and updates
- Resources for Chinese humanities research
- Comprehensive documentation for the qhchina package
- Blog with updates and announcements

To run the website locally:

```bash
cd docs
bundle install
bundle exec jekyll serve
```

## Documentation

For complete API documentation and tutorials, visit:
https://mcjkurz.github.io/qhchina/

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## For Developers

### Building Wheels with Cython Extensions

The package includes Cython extensions for performance-critical components. When installed from PyPI, precompiled wheels should be available for common platforms.

To build wheels locally:

```bash
# Install build dependencies
pip install build wheel setuptools cython numpy

# Build the wheel
python -m build --wheel
```

The precompiled wheels are built using GitHub Actions and support:
- Linux (for Google Colab and other Linux environments)
- Windows 10/11

If you need to clean up before building:

```bash
# Clean while preserving .c files (for wheel building)
python clean_cython.py

# Clean everything including .c files
python clean_cython.py --all
```
