[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "pi-tokenize"
version = "0.0.1"
description = "Latest (almost) version of CPython's tokenizer machinery"
requires-python = ">=3.13"
license = {text = "MIT"}
authors = [
    {name = "Yury Selivanov", email = "yury@geldata.com"},
]
dependencies = []
readme = "README.md"

classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.13",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Text Processing :: Linguistic",
]

[project.urls]
Repository = "https://github.com/geldata/pi"


[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "ruff",
]


[tool.setuptools.packages.find]
where = ["."]
include = ["pi_tokenize*"]
exclude = ["tests*"]


[[tool.setuptools.ext-modules]]
name = "pi_tokenize._vendor.cpython._tokenize._tokenize"
sources = [
    "pi_tokenize/_vendor/cpython/_tokenize/module.c",
    "pi_tokenize/_vendor/cpython/_tokenize/token.c",
    "pi_tokenize/_vendor/cpython/_tokenize/compat.c",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer/buffer.c",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer/lexer.c",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer/state.c",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/file_tokenizer.c",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/helpers.c",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/readline_tokenizer.c",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/string_tokenizer.c",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/utf8_tokenizer.c",
]
include-dirs = [
    "pi_tokenize/_vendor/cpython/_tokenize",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer",
]
depends = [
    "pi_tokenize/_vendor/cpython/_tokenize/errcode.h",
    "pi_tokenize/_vendor/cpython/_tokenize/pycore_token.h",
    "pi_tokenize/_vendor/cpython/_tokenize/compat.h",
    "pi_tokenize/_vendor/cpython/_tokenize/pythoncapi_compat.h",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer/state.h",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer/buffer.h",
    "pi_tokenize/_vendor/cpython/_tokenize/lexer/lexer.h",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/helpers.h",
    "pi_tokenize/_vendor/cpython/_tokenize/tokenizer/tokenizer.h",
]


[tool.ruff]
target-version = "py313"
line-length = 79
indent-width = 4
exclude = [
    "pi_tokenize/_vendor/**",
    "build/**",
]

[tool.ruff.lint]
extend-select = [
    "B", # flake8-bugbear
    "E", # error
    "F", # pyflakes
    "G", # flake8-logging-format
    "W", # warning
]
extend-ignore = [
    "B018", # useless-expression
    "B023", # function-uses-loop-variable
    "B904", # raise-without-from-inside-except
    "E402", # module-import-not-at-top-of-file
    "E252", # missing-whitespace-around-parameter-equals
    "F541", # f-string-missing-placeholders
]

[tool.ruff.format]
exclude = [
    "pi_tokenize/_vendor/**",
]
