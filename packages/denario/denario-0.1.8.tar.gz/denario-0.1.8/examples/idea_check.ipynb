{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2242dd13-f045-48bc-812a-69c923cb76fc",
   "metadata": {},
   "source": [
    "# Check Idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726e140f-9c62-489f-91e0-2dea08dbfe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from denario import Denario, Journal\n",
    "\n",
    "project_dir = \"/Users/boris/CMBAgents/AstroPilotApp/project_app\"\n",
    "den = Denario(project_dir=project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8e4f02-8c00-4442-bdce-74ccebdae801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# den.set_idea(project_dir+\"/input_files/idea.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70a6945-c4e7-4642-aea8-234ca699c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "den.set_idea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34512cf0-43a4-4316-9348-52388a2797cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idea = den.research.idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae90aaa-85b3-41ec-87eb-f53b3f0a57f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**IDEA:** Dynamic Narrative Adaptation in PlayStation Games Using Player Emotion Recognition and Generative AI\n",
      "\n",
      "**Description:** This paper explores dynamic narrative adaptation in PlayStation games by leveraging player emotion recognition from facial expressions (using the PlayStation Camera) and generative AI to modify the storyline in real-time. The system will analyze player emotions (e.g., frustration, excitement, boredom) and use a generative AI model to adjust narrative elements, such as dialogue, plot twists, and character interactions, to maintain optimal engagement and emotional investment.\n"
     ]
    }
   ],
   "source": [
    "print(idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014a2e06-fb1f-4cb1-aa3e-bdafe0b22b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from futurehouse_client import FutureHouseClient, JobNames\n",
    "from futurehouse_client.models import (\n",
    "    RuntimeConfig,\n",
    "    TaskRequest,\n",
    ")\n",
    "from ldp.agent import AgentConfig\n",
    "import os\n",
    "fhkey = os.getenv(\"FUTURE_HOUSE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32fa0ca-b5d7-4224-ad35-cf01bbb63784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fh_client = FutureHouseClient(\n",
    "    api_key=fhkey,\n",
    ")\n",
    "\n",
    "check_idea_prompt = rf\"\"\"\n",
    "Has anyone worked on or explored the following idea?\n",
    "\n",
    "{idea}\n",
    " \n",
    "<DESIRED_RESPONSE_FORMAT>\n",
    "Answer: <yes or no>\n",
    "\n",
    "Related previous work: <describe previous literature on the topic>\n",
    "</DESIRED_RESPONSE_FORMAT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0bad188-5872-402f-bb4b-2177ba332743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_data = TaskRequest(\n",
    "    name=JobNames.from_string(\"owl\"),\n",
    "    query=check_idea_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1394d006-5c56-442c-bb77-97efd26bd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_response = fh_client.run_tasks_until_done(task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25e3a98-bc06-498d-913a-839fdcf86d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: No\n",
      "\n",
      "Related previous work: The reviewed literature reveals significant investigation into affective computing in games but does not demonstrate a system that combines real-time facial emotion recognition with generative AI for dynamic narrative adaptation in PlayStation games. Sinković (sinkovic2024aibasedaffectivemirroring pages 60-62) clearly explains that, due to limitations in current emotion recognition tools and integration challenges, AI was not employed for dynamic dialogue selection or real-time narrative adaptation, and manual control was instead preferred for narrative management. Similarly, other excerpts by Sinković (sinkovic2024aibasedaffectivemirroring pages 10-13, sinkovic2024aibasedaffectivemirroring pages 13-19, sinkovic2024aibasedaffectivemirroring pages 19-28, sinkovic2024aibasedaffectivemirroring pages 39-43, sinkovic2024aibasedaffectivemirroring pages 84-87, sinkovic2024aibasedaffectivemirroring pages 28-32) emphasize the exploration of facial expression analysis for NPC behavioral adaptation but do not report an implementation that modifies the narrative dynamically in response to player emotion using generative AI on PlayStation platforms.\n",
      "\n",
      "Christou’s work (christou2017anaffectivegaming pages 15-20, christou2017anaffectivegaming pages 20-23) demonstrates affective gaming through devices collecting physiological signals and facial expressions to adjust gameplay and other elements; however, these studies do not specifically address the adaptation of narrative storylines in real time on PlayStation games via generative AI. Kumaran et al. (kumaran2023scenecraftautomatinginteractive pages 1-2) present SCENECRAFT, a framework for generating interactive narrative scenes using natural language processing, yet they rely on high-level narrative inputs rather than integrating real-time facial emotion data from PlayStation hardware.\n",
      "\n",
      "The Emo Tale project by Manogyna et al. (manogyna2025emotaleshaping pages 3-6, manogyna2025emotaleshaping pages 1-3) explores dynamically adapting narrative trajectories based on readers’ affective states using multimodal emotion recognition and generative AI, but it does not extend to a PlayStation gaming context or employ facial emotion recognition from the PlayStation Camera. Hutson (hutson2024adaptiveworldsgenerative pages 1-3, hutson2024adaptiveworldsgenerative pages 3-4) focuses on the use of generative AI for real-time adaptive content and narrative generation across gaming environments, but without leveraging player biometric data such as facial expressions for live narrative modifications on PlayStation. Finally, Li (li2025embodiedaiagent pages 1-3) discusses embodied AI that adapts its interactive responses based on detected emotions, yet this work does not involve modifying game storylines dynamically on commercial consoles.\n",
      "\n",
      "In summary, while these works collectively establish foundational concepts in affective computing, emotion recognition, and generative narrative techniques, none of them specifically combine player emotion recognition from facial expressions with generative AI to drive real-time storyline modifications in PlayStation games.\n"
     ]
    }
   ],
   "source": [
    "print(task_response[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de4d38e6-4b31-412b-9bd6-48976a70caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "Has anyone worked on or explored the following idea?\n",
      "\n",
      "**IDEA:** Dynamic Narrative Adaptation in PlayStation Games Using Player Emotion Recognition and Generative AI\n",
      "\n",
      "**Description:** This paper explores dynamic narrative adaptation in PlayStation games by leveraging player emotion recognition from facial expressions (using the PlayStation Camera) and generative AI to modify the storyline in real-time. The system will analyze player emotions (e.g., frustration, excitement, boredom) and use a generative AI model to adjust narrative elements, such as dialogue, plot twists, and character interactions, to maintain optimal engagement and emotional investment.\n",
      " \n",
      "<DESIRED_RESPONSE_FORMAT>\n",
      "Answer: <yes or no>\n",
      "\n",
      "Related previous work: <describe previous literature on the topic>\n",
      "</DESIRED_RESPONSE_FORMAT>\n",
      "\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Related previous work: The reviewed literature reveals significant investigation into affective computing in games but does not demonstrate a system that combines real-time facial emotion recognition with generative AI for dynamic narrative adaptation in PlayStation games. Sinković (sinkovic2024aibasedaffectivemirroring pages 60-62) clearly explains that, due to limitations in current emotion recognition tools and integration challenges, AI was not employed for dynamic dialogue selection or real-time narrative adaptation, and manual control was instead preferred for narrative management. Similarly, other excerpts by Sinković (sinkovic2024aibasedaffectivemirroring pages 10-13, sinkovic2024aibasedaffectivemirroring pages 13-19, sinkovic2024aibasedaffectivemirroring pages 19-28, sinkovic2024aibasedaffectivemirroring pages 39-43, sinkovic2024aibasedaffectivemirroring pages 84-87, sinkovic2024aibasedaffectivemirroring pages 28-32) emphasize the exploration of facial expression analysis for NPC behavioral adaptation but do not report an implementation that modifies the narrative dynamically in response to player emotion using generative AI on PlayStation platforms.\n",
      "\n",
      "Christou’s work (christou2017anaffectivegaming pages 15-20, christou2017anaffectivegaming pages 20-23) demonstrates affective gaming through devices collecting physiological signals and facial expressions to adjust gameplay and other elements; however, these studies do not specifically address the adaptation of narrative storylines in real time on PlayStation games via generative AI. Kumaran et al. (kumaran2023scenecraftautomatinginteractive pages 1-2) present SCENECRAFT, a framework for generating interactive narrative scenes using natural language processing, yet they rely on high-level narrative inputs rather than integrating real-time facial emotion data from PlayStation hardware.\n",
      "\n",
      "The Emo Tale project by Manogyna et al. (manogyna2025emotaleshaping pages 3-6, manogyna2025emotaleshaping pages 1-3) explores dynamically adapting narrative trajectories based on readers’ affective states using multimodal emotion recognition and generative AI, but it does not extend to a PlayStation gaming context or employ facial emotion recognition from the PlayStation Camera. Hutson (hutson2024adaptiveworldsgenerative pages 1-3, hutson2024adaptiveworldsgenerative pages 3-4) focuses on the use of generative AI for real-time adaptive content and narrative generation across gaming environments, but without leveraging player biometric data such as facial expressions for live narrative modifications on PlayStation. Finally, Li (li2025embodiedaiagent pages 1-3) discusses embodied AI that adapts its interactive responses based on detected emotions, yet this work does not involve modifying game storylines dynamically on commercial consoles.\n",
      "\n",
      "In summary, while these works collectively establish foundational concepts in affective computing, emotion recognition, and generative narrative techniques, none of them specifically combine player emotion recognition from facial expressions with generative AI to drive real-time storyline modifications in PlayStation games.\n",
      "\n",
      "References\n",
      "\n",
      "1. (sinkovic2024aibasedaffectivemirroring pages 60-62): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "2. (christou2017anaffectivegaming pages 15-20): C Christou. An affective gaming scenario using the kinect sensors. Unknown journal, 2017.\n",
      "\n",
      "3. (christou2017anaffectivegaming pages 20-23): C Christou. An affective gaming scenario using the kinect sensors. Unknown journal, 2017.\n",
      "\n",
      "4. (kumaran2023scenecraftautomatinginteractive pages 1-2): Vikram Kumaran, Jonathan Rowe, Bradford W. Mott, and James C. Lester. Scenecraft: automating interactive narrative scene generation in digital games with large language models. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 19:86-96, Oct 2023. URL: https://doi.org/10.1609/aiide.v19i1.27504, doi:10.1609/aiide.v19i1.27504. This article has 61 citations.\n",
      "\n",
      "5. (manogyna2025emotaleshaping pages 3-6): K. Sai Manogyna, M. Vasudevan, B. Susindra Reddy, S. Hussna Begum, D. Madhu Babu, and S. Ibrahim. Emo tale: shaping narrative trajectories with reader's affective states. Challenges in Information, Communication and Computing Technology, pages 678-683, Nov 2025. URL: https://doi.org/10.1201/9781003559092-117, doi:10.1201/9781003559092-117. This article has 0 citations.\n",
      "\n",
      "6. (sinkovic2024aibasedaffectivemirroring pages 10-13): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "7. (sinkovic2024aibasedaffectivemirroring pages 13-19): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "8. (sinkovic2024aibasedaffectivemirroring pages 19-28): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "9. (sinkovic2024aibasedaffectivemirroring pages 39-43): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "10. (sinkovic2024aibasedaffectivemirroring pages 84-87): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "11. (hutson2024adaptiveworldsgenerative pages 1-3): J Hutson. Adaptive worlds: generative ai in game design and future of gaming, and interactive media. Unknown journal, 2024.\n",
      "\n",
      "12. (hutson2024adaptiveworldsgenerative pages 3-4): J Hutson. Adaptive worlds: generative ai in game design and future of gaming, and interactive media. Unknown journal, 2024.\n",
      "\n",
      "13. (manogyna2025emotaleshaping pages 1-3): K. Sai Manogyna, M. Vasudevan, B. Susindra Reddy, S. Hussna Begum, D. Madhu Babu, and S. Ibrahim. Emo tale: shaping narrative trajectories with reader's affective states. Challenges in Information, Communication and Computing Technology, pages 678-683, Nov 2025. URL: https://doi.org/10.1201/9781003559092-117, doi:10.1201/9781003559092-117. This article has 0 citations.\n",
      "\n",
      "14. (sinkovic2024aibasedaffectivemirroring pages 28-32): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "15. (li2025embodiedaiagent pages 1-3): Embodied AI Agent for Co-creation Ecosystem: Elevating Human-AI Co-creation through Emotion Recognition and Dynamic Personality Adaptation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(task_response[0].formatted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e14321-3bb8-4129-9605-1ce1b747d5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_class_sz)",
   "language": "python",
   "name": "venv_class_sz312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
