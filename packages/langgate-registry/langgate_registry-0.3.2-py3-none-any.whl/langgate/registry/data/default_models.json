{
  "eleutheria/vllm/el-1": {
    "name": "El-1",
    "mode": "chat",
    "service_provider": "eleutheria/vllm",
    "model_provider": "eleutheria",
    "model_provider_name": "Eleutheria",
    "context": {
      "max_input_tokens": 4096,
      "max_output_tokens": 4096
    },
    "costs": {
      "input_cost_per_token": "0",
      "output_cost_per_token": "0"
    },
    "description": "Eleutheria's flagship language model, trained to respond without bias or censorship.",
    "source": "https://eleutheria.ai/pricing"
  },
  "openai/gpt-5-chat-latest": {
    "name": "ChatGPT-5",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "description": "ChatGPT-5 from OpenAI is designed for advanced, natural, multimodal, and context-aware conversations.",
    "context": {
      "max_input_tokens": 400000,
      "max_output_tokens": 128000
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.00001",
      "cache_read_input_token_cost": "0.000000125"
    },
    "capabilities": {
      "supports_tools": false,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_max_tokens": true,
      "supports_files": true,
      "supports_seed": true
    }
  },
  "openai/gpt-5": {
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "name": "GPT-5",
    "description": "GPT-5 is OpenAI's most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
    "context": {
      "max_input_tokens": 400000,
      "max_output_tokens": 128000
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.00001",
      "cache_read_input_token_cost": "0.000000125"
    },
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_structured_outputs": true,
      "supports_max_tokens": true,
      "supports_files": true,
      "supports_seed": true
    }
  },
  "openai/gpt-5-mini": {
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "name": "GPT-5 mini",
    "description": "GPT-5 mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost.",
    "context": {
      "max_input_tokens": 400000,
      "max_output_tokens": 128000
    },
    "costs": {
      "input_cost_per_token": "0.00000025",
      "output_cost_per_token": "0.000002",
      "cache_read_input_token_cost": "0.000000025"
    },
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_structured_outputs": true,
      "supports_max_tokens": true,
      "supports_files": true,
      "supports_seed": true
    }
  },
  "openai/gpt-5-nano": {
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "name": "GPT-5 nano",
    "description": "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.",
        "context": {
      "max_input_tokens": 400000,
      "max_output_tokens": 128000
    },
    "costs": {
      "input_cost_per_token": "0.00000005",
      "output_cost_per_token": "0.0000004",
      "cache_read_input_token_cost": "0.000000005"
    },
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_structured_outputs": true,
      "supports_max_tokens": true,
      "supports_files": true,
      "supports_seed": true
    }
  },
  "openrouter/openai/gpt-oss-120b": {
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "name": "GPT-OSS 120b",
    "description": "GPT-OSS 120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass. The model supports configurable reasoning depth, chain-of-thought access, native tool use, browsing, and generating structured output.",
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 131072
    },
    "costs": {
      "input_cost_per_token": "0.00000007256312",
      "output_cost_per_token": "0.0000002903936"
    },
    "capabilities": {
      "supports_tools": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "_data_source": "openrouter"
  },
  "openai/gpt-4o": {
    "name": "GPT-4o",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.0000025",
      "output_cost_per_token": "0.00001",
      "input_cost_per_token_batches": "0.00000125",
      "output_cost_per_token_batches": "0.000005",
      "cache_read_input_token_cost": "0.00000125",
      "input_cost_per_image": "0.003613"
    },
    "description": "The GPT-4o (omni) model from OpenAI builds upon the GPT-4 series with improved performance and multimodal capabilities. GPT-4o is great for most tasks.",
    "_last_updated": "2025-07-01T16:47:00.873641+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4o"
  },
  "openai/gpt-4o-mini": {
    "name": "GPT-4o mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "1.5E-7",
      "output_cost_per_token": "6E-7",
      "input_cost_per_token_batches": "7.5E-8",
      "output_cost_per_token_batches": "3E-7",
      "cache_read_input_token_cost": "7.5E-8",
      "input_cost_per_image": "0.000217"
    },
    "description": "A smaller version of GPT-4o optimized for efficiency.",
    "_last_updated": "2025-07-01T16:47:00.874047+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4o-mini"
  },
  "openai/o1": {
    "name": "o1",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.00006",
      "cache_read_input_token_cost": "0.0000075",
      "input_cost_per_image": "0.021675"
    },
    "description": "o1 is the first generation reasoning model from OpenAI. o1 spends additional time thinking (generating a chain of thought) before generating an answer, which makes it better for complex reasoning tasks, particularly in science, mathematics and coding.",
    "_last_updated": "2025-07-01T16:47:00.874359+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o1"
  },
  "openai/o3": {
    "name": "o3",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": false,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000008",
      "cache_read_input_token_cost": "5E-7",
      "reasoning_cost_per_token": "0",
      "input_cost_per_image": "0.00153"
    },
    "description": "OpenAI's o3 is their most powerful reasoning model, setting new state-of-the-art benchmarks in coding, math, science, and visual perception. It excels at complex queries requiring multi-faceted analysis, with particular strength in analyzing images, charts, and graphics.",
    "_last_updated": "2025-07-01T16:47:00.874586+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o3"
  },
  "openai/o3-mini": {
    "name": "o3-mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": false,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_vision": false,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.0000011",
      "output_cost_per_token": "0.0000044",
      "cache_read_input_token_cost": "5.5E-7"
    },
    "description": "o3-mini is a cost-efficient reasoning model from OpenAI. o3-mini excels at STEM, especially math and coding. As an alternative to o1-pro, it has a lower cost and lower latency.",
    "_last_updated": "2025-07-01T16:47:00.874946+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o3-mini"
  },
  "openai/o4-mini": {
    "name": "o4-mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.0000011",
      "output_cost_per_token": "0.0000044",
      "cache_read_input_token_cost": "2.75E-7",
      "input_cost_per_image": "0.0008415"
    },
    "description": "o4-mini is a new reasoning model from OpenAI that balances performance and cost.",
    "openrouter_model_id": "openai/o4-mini",
    "_last_updated": "2025-07-01T16:47:00.875185+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o4-mini"
  },
  "openai/o3-pro": {
    "name": "o3 Pro",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": false,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.00002",
      "output_cost_per_token": "0.00008",
      "reasoning_cost_per_token": "0",
      "input_cost_per_image": "0.0153"
    },
    "description": "OpenAI's o-series models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3 Pro model uses more compute than o3, spending longer thinking through a problem.",
    "_last_updated": "2025-07-01T16:47:00.875389+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o3-pro"
  },
  "openai/gpt-4.1": {
    "name": "GPT-4.1",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1047576,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000008",
      "cache_read_input_token_cost": "5E-7"
    },
    "description": "GPT-4.1 is the latest iteration of OpenAI's flagship model with improved capabilities across all domains.",
    "openrouter_model_id": "openai/gpt-4.1",
    "_last_updated": "2025-07-01T16:47:00.875592+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4.1"
  },
  "openai/gpt-4.1-mini": {
    "name": "GPT-4.1 mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1047576,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "4E-7",
      "output_cost_per_token": "0.0000016",
      "cache_read_input_token_cost": "1E-7"
    },
    "description": "GPT-4.1 mini is a cost-effective version of GPT-4.1 optimized for efficiency.",
    "openrouter_model_id": "openai/gpt-4.1-mini",
    "_last_updated": "2025-07-01T16:47:00.875823+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4.1-mini"
  },
  "openai/gpt-4.1-nano": {
    "name": "GPT-4.1 nano",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1047576,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "1E-7",
      "output_cost_per_token": "4E-7",
      "cache_read_input_token_cost": "2.5E-8"
    },
    "description": "GPT-4.1 nano is the smallest and most cost-effective version of GPT-4.1.",
    "openrouter_model_id": "openai/gpt-4.1-nano",
    "_last_updated": "2025-07-01T16:47:00.876023+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4.1-nano"
  },
  "openai/chatgpt-4o-latest": {
    "name": "ChatGPT-4o",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.000005",
      "output_cost_per_token": "0.000015",
      "input_cost_per_image": "0.007225"
    },
    "description": "ChatGPT-4o is the most recent version of the conversational GPT-4o model used in ChatGPT.",
    "openrouter_model_id": "openai/chatgpt-4o-latest",
    "_last_updated": "2025-07-01T16:47:00.876208+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/chatgpt-4o-latest"
  },
  "anthropic/claude-3-7-sonnet-latest": {
    "name": "Claude-3.7 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 64000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 3.7 Sonnet is Anthropic's hybrid reasoning model, featuring new 'extended thinking' capabilities. It excels at complex coding, STEM tasks, and multimodal data analysis with large context windows.",
    "openrouter_model_id": "anthropic/claude-3.7-sonnet",
    "_last_updated": "2025-07-01T16:47:00.876401+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.7-sonnet"
  },
  "anthropic/claude-3-5-sonnet-latest": {
    "name": "Claude-3.5 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed, particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",
    "deprecation_date": "2025-06-01",
    "tool_use_system_prompt_tokens": 159,
    "openrouter_model_id": "anthropic/claude-3.5-sonnet",
    "_last_updated": "2025-07-01T16:47:00.876601+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-sonnet"
  },
  "anthropic/claude-3-5-sonnet-20240620": {
    "name": "Claude-3.5 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed, particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",
    "deprecation_date": "2025-06-01",
    "tool_use_system_prompt_tokens": 159,
    "openrouter_model_id": "anthropic/claude-3.5-sonnet-20240620",
    "_last_updated": "2025-07-01T16:47:00.876791+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-sonnet-20240620"
  },
  "anthropic/claude-3-5-haiku-latest": {
    "name": "Claude 3.5 Haiku",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "8E-7",
      "output_cost_per_token": "0.000004",
      "cache_read_input_token_cost": "8E-8",
      "cache_creation_input_token_cost": "0.00000125",
      "input_cost_per_image": "0.0004"
    },
    "description": "Claude 3 Haiku is Anthropic's fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku quickly analyzes large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.",
    "deprecation_date": "2025-10-01",
    "tool_use_system_prompt_tokens": 264,
    "openrouter_model_id": "anthropic/claude-3.5-haiku",
    "_last_updated": "2025-07-01T16:47:00.876983+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-haiku"
  },
  "anthropic/claude-3-5-haiku-20241022": {
    "name": "Claude 3.5 Haiku",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "8E-7",
      "output_cost_per_token": "0.000004",
      "cache_read_input_token_cost": "8E-8",
      "cache_creation_input_token_cost": "0.00000125",
      "input_cost_per_image": "0.0004"
    },
    "description": "Claude 3 Haiku is Anthropic's fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku quickly analyzes large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.",
    "deprecation_date": "2025-10-01",
    "tool_use_system_prompt_tokens": 264,
    "openrouter_model_id": "anthropic/claude-3.5-haiku-20241022",
    "_last_updated": "2025-07-01T16:47:00.877155+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-haiku-20241022"
  },
  "anthropic/claude-3-opus-latest": {
    "name": "Claude 3 Opus",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 4096
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.000075",
      "cache_read_input_token_cost": "0.0000015",
      "cache_creation_input_token_cost": "0.00001875",
      "input_cost_per_image": "0.024"
    },
    "description": "Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It navigates open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showcasing the outer limits of generative AI.",
    "deprecation_date": "2025-03-01",
    "tool_use_system_prompt_tokens": 395,
    "openrouter_model_id": "anthropic/claude-3-opus",
    "_last_updated": "2025-07-01T16:47:00.877345+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3-opus"
  },
  "anthropic/claude-sonnet-4-0": {
    "name": "Claude-4 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 64000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 4 Sonnet is Anthropic's latest flagship model offering enhanced reasoning capabilities and improved performance across all domains.",
    "openrouter_model_id": "anthropic/claude-sonnet-4",
    "_last_updated": "2025-07-01T16:47:00.877539+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-sonnet-4"
  },
    "anthropic/claude-opus-4-1": {
    "name": "Claude-4.1 Opus",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 32000
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.000075",
      "cache_read_input_token_cost": "0.0000015",
      "cache_creation_input_token_cost": "0.00001875",
      "input_cost_per_image": "0.024"
    },
    "description": "Claude-4 Opus is Anthropic's most advanced model, featuring superior reasoning, analysis, and complex task capabilities.",
    "openrouter_model_id": "anthropic/claude-opus-4",
    "_last_updated": "2025-07-01T16:47:00.877727+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-opus-4"
  },
  "anthropic/claude-opus-4-0": {
    "name": "Claude-4 Opus",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 32000
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.000075",
      "cache_read_input_token_cost": "0.0000015",
      "cache_creation_input_token_cost": "0.00001875",
      "input_cost_per_image": "0.024"
    },
    "description": "Claude-4 Opus is Anthropic's most advanced model, featuring superior reasoning, analysis, and complex task capabilities.",
    "openrouter_model_id": "anthropic/claude-opus-4",
    "_last_updated": "2025-07-01T16:47:00.877727+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-opus-4"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-r1": {
    "name": "DeepSeek-R1",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000008"
    },
    "description": "R1 is a groundbreaking open source reasoning model developed by DeepSeek. R1 uses Chain of Thought (CoT) deductions to spend more time working through solutions to improve accuracy. Before providing a final answer, it generates detailed reasoning steps, allowing users to examine and leverage the model's thought process. This self-hosted version of R1 is deployed on US servers.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-v3": {
    "name": "DeepSeek-V3",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_response_schema": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.0000009",
      "output_cost_per_token": "0.0000009"
    },
    "description": "DeepSeek-V3 is an open-source 671B parameter Mixture-of-Experts (MoE) model that builds upon Meta's LLaMA, rivalling or exceeding similar closed source models across numerous benchmarks. This self-hosted version of V3 is deployed on US servers.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-chat-v3-0324": {
    "name": "DeepSeek Chat V3 0324",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 163840,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.0000009",
      "output_cost_per_token": "0.0000009"
    },
    "description": "DeepSeek Chat V3 0324 is an open-source 671B parameter Mixture-of-Experts (MoE) model that builds upon Meta's LLaMA, rivalling or exceeding similar closed source models across numerous benchmarks. This self-hosted version of V3 is deployed on US servers.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-r1-0528": {
    "name": "DeepSeek R1 0528",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_response_schema": true,
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 163840,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000008"
    },
    "description": "The updated DeepSeek-R1-0528 model delivers major improvements in reasoning, inference, and accuracy through enhanced post-training optimization and greater computational resources. This version performs at a level approaching models like o3 and Gemini 2.5 Pro, with notable gains in complex tasks such as math and programming. In the AIME 2025 benchmark, accuracy jumped from 70% to 87.5%, supported by deeper reasoning (23K vs. 12K tokens per question). The update also reduces hallucinations, improves function calling, and enhances the coding experience.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-r1-basic": {
    "name": "DeepSeek R1 Econ",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 163840,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000055",
      "output_cost_per_token": "0.00000219"
    },
    "description": "DeepSeek R1 Econ (Economy) is a cost-optimized deployment of DeepSeek-R1. Compared to the standard DeepSeek R1, this R1 (Economy) version provides lower per-token prices with slower speeds. Both models are identical, so there are no quality differences.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "name": "Llama 3.3 70B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 2048
    },
    "costs": {
      "input_cost_per_token": "9E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Llama 3.3 70B by Meta is an instruction tuned text only model, optimized for multilingual dialogue use cases. It improves upon Llama 3.1 70B with advances in tool calling, multilingual text support, math and coding. The model achieves stellar results in reasoning, math and instructions, providing similar performance as 3.1 405B but with significant speed and cost improvements.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "name": "Llama 3.1 405B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 128000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000003"
    },
    "description": "Llama is a 405 billion parameter open source model by Meta, fine-tuned for instruction following purposes. It excels at multilingual dialogue use cases. As the largest of the Llama 3.1 instruction tuned text only models, the 405B version is the most capable from the Llama 3.1 family. This model is served in FP8 closely matching reference implementation.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "name": "Llama 3.1 8B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "2E-7",
      "output_cost_per_token": "2E-7"
    },
    "description": "The Meta Llama 3.1 collection of multilingual models are pretrained and instruction tuned generative models in 8B, 70B and 405B sizes, optimized for multilingual dialogue use cases. The 8B model is the smallest of the family and most cost effective for basic instruction following tasks.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic": {
    "name": "Llama 4 Maverick",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_vision": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.00000022",
      "output_cost_per_token": "0.00000088"
    },
    "description": "The Llama 4 collection of models are natively multimodal AI models. Llama 4 Maverick is a general purpose LLM containing 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic": {
    "name": "Llama 4 Scout",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_vision": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.00000015",
      "output_cost_per_token": "0.0000006"
    },
    "description": "The Llama 4 collection of models are natively multimodal AI models. Llama 4 Scout is smaller than Maverick, with 17B active parameters, 16 experts and 109B total parameters. Its low cost, large context window, and multimodal understanding make it ideal for processing large amounts of data, such as documents, code, and images.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen2p5-72b-instruct": {
    "name": "Qwen 2.5 72B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "9E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Qwen2.5 72B is Alibaba's largest open-source language model, trained on 18 trillion tokens. It achieves strong performance across knowledge tasks (85+ on MMLU), coding (85+ on HumanEval), and mathematics (80+ on MATH). The model supports 29+ languages, handles up to 128K context tokens, can generate 8K tokens, and shows improved capabilities in instruction following, long-text generation, and structured data handling. It competes with larger models like Llama-3.1-405B and DeepSeek-V2.5.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "name": "Qwen 2.5 Coder 32B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": false
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 4096
    },
    "costs": {
      "input_cost_per_token": "9E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Qwen2.5 Coder is Alibaba's latest series of models trained specifically for coding tasks. The 32B model is the largest of the series. It achieves strong performance across coding tasks, matching the coding capabilities of GPT-4o. It also possesses good general and mathematical skills.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b": {
    "name": "Qwen 3 235B A22B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000022",
      "output_cost_per_token": "0.00000088"
    },
    "description": "Qwen3 is the latest evolution in the Qwen LLM series, featuring both dense and MoE models with major advancements in reasoning, agent capabilities, multilingual support, and instruction following. It uniquely allows seamless switching between \"thinking\" (for complex logic, math, coding) and \"non-thinking\" modes (for fast, general dialogue), delivering strong performance across tasks. The flagship model, Qwen3-235B-A22B, has 235B parameters (22B active), 94 layers, and a native context length of 32K, extendable to 131K with YaRN.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b": {
    "name": "Qwen 3 30B A3B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 40000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000015",
      "output_cost_per_token": "0.0000006"
    },
    "description": "Qwen3 is the latest evolution in the Qwen LLM series, featuring both dense and MoE models with major advancements in reasoning, agent capabilities, multilingual support, and instruction following. It uniquely allows seamless switching between \"thinking\" (for complex logic, math, coding) and \"non-thinking\" modes (for fast, general dialogue), delivering strong performance across tasks.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "gemini/gemini-2.0-flash": {
    "name": "Gemini 2.0 Flash",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_audio_output": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "1E-7",
      "output_cost_per_token": "4E-7",
      "cache_read_input_token_cost": "2.5E-8",
      "input_cost_per_audio_token": "7E-7",
      "input_cost_per_image": "0.0000258"
    },
    "description": "Gemini 2.0 Flash is a versatile multimodal model with a 1 million token context window, designed for diverse tasks.",
    "rpm": 10000,
    "tpm": 10000000,
    "openrouter_model_id": "google/gemini-2.0-flash-001",
    "_last_updated": "2025-07-01T16:47:00.880380+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.0-flash-001"
  },
  "gemini/gemini-2.0-flash-lite": {
    "name": "Gemini 2.0 Flash-Lite",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_audio_output": false,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "7.5E-8",
      "output_cost_per_token": "3E-7",
      "input_cost_per_audio_token": "7.5E-8"
    },
    "description": "Gemini 2.0 Flash-Lite is a cost-effective model optimized for low-latency and high-volume tasks. Flash-Lite's 1 million token context window makes it suitable for processing large amounts of data.",
    "rpm": 60000,
    "tpm": 10000000,
    "openrouter_model_id": "google/gemini-2.0-flash-lite-001",
    "_last_updated": "2025-07-01T16:47:00.880582+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.0-flash-lite-001"
  },
  "gemini/gemini-1.5-pro": {
    "name": "Gemini 1.5 Pro",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 2000000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.000005",
      "input_cost_per_token_above_128k_tokens": "0.000007",
      "output_cost_per_token_above_128k_tokens": "0.000021",
      "input_cost_per_image": "0.0006575"
    },
    "description": "Gemini 1.5 Pro is Google's flagship model with a 2 million token context window, capable of seamlessly analyzing, classifying, and summarizing large volumes of content for complex analytical tasks.",
    "rpm": 1000,
    "tpm": 4000000,
    "openrouter_model_id": "google/gemini-pro-1.5",
    "_last_updated": "2025-07-01T16:47:00.880765+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-pro-1.5"
  },
  "gemini/gemini-1.5-flash": {
    "name": "Gemini 1.5 Flash",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "7.5E-8",
      "output_cost_per_token": "3E-7",
      "cache_read_input_token_cost": "1.875E-8",
      "input_cost_per_token_above_128k_tokens": "1.5E-7",
      "output_cost_per_token_above_128k_tokens": "6E-7",
      "input_cost_per_image": "0.00004"
    },
    "description": "Gemini 1.5 Flash offers balanced performance for diverse tasks with a 1 million token context window.",
    "rpm": 2000,
    "tpm": 4000000,
    "openrouter_model_id": "google/gemini-flash-1.5",
    "_last_updated": "2025-07-01T16:47:00.880961+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-flash-1.5"
  },
  "gemini/gemini-1.5-flash-8b": {
    "name": "Gemini 1.5 Flash 8B",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "3.75E-8",
      "output_cost_per_token": "1.5E-7",
      "cache_read_input_token_cost": "1E-8",
      "input_cost_per_token_above_128k_tokens": "0",
      "output_cost_per_token_above_128k_tokens": "0"
    },
    "description": "Gemini 1.5 Flash 8B is designed for high-volume, lower-intelligence tasks with a 1 million token context window.",
    "rpm": 4000,
    "tpm": 4000000,
    "openrouter_model_id": "google/gemini-flash-1.5-8b",
    "_last_updated": "2025-07-01T16:47:00.881165+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-flash-1.5-8b"
  },
  "gemini/gemini-2.5-pro": {
    "name": "Gemini 2.5 Pro",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65536
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.00001",
      "cache_read_input_token_cost": "3.1E-7",
      "input_cost_per_image": "0.00516"
    },
    "description": "Gemini 2.5 Pro is Google's state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",
    "openrouter_model_id": "google/gemini-2.5-pro",
    "_last_updated": "2025-07-01T16:47:00.881327+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-pro"
  },
  "gemini/gemini-2.5-pro-preview-06-05": {
    "name": "Gemini 2.5 Pro 0605",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.00001",
      "cache_read_input_token_cost": "3.1E-7",
      "input_cost_per_image": "0.00516"
    },
    "description": "Gemini 2.5 Pro is Google's state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",
    "openrouter_model_id": "google/gemini-2.5-pro-preview-05-06",
    "_last_updated": "2025-07-01T16:47:00.881526+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-pro-preview-05-06"
  },
  "gemini/gemini-2.5-flash": {
    "name": "Gemini 2.5 Flash",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "modalities": {
      "input": ["text", "image", "audio", "video"],
      "output": ["text"]
    },
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "0.0000025",
      "cache_read_input_token_cost": "7.5E-8",
      "input_cost_per_image": "0.001238"
    },
    "description": "Gemini 2.5 Flash is a versatile multimodal model with a 1 million token context window, designed for diverse tasks.",
    "openrouter_model_id": "google/gemini-2.5-flash",
    "_last_updated": "2025-07-01T16:47:00.881700+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-flash"
  },
  "gemini/gemini-2.5-flash-lite-preview-06-17": {
    "name": "Gemini 2.5 Flash Lite",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "modalities": {
      "input": ["file", "text", "image"],
      "output": ["text"]
    },
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535
    },
    "costs": {
      "input_cost_per_token": "1E-7",
      "output_cost_per_token": "4E-7"
    },
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance compared to earlier Flash models.",
    "openrouter_model_id": "google/gemini-2.5-flash-lite-preview-06-17",
    "_last_updated": "2025-07-01T16:47:00.881890+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-flash-lite-preview-06-17"
  },
  "gemini/gemma-3-27b-it": {
    "name": "Gemma 3 27B",
    "mode": "chat",
    "service_provider": "google",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": false,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "9E-8",
      "output_cost_per_token": "1.7E-7",
      "input_cost_per_image": "0.0000256"
    },
    "description": "Gemma 3 is an open-source model family from Google that supports multimodal inputs with a 128k token context window and comes in four sizes (1B, 4B, 12B, and 27B parameters). It excels at math, reasoning, coding, and multilingual tasks across 140+ languages.",
    "openrouter_model_id": "google/gemma-3-27b-it",
    "_last_updated": "2025-07-01T16:47:00.882080+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemma-3-27b-it"
  },
  "openrouter/google/gemma-3-27b-it:free": {
    "name": "Gemma 3 27B",
    "mode": "chat",
    "service_provider": "google",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": false,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "9E-8",
      "output_cost_per_token": "1.7E-7",
      "input_cost_per_image": "0.0000256"
    },
    "description": "Gemma 3 is an open-source model family from Google that supports multimodal inputs with a 128k token context window and comes in four sizes (1B, 4B, 12B, and 27B parameters). It excels at math, reasoning, coding, and multilingual tasks across 140+ languages.",
    "openrouter_model_id": "google/gemma-3-27b-it",
    "_last_updated": "2025-07-01T16:47:00.882226+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemma-3-27b-it"
  },
  "xai/grok-2-latest": {
    "name": "Grok 2",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 131072
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.00001"
    },
    "description": "Grok-2 is the latest API-accessible release from xAI, supporting structured outputs. Grok-2 has yet to be open sourced and the architecture remains unknown. Grok 1 was a 314 billion parameter Mixture-of-Experts model. Grok models are known to have a sense of humour and xAI CEO Elon Musk says Grok is intended to be \"a maximum truth-seeking AI that tries to understand the nature of the universe.\"",
    "openrouter_model_id": "x-ai/grok-2-1212",
    "_last_updated": "2025-07-01T16:47:00.882494+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-2-1212"
  },
  "xai/grok-2-vision-latest": {
    "name": "Grok 2 Vision",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.00001",
      "input_cost_per_image": "0.0036"
    },
    "description": "Grok-2 Vision is the latest API-accessible multi-modal release from xAI. Grok-2 Vision can process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs.",
    "openrouter_model_id": "x-ai/grok-2-vision-1212",
    "_last_updated": "2025-07-01T16:47:00.882662+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-2-vision-1212"
  },
  "xai/grok-3-latest": {
    "name": "Grok 3",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "7.5E-7",
      "cached_prompt_text_token_price": "0.0000075"
    },
    "description": "Grok-3 is the latest model from xAI. Grok models are known to have a sense of humour and xAI CEO Elon Musk says Grok is intended to be \"a maximum truth-seeking AI that tries to understand the nature of the universe.\"",
    "openrouter_model_id": "x-ai/grok-3-beta",
    "_last_updated": "2025-07-01T16:47:00.883191+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-3-beta"
  },
  "xai/grok-3-fast": {
    "name": "Grok 3 Fast",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "0.00005",
      "output_cost_per_token": "0.00025",
      "cached_prompt_text_token_price": "0.0000125"
    },
    "description": "Grok 3 Fast is a faster variant of Grok-3 from xAI, optimized for speed while maintaining high performance.",
    "source": "https://docs.x.ai/docs/models"
  },
  "xai/grok-3-mini": {
    "name": "Grok 3 Mini",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "5E-7",
      "cache_read_input_token_cost": "7.5E-8",
      "cached_prompt_text_token_price": "0.00000075"
    },
    "description": "Grok 3 Mini is a A lightweight model from xAI that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge.",
    "openrouter_model_id": "x-ai/grok-3-mini-beta",
    "_last_updated": "2025-07-01T16:47:00.884168+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-3-mini-beta"
  },
  "xai/grok-3-mini-fast": {
    "name": "Grok 3 Mini Fast",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "0.000006",
      "output_cost_per_token": "0.00004",
      "cached_prompt_text_token_price": "0.0000015"
    },
    "description": "Grok 3 Mini Fast is a speed-omtimized deployment of Grok 3 Mini. It responds faster in exchange for a higher per-token cost. Both models are identical, so there are no quality differences.",
    "source": "https://docs.x.ai/docs/models"
  },
  "xai/grok-4-latest": {
    "name": "Grok 4",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_vision": true
    },
    "context": {
      "max_input_tokens": 256000,
      "max_output_tokens": 256000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "7.5E-7",
      "cached_prompt_text_token_price": "0.0000075"
    },
    "description": "Grok-4 is the latest groundbreaking reasoning model from xAI. Grok models are known to have a sense of humour and xAI CEO Elon Musk says Grok is intended to be \"a maximum truth-seeking AI that tries to understand the nature of the universe.\" Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs.",
    "openrouter_model_id": "x-ai/grok-4"
  },
  "minimax/MiniMax-Text-01": {
    "name": "MiniMax-01",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_vision": true,
      "supports_system_messages": true
    },
    "context": {
      "max_input_tokens": 1000192,
      "max_output_tokens": 1000192
    },
    "costs": {
      "input_cost_per_token": "2E-7",
      "output_cost_per_token": "0.0000011"
    },
    "openrouter_model_id": "minimax/minimax-01",
    "description": "MiniMax-01 combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding that can handle a context of over a million tokens. It is a 456 billion parameter Mixture-of-Experts (MoE) model with 45.9 billion params active at inference, combining Lightning and Softmax Attention. The image model adopts the \u201cViT-MLP-LLM\u201d framework, trained on the text model.",
    "_last_updated": "2025-07-01T16:47:00.884550+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-01"
  },
  "openrouter/minimax/minimax-01": {
    "name": "MiniMax-01",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_vision": true,
      "supports_system_messages": true
    },
    "context": {
      "max_input_tokens": 1000192,
      "max_output_tokens": 1000192
    },
    "costs": {
      "input_cost_per_token": "2E-7",
      "output_cost_per_token": "0.0000011"
    },
    "openrouter_model_id": "minimax/minimax-01",
    "description": "MiniMax-01 combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding that can handle a context of over a million tokens. It is a 456 billion parameter Mixture-of-Experts (MoE) model with 45.9 billion params active at inference, combining Lightning and Softmax Attention. The image model adopts the \u201cViT-MLP-LLM\u201d framework, trained on the text model.",
    "_last_updated": "2025-07-01T16:47:00.884798+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-01"
  },
  "minimax/MiniMax-M1": {
    "name": "MiniMax-M1",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "0.00000165"
    },
    "openrouter_model_id": "minimax/minimax-m1",
    "description": "MiniMax-M1 is an open-weight, large-scale reasoning model with extended context, designed for high-efficiency reasoning on complex, multi-step tasks. It leverages a 456 billion parameter hybrid Mixture-of-Experts (MoE) architecture with 45.9B params active per token, paired with a custom \"lightning attention\" mechanism, allowing it to process up to 1 million tokens while maintaining competitive FLOP efficiency. Trained via a custom reinforcement learning pipeline (CISPO), M1 is competitive against other open models like DeepSeek R1 and Qwen3-235B.",
    "_last_updated": "2025-07-01T16:47:00.885078+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-m1"
  },
  "openrouter/minimax/minimax-m1": {
    "name": "MiniMax-M1",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_response_schema": true,
      "supports_system_messages": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "0.00000165"
    },
    "openrouter_model_id": "minimax/minimax-m1",
    "description": "MiniMax-M1 is an open-weight, large-scale reasoning model with extended context, designed for high-efficiency reasoning on complex, multi-step tasks. It leverages a 456 billion parameter hybrid Mixture-of-Experts (MoE) architecture with 45.9B params active per token, paired with a custom \"lightning attention\" mechanism, allowing it to process up to 1 million tokens while maintaining competitive FLOP efficiency. Trained via a custom reinforcement learning pipeline (CISPO), M1 is competitive against other open models like DeepSeek R1 and Qwen3-235B.",
    "_last_updated": "2025-07-01T16:47:00.885301+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-m1"
  },
  "mistralai/magistral-medium-latest": {
    "name": "Magistral Medium",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 40960,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000005"
    },
    "description": "Magistral is the first reasoning model series by Mistral AI, released in June 2025. Magistral is designed to think before responding, excelling in domain-specific, transparent, and multilingual reasoning. Magistral Small is a 24B parameter open-source model while Magistral Medium is a more powerful, enterprise version.",
    "_last_updated": "2025-07-01T16:47:00.885747+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/magistral-medium-2506"
  },
  "mistralai/magistral-small-latest": {
    "name": "Magistral Small",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 40000,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "5E-7",
      "output_cost_per_token": "0.0000015"
    },
    "description": "Magistral is the first reasoning model series by Mistral AI, released in June 2025. Magistral Small is a 24B parameter model based on Mistral-Small-3.1, enhanced through supervised fine-tuning on traces from Magistral Medium and reinforcement learning. It is optimized for reasoning with support for over 20 languages.",
    "_last_updated": "2025-07-01T16:47:00.886199+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/magistral-small-2506"
  },
  "mistralai/mistral-large-latest": {
    "name": "Mistral Large Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000006"
    },
    "description": "Mistral Large is Mistral AI's flagship model designed for complex problem solving and advanced tasks.",
    "openrouter_model_id": "mistralai/mistral-large-latest",
    "_last_updated": "2025-07-01T16:47:00.886713+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-large-2411"
  },
  "mistralai/mistral-medium-latest": {
    "name": "Mistral Medium Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "4E-7",
      "output_cost_per_token": "0.000002"
    },
    "description": "Mistral Medium offers balanced performance for general-purpose tasks.",
    "openrouter_model_id": "mistralai/mistral-medium-latest",
    "_last_updated": "2025-07-01T16:47:00.887139+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-medium-3"
  },
  "mistralai/pixtral-large-latest": {
    "name": "Pixtral Large Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000006",
      "input_cost_per_image": "0.002888"
    },
    "description": "Pixtral Large is Mistral AI's multimodal model with vision capabilities for processing images and text.",
    "openrouter_model_id": "mistralai/pixtral-large-latest",
    "_last_updated": "2025-07-01T16:47:00.887511+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/pixtral-large-2411"
  },
  "mistralai/mistral-small-latest": {
    "name": "Mistral Small Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 96000,
      "max_output_tokens": 96000
    },
    "costs": {
      "input_cost_per_token": "0.0000002",
      "output_cost_per_token": "0.0000006"
    },
    "description": "Mistral Small is a cost-effective model optimized for simple tasks and high throughput.",
    "openrouter_model_id": "mistralai/mistral-small-latest",
    "_last_updated": "2025-07-01T16:47:00.888055+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-small-3.2-24b-instruct:free"
  },
  "mistralai/codestral-latest": {
    "name": "Codestral Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 262144,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Codestral is Mistral AI's specialized model for code generation and programming tasks.",
    "openrouter_model_id": "mistralai/codestral-latest",
    "_last_updated": "2025-07-01T16:47:00.888474+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/codestral-2501"
  },
  "mistralai/open-mixtral-8x22b": {
    "name": "Mixtral 8x22B",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 65536,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.0000002",
      "output_cost_per_token": "0.0000006"
    },
    "description": "Open Mixtral 8x22B is a mixture-of-experts model with 22B active parameters out of 141B total.",
    "source": "https://mistral.ai/pricing#api-pricing"
  },
  "mistralai/open-mistral-nemo": {
    "name": "Mistral Nemo",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 131072
    },
    "costs": {
      "input_cost_per_token": "1E-8",
      "output_cost_per_token": "1.1E-8"
    },
    "description": "Mistral Nemo is a 12B model with state-of-the-art reasoning, world knowledge, and coding performance.",
    "openrouter_model_id": "mistralai/mistral-nemo",
    "_last_updated": "2025-07-01T16:47:00.889029+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-nemo"
  },
  "openai/gpt-image-1": {
    "name": "GPT Image 1",
    "mode": "image",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "description": "GPT-Image-1 is a multimodal model that accepts both text and image inputs, and produces image outputs.",
    "costs": {
      "token_costs": {
        "input_cost_per_token": 0.00001,
        "output_cost_per_token": 0.00004,
        "input_cached_cost_per_token": 0.0000025
      },
      "image_generation": {
        "quality_tiers": {
          "low": {
            "1024x1024": 0.011,
            "1024x1536": 0.016,
            "1536x1024": 0.016
          },
          "medium": {
            "1024x1024": 0.042,
            "1024x1536": 0.063,
            "1536x1024": 0.063
          },
          "high": {
            "1024x1024": 0.167,
            "1024x1536": 0.25,
            "1536x1024": 0.25
          }
        }
      }
    }
  },
  "openai/dall-e-3": {
    "name": "DALL-E 3",
    "mode": "image",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "description": "DALL-E 3 is the third version of OpenAI's DALL-E model, which generates images from textual descriptions.",
    "costs": {
      "image_generation": {
        "quality_tiers": {
          "standard": {
            "1024x1024": 0.04,
            "1024x1792": 0.08,
            "1792x1024": 0.08
          },
          "hd": {
            "1024x1024": 0.08,
            "1024x1792": 0.12,
            "1792x1024": 0.12
          }
        }
      }
    }
  },
  "replicate/black-forest-labs/flux-kontext-max": {
    "name": "FLUX.1 Kontext [max]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 Kontext [max] is a premium text-based image editing model that delivers maximum performance and improved typography generation for transforming images through natural language prompts.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.08
      }
    }
  },
  "replicate/black-forest-labs/flux-kontext-pro": {
    "name": "FLUX.1 Kontext [pro]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 Kontext [pro] is a state-of-the-art text-based image editing model that delivers high-quality outputs with excellent prompt following and consistent results for transforming images through natural language.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.04
      }
    }
  },
  "replicate/black-forest-labs/flux-kontext-dev": {
    "name": "FLUX.1 Kontext [dev]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 Kontext [dev] is an open-weight version of FLUX.1 Kontext, a text-based image editing model with excellent prompt following and consistent results for transforming images through natural language.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.025
      }
    }
  },
  "replicate/black-forest-labs/flux-1.1-pro-ultra": {
    "name": "FLUX 1.1 [pro] Ultra",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX1.1 [pro] in ultra and raw modes. Images are up to 4 megapixels. Use raw mode for realism.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.06
      }
    }
  },
  "replicate/black-forest-labs/flux-1.1-pro": {
    "name": "FLUX 1.1 [pro]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX 1.1 [pro] is a closed-source image generation model from Black Forest Labs, with excellent prompt following, visual quality, image detail, and output diversity. It offers faster generation times, achieving up to six times faster results than FLUX 1.0",
    "costs": {
      "image_generation": {
        "flat_rate": 0.04
      }
    }
  },
  "replicate/black-forest-labs/flux-pro": {
    "name": "FLUX.1 [pro]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 [pro] is a closed-source image generation model from Black Forest Labs, with excellent prompt following, visual quality, image detail, and output diversity.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.055
      }
    }
  },
  "replicate/black-forest-labs/flux-dev": {
    "name": "FLUX.1 [dev]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 dev is an open-weight, 12 billion parameter rectified flow transformer, distilled from FLUX.1 [pro], FLUX.1 [dev] obtains similar quality and prompt adherence capabilities, while being more efficient and faster.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.025
      }
    }
  },
  "replicate/black-forest-labs/flux-schnell": {
    "name": "FLUX.1 [schnell]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 [schnell] is a high-speed version of the FLUX.1 model.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.003
      }
    }
  },
  "replicate/black-forest-labs/flux-fill-pro": {
    "name": "FLUX.1 Fill [pro]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 Fill [pro] is a professional inpainting and outpainting model. Paint a mask over what you want to change and describe what you want there instead.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.05
      }
    }
  },
  "replicate/black-forest-labs/flux-fill-dev": {
    "name": "FLUX.1 Fill [dev]",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "black-forest-labs",
    "model_provider_name": "Black Forest Labs",
    "description": "FLUX.1 Fill [dev] is an open-weight inpainting model for editing and extending images, guidance-distilled from FLUX.1 Fill [pro]. Paint a mask over what you want to change and describe what you want there instead.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.04
      }
    }
  },
  "replicate/stability-ai/stable-diffusion-3.5-large": {
    "name": "SD 3.5 Large",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "stability-ai",
    "model_provider_name": "Stability AI",
    "description": "Stable Diffusion 3.5 Large is a text-to-image model from Stability AI that generates high-resolution images with fine details. It supports various artistic styles and produces diverse outputs from the same prompt, thanks to Query-Key Normalization.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.065
      }
    }
  },
  "replicate/stability-ai/stable-diffusion-3.5-large-turbo": {
    "name": "SD 3.5 Large Turbo",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "stability-ai",
    "model_provider_name": "Stability AI",
    "description": "Stable Diffusion 3.5 Large Turbo is a Multimodal Diffusion Transformer (MMDiT) text-to-image model with Adversarial Diffusion Distillation (ADD) that generates high-resolution images with fine details in just 4 inference steps. It supports various artistic styles, produces diverse outputs from the same prompt, and features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency with a focus on fewer inference steps.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.04
      }
    }
  },
  "replicate/stability-ai/stable-diffusion-3.5-medium": {
    "name": "SD 3.5 Medium",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "stability-ai",
    "model_provider_name": "Stability AI",
    "description": "Stable Diffusion 3.5 Medium is a 2.5 billion parameter Multimodal Diffusion Transformer with improvements (MMDiT-X) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. It introduces self-attention modules in the first 13 layers of the transformer, enhancing multi-resolution generation and overall image coherence.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.035
      }
    }
  },
  "replicate/stability-ai/sdxl:7762fd07cf82c948538e41f63f77d685e02b063e37e496e96eefd46c929f9bdc": {
    "name": "SDXL",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "stability-ai",
    "model_provider_name": "Stability AI",
    "description": "SDXL is a text-to-image generative AI model from Stability AI that creates beautiful images.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.0052
      }
    }
  },
  "replicate/bytedance/sdxl-lightning-4step:6f7a773af6fc3e8de9d5a3c00be77c17308914bf67772726aff83496ba1e3bbe": {
    "name": "SDXL-Lightning",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "bytedance",
    "model_provider_name": "ByteDance",
    "description": "SDXL-Lightning by ByteDance is a fast text-to-image model based on Stable Diffusion XL that makes high-quality images in as few as 4 steps.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.0014
      }
    }
  },
  "replicate/google/imagen-4-ultra": {
    "name": "Imagen 4 Ultra",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "google",
    "model_provider_name": "Google",
    "description": "Imagen 4 Ultra, Google's flagship text-to-image generation model, optimized higher quality at the expense of speed and cost.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.06
      }
    }
  },
  "replicate/google/imagen-4": {
    "name": "Imagen 4",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "google",
    "model_provider_name": "Google",
    "description": "Google's Imagen 4 flagship model for high-quality text-to-image generation.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.04
      }
    }
  },
  "replicate/google/imagen-4-fast": {
    "name": "Imagen 4 Fast",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "google",
    "model_provider_name": "Google",
    "description": "Imagen 4 Fast, Google's flagship text-to-image generation model, optimized for speed and cost efficiency.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.02
      }
    }
  },
  "replicate/recraft-ai/recraft-v3": {
    "name": "Recraft V3",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "recreaft-ai",
    "model_provider_name": "Recraft AI",
    "description": "Recraft V3 is a text-to-image model with the ability to generate long texts, and images in a wide list of styles.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.04
      }
    }
  },
  "replicate/recraft-ai/recraft-20b": {
    "name": "Recraft 20b",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "recreaft-ai",
    "model_provider_name": "Recraft AI",
    "description": "Recraft 20b is a text-to-image model designed to generate affordable images quickly.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.022
      }
    }
  },
  "replicate/recraft-ai/recraft-creative-upscale": {
    "name": "Recraft Creative Upscale",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "recreaft-ai",
    "model_provider_name": "Recraft AI",
    "description": "Creative Upscale focuses on enhancing details and refining complex elements in the image. It doesn't just increase resolution but adds depth by improving textures, fine details, and facial features.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.3
      }
    }
  },
  "replicate/recraft-ai/recraft-crisp-upscale": {
    "name": "Recraft Crisp Upscale",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "recreaft-ai",
    "model_provider_name": "Recraft AI",
    "description": "Designed to make images sharper and cleaner, Crisp Upscale increases overall quality, aiming to maintain the original image's essence in higher resolution.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.006
      }
    }
  },
  "replicate/minimax/image-01": {
    "name": "MiniMax Image-01",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "description": "Minimax's first image model, with character reference image support (human face) to use as the subject in the generated image.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.01
      }
    }
  },
  "replicate/prunaai/hidream-l1-full:3a1457ebedf387206f83c3c8b30e8ce495084c3b7d7328b5ed2d55304124851c": {
    "name": "HiDream-I1 Full",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "hidream",
    "model_provider_name": "HiDream",
    "description": "HiDream-I1 is an advanced open-source text-to-image model for generating photorealistic images, featuring 17 billion parameters and a Mixture of Experts architecture.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.063
      }
    }
  },
  "replicate/prunaai/hidream-l1-dev:597c67f9baf9bd7f4c363366c1991ff4e126b566437e10c5f5d83e25208be34b": {
    "name": "HiDream-I1 Dev",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "hidream",
    "model_provider_name": "HiDream",
    "description": "HiDream-I1 is an advanced open-source text-to-image model for generating photorealistic images, featuring 17 billion parameters and a Mixture of Experts architecture.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.019
      }
    }
  },
  "replicate/prunaai/hidream-l1-fast:17c237d753218fed0ed477cb553902b6b75735f48c128537ab829096ef3d3645": {
    "name": "HiDream-I1 Fast",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "hidream",
    "model_provider_name": "HiDream",
    "description": "HiDream-I1 is an advanced open-source text-to-image model for generating photorealistic images, featuring 17 billion parameters and a Mixture of Experts architecture.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.014
      }
    }
  },
  "replicate/prunaai/hidream-e1:ea6549775ccda226776338114de4369854113dd9ce2ab1249dc229b90357572e": {
    "name": "HiDream-E1",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "hidream",
    "model_provider_name": "HiDream",
    "description": "HiDream-E1 is an image editing model built on HiDream-I1.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.18
      }
    }
  },
  "replicate/easel/ai-avatars": {
    "name": "Easel Avatar",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "easel",
    "model_provider_name": "Easel",
    "description": "Easel Avatar creates scenes with one or two people using face input images and a text prompt.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.05
      }
    }
  },
  "replicate/easel/advanced-face-swap": {
    "name": "Easel Face Swap",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "easel",
    "model_provider_name": "Easel",
    "description": "Easel Advanced Face Swap allows swapping faces of one or two people at once, while preserving body and scene details.",
    "costs": {
      "image_generation": {
        "flat_rate": 0.05
      }
    }
  },
  "replicate/topazlabs/image-upscale": {
    "name": "Topaz Upscale",
    "mode": "image",
    "service_provider": "replicate",
    "model_provider": "topazlabs",
    "model_provider_name": "Topaz Labs",
    "costs": {
      "image_generation": {
        "quality_tiers": {
          "12mp": { "default": 0.05 },
          "24mp": { "default": 0.05 },
          "36mp": { "default": 0.1 },
          "48mp": { "default": 0.1 },
          "60mp": { "default": 0.15 },
          "96mp": { "default": 0.2 },
          "132mp": { "default": 0.24 },
          "168mp": { "default": 0.29 },
          "336mp": { "default": 0.53 },
          "512mp": { "default": 0.82 }
        }
      }
    },
    "description": "Professional-grade image upscaling, from Topaz Labs, enhances image resolution and quality."
  },
  "xai/grok-2-image": {
    "name": "Grok 2 Image",
    "mode": "image",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "costs": {
      "image_generation": {
        "flat_rate": 0.07
      }
    },
    "description": "Grok-2 Image from xAI transforms text prompts into high-qiality images."
  }
}
