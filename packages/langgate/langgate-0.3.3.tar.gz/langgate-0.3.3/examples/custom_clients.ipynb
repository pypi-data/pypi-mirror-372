{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint as pp\n",
    "\n",
    "os.environ[\"LOG_LEVEL\"] = \"debug\"\n",
    "\n",
    "LANGGATE_URL = \"http://localhost:4000/api/v1\"\n",
    "\n",
    "# If running the langgate server behind Envoy proxy in docker:\n",
    "# LANGGATE_URL = \"http://localhost:10000/api/v1\"\n",
    "\n",
    "# If running and accessing langgate from within a kubernetes cluster:\n",
    "# LANGGATE_URL = \"http://langgate.ns.svc.cluster.local:10000/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Local Registry Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wan to set a custom config file:\n",
    "# os.environ[\"LANGGATE_CONFIG\"] = \"some_custom_path/langgate_config.yaml\"\n",
    "# os.environ[\"LANGGATE_CONFIG\"] = \"some_custom_path/langgate_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Using the SDK's default local registry client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMInfo(id='openai/gpt-5-chat', name='ChatGPT-5', provider_id='openai', description='ChatGPT-5 from OpenAI is designed for advanced, natural, multimodal, and context-aware conversations.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=False, supports_parallel_tool_calls=None, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 52, 770913, tzinfo=datetime.timezone.utc))\n"
     ]
    }
   ],
   "source": [
    "from langgate.sdk import LangGateLocal\n",
    "\n",
    "# `LangGateLocal.registry` is a singleton\n",
    "client = LangGateLocal()\n",
    "models = await client.list_llms()  # returns the default LLMInfo schema list\n",
    "pp(models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Using the standalone default local registry client\n",
    "The registry can be installed without the SDK if you do not need to use any other SDK features such as transforming paramaters.\n",
    "It can be installed separately with:\n",
    "```bash\n",
    "uv add langgate[registry]\n",
    "```\n",
    "or with pip:\n",
    "```bash\n",
    "pip install langgate[registry]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMInfo(id='openai/gpt-5-chat', name='ChatGPT-5', provider_id='openai', description='ChatGPT-5 from OpenAI is designed for advanced, natural, multimodal, and context-aware conversations.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=False, supports_parallel_tool_calls=None, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 52, 770913, tzinfo=datetime.timezone.utc))\n"
     ]
    }
   ],
   "source": [
    "from langgate.registry.local import LocalRegistryClient\n",
    "\n",
    "# The concrete `LocalRegistryClient` class is a singleton\n",
    "client = LocalRegistryClient()\n",
    "models = await client.list_llms()  # returns the default LLMInfo schema list\n",
    "pp(models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Subclassing BaseLocalRegistryClient with custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-18 13:56:10\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mreusing_registry_singleton    \u001b[0m \u001b[36minitialized\u001b[0m=\u001b[35mTrue\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:10\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_local_registry_client\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:10\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:10\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m4\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m5\u001b[0m\n",
      "CustomLLMInfo(id='openai/gpt-5-chat', name='ChatGPT-5', provider_id='openai', description='ChatGPT-5 from OpenAI is designed for advanced, natural, multimodal, and context-aware conversations.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=False, supports_parallel_tool_calls=None, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 52, 770913, tzinfo=datetime.timezone.utc), extra_field='', custom_metadata={})\n",
      "CustomImageModelInfo(id='openai/gpt-image-1', name='GPT Image 1', provider_id='openai', description='GPT-Image-1 is a multimodal model that accepts both text and image inputs, and produces image outputs.', costs=ImageModelCost(token_costs=TokenCosts(input_cost_per_token=Decimal('0.00001'), output_cost_per_token=Decimal('0.00004'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=None, input_cached_cost_per_token=Decimal('0.0000025')), image_generation=ImageGenerationCost(flat_rate=None, quality_tiers={'low': {'1024x1024': Decimal('0.011'), '1024x1536': Decimal('0.016'), '1536x1024': Decimal('0.016')}, 'medium': {'1024x1024': Decimal('0.042'), '1024x1536': Decimal('0.063'), '1536x1024': Decimal('0.063')}, 'high': {'1024x1024': Decimal('0.167'), '1024x1536': Decimal('0.25'), '1536x1024': Decimal('0.25')}}, cost_per_megapixel=None, cost_per_second=None)), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 52, 771006, tzinfo=datetime.timezone.utc), extra_field='', custom_metadata={})\n"
     ]
    }
   ],
   "source": [
    "from langgate.registry.local import BaseLocalRegistryClient\n",
    "from langgate.core.models import LLMInfo, ImageModelInfo\n",
    "\n",
    "\n",
    "class CustomLLMInfo(LLMInfo):\n",
    "    extra_field: str = \"\"\n",
    "    custom_metadata: dict = {}\n",
    "\n",
    "\n",
    "class CustomImageModelInfo(ImageModelInfo):\n",
    "    extra_field: str = \"\"\n",
    "    custom_metadata: dict = {}\n",
    "\n",
    "\n",
    "class CustomLocalRegistryClient(\n",
    "    BaseLocalRegistryClient[CustomLLMInfo, CustomImageModelInfo]\n",
    "):\n",
    "    \"\"\"Custom Local registry client with both CustomLLMInfo and CustomImageModelInfo schemas.\n",
    "\n",
    "    Note: If you only want to extend one schema, pass the default class for the other type parameter.\n",
    "    For example: BaseLocalRegistryClient[CustomLLMInfo, ImageModelInfo]\n",
    "    \"\"\"\n",
    "\n",
    "    # This is not a singleton unless you implement it as such.\n",
    "\n",
    "\n",
    "custom_client = CustomLocalRegistryClient()\n",
    "custom_llms = (\n",
    "    await custom_client.list_llms()\n",
    ")  # Typed and validated as list[CustomLLMInfo]\n",
    "pp(custom_llms[0])\n",
    "\n",
    "custom_image_models = (\n",
    "    await custom_client.list_image_models()\n",
    ")  # Typed and validated as list[CustomImageModelInfo]\n",
    "pp(custom_image_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Using the base class directly with type parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-18 13:56:17\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mreusing_registry_singleton    \u001b[0m \u001b[36minitialized\u001b[0m=\u001b[35mTrue\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:17\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_local_registry_client\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:17\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:17\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m4\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m5\u001b[0m\n",
      "CustomLLMInfo(id='openai/gpt-5-chat', name='ChatGPT-5', provider_id='openai', description='ChatGPT-5 from OpenAI is designed for advanced, natural, multimodal, and context-aware conversations.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=False, supports_parallel_tool_calls=None, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 52, 770913, tzinfo=datetime.timezone.utc), extra_field='', custom_metadata={})\n",
      "CustomImageModelInfo(id='openai/gpt-image-1', name='GPT Image 1', provider_id='openai', description='GPT-Image-1 is a multimodal model that accepts both text and image inputs, and produces image outputs.', costs=ImageModelCost(token_costs=TokenCosts(input_cost_per_token=Decimal('0.00001'), output_cost_per_token=Decimal('0.00004'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=None, input_cached_cost_per_token=Decimal('0.0000025')), image_generation=ImageGenerationCost(flat_rate=None, quality_tiers={'low': {'1024x1024': Decimal('0.011'), '1024x1536': Decimal('0.016'), '1536x1024': Decimal('0.016')}, 'medium': {'1024x1024': Decimal('0.042'), '1024x1536': Decimal('0.063'), '1536x1024': Decimal('0.063')}, 'high': {'1024x1024': Decimal('0.167'), '1024x1536': Decimal('0.25'), '1536x1024': Decimal('0.25')}}, cost_per_megapixel=None, cost_per_second=None)), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 52, 771006, tzinfo=datetime.timezone.utc), extra_field='', custom_metadata={})\n"
     ]
    }
   ],
   "source": [
    "# This is not a singleton.\n",
    "local_client_with_custom_schema = BaseLocalRegistryClient[\n",
    "    CustomLLMInfo, CustomImageModelInfo\n",
    "](llm_info_cls=CustomLLMInfo, image_info_cls=CustomImageModelInfo)\n",
    "direct_llms = (\n",
    "    await local_client_with_custom_schema.list_llms()\n",
    ")  # Typed and validated as list[CustomLLMInfo]\n",
    "\n",
    "pp(direct_llms[0])\n",
    "\n",
    "direct_image_models = (\n",
    "    await local_client_with_custom_schema.list_image_models()\n",
    ")  # Typed and validated as list[CustomImageModelInfo]\n",
    "pp(direct_image_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom HTTP Registry Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import SecretStr\n",
    "\n",
    "# Optional: Set API key (if your LangGate service requires authentication)\n",
    "api_key = SecretStr(\"your_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Using the default http client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-18 13:56:20\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mcreating_http_registry_client_singleton\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:20\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_http_registry_client\u001b[0m \u001b[36mapi_key\u001b[0m=\u001b[35mSecretStr('**********')\u001b[0m \u001b[36mbase_url\u001b[0m=\u001b[35mhttp://localhost:4000/api/v1\u001b[0m \u001b[36mimage_info_cls\u001b[0m=\u001b[35m<class 'langgate.core.models.ImageModelInfo'>\u001b[0m \u001b[36mllm_info_cls\u001b[0m=\u001b[35m<class 'langgate.core.models.LLMInfo'>\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:20\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_http_registry_client_singleton\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:20\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:20\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m33\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m65\u001b[0m\n",
      "LLMInfo(id='openai/gpt-5', name='GPT-5', provider_id='openai', description='GPT-5 is OpenAI\\'s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=True, supports_parallel_tool_calls=True, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_tool_choice=True, supports_structured_outputs=True, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 35, 221259, tzinfo=TzInfo(UTC)))\n"
     ]
    }
   ],
   "source": [
    "from langgate.client.http import HTTPRegistryClient\n",
    "\n",
    "# The concrete `HTTPRegistryClient` class is a singleton\n",
    "client = HTTPRegistryClient(base_url=LANGGATE_URL, api_key=api_key)\n",
    "llms = await client.list_llms()  # returns the default LLMInfo schema list\n",
    "pp(llms[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Subclassing BaseHTTPRegistryClient with custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-18 13:56:25\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_http_registry_client\u001b[0m \u001b[36mapi_key\u001b[0m=\u001b[35mSecretStr('**********')\u001b[0m \u001b[36mbase_url\u001b[0m=\u001b[35mhttp://localhost:4000/api/v1\u001b[0m \u001b[36mimage_info_cls\u001b[0m=\u001b[35m<class '__main__.CustomImageModelInfo'>\u001b[0m \u001b[36mllm_info_cls\u001b[0m=\u001b[35m<class '__main__.CustomLLMInfo'>\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:25\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:25\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m33\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m65\u001b[0m\n",
      "CustomLLMInfo(id='openai/gpt-5', name='GPT-5', provider_id='openai', description='GPT-5 is OpenAI\\'s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=True, supports_parallel_tool_calls=True, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_tool_choice=True, supports_structured_outputs=True, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 35, 221259, tzinfo=TzInfo(UTC)), extra_field='', custom_metadata={})\n",
      "CustomImageModelInfo(id='openai/gpt-image-1', name='GPT Image 1', provider_id='openai', description='GPT-Image-1 is a multimodal model that accepts both text and image inputs, and produces image outputs.', costs=ImageModelCost(token_costs=TokenCosts(input_cost_per_token=Decimal('0.00001'), output_cost_per_token=Decimal('0.00004'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=None, input_cached_cost_per_token=Decimal('0.0000025')), image_generation=ImageGenerationCost(flat_rate=None, quality_tiers={'low': {'1024x1024': Decimal('0.011'), '1024x1536': Decimal('0.016'), '1536x1024': Decimal('0.016')}, 'medium': {'1024x1024': Decimal('0.042'), '1024x1536': Decimal('0.063'), '1536x1024': Decimal('0.063')}, 'high': {'1024x1024': Decimal('0.167'), '1024x1536': Decimal('0.25'), '1536x1024': Decimal('0.25')}}, cost_per_megapixel=None, cost_per_second=None)), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 35, 222066, tzinfo=TzInfo(UTC)), extra_field='', custom_metadata={})\n"
     ]
    }
   ],
   "source": [
    "from langgate.client.http import BaseHTTPRegistryClient\n",
    "from langgate.core.models import LLMInfo, ImageModelInfo\n",
    "\n",
    "\n",
    "class CustomLLMInfo(LLMInfo):\n",
    "    extra_field: str = \"\"\n",
    "    custom_metadata: dict = {}\n",
    "\n",
    "\n",
    "class CustomImageModelInfo(ImageModelInfo):\n",
    "    extra_field: str = \"\"\n",
    "    custom_metadata: dict = {}\n",
    "\n",
    "\n",
    "class CustomHTTPClient(BaseHTTPRegistryClient[CustomLLMInfo, CustomImageModelInfo]):\n",
    "    \"\"\"Custom HTTP client with both CustomLLMInfo and CustomImageModelInfo schemas.\n",
    "\n",
    "    Note: If you only want to extend one schema, pass the default class for the other type parameter.\n",
    "    For example: BaseHTTPRegistryClient[CustomLLMInfo, ImageModelInfo]\n",
    "    \"\"\"\n",
    "\n",
    "    # This is not a singleton unless you implement it as such.\n",
    "\n",
    "\n",
    "custom_client = CustomHTTPClient(LANGGATE_URL, api_key=api_key)\n",
    "custom_models = (\n",
    "    await custom_client.list_llms()\n",
    ")  # Typed and validated as list[CustomLLMInfo]\n",
    "pp(custom_models[0])\n",
    "\n",
    "custom_image_models = (\n",
    "    await custom_client.list_image_models()\n",
    ")  # Typed and validated as list[CustomImageModelInfo]\n",
    "pp(custom_image_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Using the base class directly with type parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-18 13:56:27\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_http_registry_client\u001b[0m \u001b[36mapi_key\u001b[0m=\u001b[35mSecretStr('**********')\u001b[0m \u001b[36mbase_url\u001b[0m=\u001b[35mhttp://localhost:4000/api/v1\u001b[0m \u001b[36mimage_info_cls\u001b[0m=\u001b[35m<class '__main__.CustomImageModelInfo'>\u001b[0m \u001b[36mllm_info_cls\u001b[0m=\u001b[35m<class '__main__.CustomLLMInfo'>\u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:27\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-08-18 13:56:27\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m33\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m65\u001b[0m\n",
      "CustomLLMInfo(id='openai/gpt-5', name='GPT-5', provider_id='openai', description='GPT-5 is OpenAI\\'s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.', costs=TokenCosts(input_cost_per_token=Decimal('0.00000125'), output_cost_per_token=Decimal('0.00001'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=Decimal('1.25E-7'), input_cached_cost_per_token=None), capabilities=ModelCapabilities(supports_tools=True, supports_parallel_tool_calls=True, supports_vision=True, supports_audio_input=None, supports_audio_output=None, supports_prompt_caching=True, supports_reasoning=True, supports_response_schema=True, supports_system_messages=None, supports_tool_choice=True, supports_structured_outputs=True, supports_max_tokens=True, supports_files=True, supports_seed=True), context_window=ContextWindow(max_input_tokens=400000, max_output_tokens=128000), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 35, 221259, tzinfo=TzInfo(UTC)), extra_field='', custom_metadata={})\n",
      "CustomImageModelInfo(id='openai/gpt-image-1', name='GPT Image 1', provider_id='openai', description='GPT-Image-1 is a multimodal model that accepts both text and image inputs, and produces image outputs.', costs=ImageModelCost(token_costs=TokenCosts(input_cost_per_token=Decimal('0.00001'), output_cost_per_token=Decimal('0.00004'), input_cost_per_token_batches=None, output_cost_per_token_batches=None, cache_read_input_token_cost=None, input_cached_cost_per_token=Decimal('0.0000025')), image_generation=ImageGenerationCost(flat_rate=None, quality_tiers={'low': {'1024x1024': Decimal('0.011'), '1024x1536': Decimal('0.016'), '1536x1024': Decimal('0.016')}, 'medium': {'1024x1024': Decimal('0.042'), '1024x1536': Decimal('0.063'), '1536x1024': Decimal('0.063')}, 'high': {'1024x1024': Decimal('0.167'), '1024x1536': Decimal('0.25'), '1536x1024': Decimal('0.25')}}, cost_per_megapixel=None, cost_per_second=None)), provider=ModelProvider(id='openai', name='OpenAI', description=None), updated_dt=datetime.datetime(2025, 8, 18, 12, 55, 35, 222066, tzinfo=TzInfo(UTC)), extra_field='', custom_metadata={})\n"
     ]
    }
   ],
   "source": [
    "# This is not a singleton.\n",
    "client_with_custom_schema = BaseHTTPRegistryClient[CustomLLMInfo, CustomImageModelInfo](\n",
    "    base_url=LANGGATE_URL,\n",
    "    api_key=api_key,\n",
    "    llm_info_cls=CustomLLMInfo,\n",
    "    image_info_cls=CustomImageModelInfo,\n",
    ")\n",
    "direct_llms = (\n",
    "    await client_with_custom_schema.list_llms()\n",
    ")  # Typed and validated as list[CustomLLMInfo]\n",
    "\n",
    "pp(direct_llms[0])\n",
    "\n",
    "direct_image_models = (\n",
    "    await client_with_custom_schema.list_image_models()\n",
    ")  # Typed and validated as list[CustomImageModelInfo]\n",
    "pp(direct_image_models[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
