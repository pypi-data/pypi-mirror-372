{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Integration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint as pp\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "# The below environment variables are optional.\n",
    "\n",
    "# The yaml config resolution priority is: args > env > cwd > package default.\n",
    "# If you don't want to use either the package default (langgate/core/data/default_config.yaml)\n",
    "# or a config in your cwd, set:\n",
    "# os.environ[\"LANGGATE_CONFIG\"] = \"some_other_path_not_in_your_cwd/langgate_config.yaml\"\n",
    "\n",
    "# The models data resolution priority is: args > env > cwd > package default\n",
    "# If you don't want to use either the package default (langgate/registry/data/default_models.json)\n",
    "# or a models data file in your cwd, set:\n",
    "# os.environ[\"LANGGATE_MODELS\"] = \"some_other_path_not_in_your_cwd/langgate_models.json\"\n",
    "\n",
    "# The .env file resolution priority is: args > env > cwd > None\n",
    "# If you don't want to use either the package default or a .env file in your cwd, set:\n",
    "# os.environ[\"LANGGATE_ENV_FILE\"] = \"some_other_path_not_in_your_cwd/.env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic SDK Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capabilities': {'supports_assistant_prefill': True,\n",
      "                  'supports_prompt_caching': True,\n",
      "                  'supports_reasoning': True,\n",
      "                  'supports_response_schema': True,\n",
      "                  'supports_tool_choice': True,\n",
      "                  'supports_tools': True,\n",
      "                  'supports_vision': True},\n",
      " 'context_window': {'max_input_tokens': 200000, 'max_output_tokens': 64000},\n",
      " 'costs': {'cache_creation_input_token_cost': '0.00000375',\n",
      "           'cache_read_input_token_cost': Decimal('3E-7'),\n",
      "           'input_cost_per_image': '0.0048',\n",
      "           'input_cost_per_token': Decimal('0.000003'),\n",
      "           'output_cost_per_token': Decimal('0.000015')},\n",
      " 'description': 'Claude-4 Sonnet with reasoning capabilities.',\n",
      " 'id': 'anthropic/claude-sonnet-4-reasoning',\n",
      " 'name': 'Claude-4 Sonnet R',\n",
      " 'provider': {'id': 'anthropic', 'name': 'Anthropic'},\n",
      " 'provider_id': 'anthropic',\n",
      " 'updated_dt': datetime.datetime(2025, 8, 18, 12, 58, 28, 591935, tzinfo=datetime.timezone.utc)}\n"
     ]
    }
   ],
   "source": [
    "from langgate.sdk import LangGateLocal\n",
    "\n",
    "client = LangGateLocal()\n",
    "\n",
    "\n",
    "# LangGate allows us to register \"virtual models\" - models with specific parameters.\n",
    "# `langgate_config.yaml` defines this `claude-sonnet-4-reasoning` model\n",
    "# which is a wrapper around the `claude-sonnet-4-0` model,\n",
    "# with specific parameters and metadata.\n",
    "# In `langgate_config.yaml`, Anthropic is set as the inference service provider,\n",
    "# but you could configure any backend API that offers the model, e.g. AWS Bedrock.\n",
    "model_id = \"anthropic/claude-sonnet-4-reasoning\"\n",
    "\n",
    "# get metadata for a model\n",
    "model_info = await client.get_llm_info(model_id)\n",
    "\n",
    "# returns a Pydantic model instance (langgate.core.models.LLMInfo)\n",
    "pp(model_info.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('anthropic',\n",
      " {'api_key': SecretStr('**********'),\n",
      "  'base_url': 'https://api.anthropic.com',\n",
      "  'model': 'claude-sonnet-4-0',\n",
      "  'stream': True,\n",
      "  'thinking': {'budget_tokens': 1024, 'type': 'enabled'}})\n"
     ]
    }
   ],
   "source": [
    "model_params = await client.get_params(model_id, {\"temperature\": 0.7, \"stream\": True})\n",
    "pp(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --resolution highest -U langchain langchain-openai langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Factory Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgate.sdk import LangGateLocal, LangGateLocalProtocol\n",
    "from langgate.core.models import (\n",
    "    # `ModelProviderId` is a string alias for better type safety\n",
    "    ModelProviderId,\n",
    "    # ids for common providers are included for convenience\n",
    "    MODEL_PROVIDER_OPENAI,\n",
    "    MODEL_PROVIDER_ANTHROPIC,\n",
    ")\n",
    "\n",
    "# Map providers to model classes\n",
    "MODEL_CLASS_MAP: dict[ModelProviderId, type[BaseChatModel]] = {\n",
    "    MODEL_PROVIDER_OPENAI: ChatOpenAI,\n",
    "    MODEL_PROVIDER_ANTHROPIC: ChatAnthropic,\n",
    "}\n",
    "\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    Factory for creating a Langchain `BaseChatModel` instance\n",
    "    with paramaters from LangGate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, langgate_client: LangGateLocalProtocol | None = None):\n",
    "        self.langgate_client = langgate_client or LangGateLocal()\n",
    "\n",
    "    async def create_model(\n",
    "        self, model_id: str, input_params: dict[str, Any] | None = None\n",
    "    ) -> tuple[BaseChatModel, dict[str, Any]]:\n",
    "        \"\"\"Create a model instance for the given model ID.\"\"\"\n",
    "        params = {\"temperature\": 0.7, \"streaming\": True}\n",
    "        if input_params:\n",
    "            params.update(input_params)\n",
    "\n",
    "        # Get model info from the registry cache\n",
    "        model_info = await self.langgate_client.get_llm_info(model_id)\n",
    "\n",
    "        # Transform parameters using the transformer client\n",
    "        # If switching to using the proxy, you would remove this line\n",
    "        # and let the proxy handle the parameter transformation instead.\n",
    "        api_format, model_params = await self.langgate_client.get_params(\n",
    "            model_id, params\n",
    "        )\n",
    "        # api_format defaults to the provider id unless specified in the config.\n",
    "        # e.g. Specify \"openai\" for OpenAI-compatible APIs, etc.\n",
    "        print(\"API format:\", api_format)\n",
    "        pp(model_params)\n",
    "\n",
    "        # Get the appropriate model class based on provider\n",
    "        client_cls_key = ModelProviderId(api_format)\n",
    "        model_class = MODEL_CLASS_MAP.get(client_cls_key)\n",
    "        if not model_class:\n",
    "            raise ValueError(f\"No model class for provider {model_info.provider.id}\")\n",
    "\n",
    "        # Create model instance with parameters\n",
    "        model = model_class(**model_params)\n",
    "\n",
    "        # Create model info dict\n",
    "        model_metadata = model_info.model_dump(exclude_none=True)\n",
    "\n",
    "        return model, model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API format: openai\n",
      "{'api_key': SecretStr('**********'),\n",
      " 'base_url': 'https://api.openai.com/v1',\n",
      " 'include_reasoning': True,\n",
      " 'model': 'gpt-5',\n",
      " 'streaming': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saran/langgate/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3547: UserWarning: WARNING! include_reasoning is not default parameter.\n",
      "                include_reasoning was transferred to model_kwargs.\n",
      "                Please confirm that include_reasoning is what you intended.\n",
      "  await eval(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10cbacec0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10cbad940>, root_client=<openai.OpenAI object at 0x10c40e270>, root_async_client=<openai.AsyncOpenAI object at 0x10cbad6a0>, model_name='gpt-5', model_kwargs={'include_reasoning': True}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', streaming=True),\n",
       " {'id': 'openai/gpt-5',\n",
       "  'name': 'GPT-5',\n",
       "  'provider_id': 'openai',\n",
       "  'description': 'GPT-5 is OpenAI\\'s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.',\n",
       "  'costs': {'input_cost_per_token': Decimal('0.00000125'),\n",
       "   'output_cost_per_token': Decimal('0.00001'),\n",
       "   'cache_read_input_token_cost': Decimal('1.25E-7')},\n",
       "  'capabilities': {'supports_tools': True,\n",
       "   'supports_parallel_tool_calls': True,\n",
       "   'supports_vision': True,\n",
       "   'supports_prompt_caching': True,\n",
       "   'supports_reasoning': True,\n",
       "   'supports_response_schema': True,\n",
       "   'supports_tool_choice': True,\n",
       "   'supports_structured_outputs': True,\n",
       "   'supports_max_tokens': True,\n",
       "   'supports_files': True,\n",
       "   'supports_seed': True},\n",
       "  'context_window': {'max_input_tokens': 400000, 'max_output_tokens': 128000},\n",
       "  'provider': {'id': 'openai', 'name': 'OpenAI'},\n",
       "  'updated_dt': datetime.datetime(2025, 8, 18, 12, 58, 28, 591896, tzinfo=datetime.timezone.utc)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_factory = ModelFactory()\n",
    "model_id = \"openai/gpt-5\"\n",
    "model, model_info = await model_factory.create_model(model_id, {\"temperature\": 0.7})\n",
    "model, model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Factory Pattern (Decoupled)\n",
    "- The `LangGateLocal` client is composed of the `LocalRegistryClient` and `LocalTransformerClient`.\n",
    "- This example is the same as the above, but with each adapter passed in separately to the factory.\n",
    " \n",
    "\n",
    "\n",
    "For example, you may want to sync model data from the registry to a database, along with additional fields.\n",
    "You could create a `PgModelRegistry` that implements `RegistryClientProtocol`. This would ensure consistent typing throughout your codebase, and you could apply any app-specific validation rules or transformations to the model data before it is sent to the database. For example, limiting the synced models to a subset of the registry models, or transforming the model data to match the database schema. This would allow you to use the same `LangGateLocal` client, while still being able to customize the behavior of each adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langgate.registry import RegistryClientProtocol, LocalRegistryClient\n",
    "from langgate.transform import LocalTransformerClient, TransformerClientProtocol\n",
    "\n",
    "\n",
    "class ModelFactoryDecoupled:\n",
    "    \"\"\"\n",
    "    Factory for creating a Langchain `BaseChatModel` instance\n",
    "    with paramaters from LangGate. Adapters for the registry and transformer\n",
    "    clients are injected separately, allowing for more flexibility.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_APP_PARAMS: dict[str, Any] = {\n",
    "        \"streaming\": True,\n",
    "        \"verbose\": True,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_registry: RegistryClientProtocol | None = None,\n",
    "        param_transformer: TransformerClientProtocol | None = None,\n",
    "    ):\n",
    "        self.model_registry = model_registry or LocalRegistryClient()\n",
    "        self.param_transformer = param_transformer or LocalTransformerClient()\n",
    "\n",
    "    async def create_model(\n",
    "        self, model_id: str, input_params: dict[str, Any] | None = None\n",
    "    ) -> tuple[BaseChatModel, dict[str, Any]]:\n",
    "        \"\"\"Create a model instance for the given model ID.\"\"\"\n",
    "        params = self.DEFAULT_APP_PARAMS.copy()\n",
    "        if input_params:\n",
    "            params.update(input_params)\n",
    "\n",
    "        # Get model info from the registry cache\n",
    "        model_info = await self.model_registry.get_llm_info(model_id)\n",
    "\n",
    "        # Transform parameters using the transformer client\n",
    "        # If switching to using the proxy, you would remove this line\n",
    "        # and let the proxy handle the parameter transformation instead.\n",
    "        _, model_params = await self.param_transformer.get_params(model_id, params)\n",
    "        pp(model_params)\n",
    "\n",
    "        # Create the appropriate model instance based on provider\n",
    "        model = init_chat_model(\n",
    "            model=model_params[\"model\"],\n",
    "            model_provider=model_info.provider.id,\n",
    "            kwargs=model_params,\n",
    "        )\n",
    "        # Note the use of `langchain.chat_models.init_chat_model` to create the model instance here\n",
    "        # is more concise and idiomatic but less flexible than the previous explicit mapping of providers to classes.\n",
    "        # For example, many inference services use the OpenAI API format. `init_chat_model` would\n",
    "        # not automatically map unknown providers to `ChatOpenAI`. It would raise an error.\n",
    "        # If you want to use `init_chat_model` with unknown providers, we recommend adding a mapping\n",
    "        # for the model in `langgate_config.yaml, such as `langchain_class: \"ChatOpenAI\"`.\n",
    "        # TODO: We may add this as a built-in feature in future.\n",
    "        if not model:\n",
    "            raise ValueError(f\"No model class for provider {model_info.provider.id}\")\n",
    "\n",
    "        # Create model info dict\n",
    "        model_metadata = model_info.model_dump(exclude_none=True)\n",
    "\n",
    "        return model, model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'api_key': SecretStr('**********'),\n",
      " 'base_url': 'https://api.openai.com/v1',\n",
      " 'include_reasoning': True,\n",
      " 'model': 'gpt-5',\n",
      " 'streaming': True,\n",
      " 'verbose': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/d_sz9h1577l6qc91t18wm3yh0000gn/T/ipykernel_17963/2849443794.py:50: UserWarning: WARNING! kwargs is not default parameter.\n",
      "                kwargs was transferred to model_kwargs.\n",
      "                Please confirm that kwargs is what you intended.\n",
      "  model = init_chat_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10cd50050>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10cd50a50>, root_client=<openai.OpenAI object at 0x10c7b3d90>, root_async_client=<openai.AsyncOpenAI object at 0x10cd507d0>, model_name='gpt-5', model_kwargs={'kwargs': {'streaming': True, 'verbose': True, 'api_key': SecretStr('**********'), 'base_url': 'https://api.openai.com/v1', 'include_reasoning': True, 'model': 'gpt-5'}}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_factory = ModelFactoryDecoupled()\n",
    "model_id = \"openai/gpt-5\"\n",
    "model, model_info = await model_factory.create_model(model_id, {\"temperature\": 0.7})\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
