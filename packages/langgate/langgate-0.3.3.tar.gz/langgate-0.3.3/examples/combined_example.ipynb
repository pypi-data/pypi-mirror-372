{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Client Example\n",
    "\n",
    "This notebook demonstrates how to use the combined `LangGateLocal` client, which provides access to both registry and parameter transformation functionality in a single client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Optional environment variables\n",
    "# os.environ[\"LANGGATE_CONFIG\"] = \"path/to/langgate_config.yaml\"\n",
    "# os.environ[\"LANGGATE_MODELS\"] = \"path/to/langgate_models.json\"\n",
    "# os.environ[\"LOG_LEVEL\"] = \"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Combined Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgate.sdk import LangGateLocal\n",
    "\n",
    "# Initialize the combined client\n",
    "# LangGateLocal is a singleton that combines both registry and transformer functionality\n",
    "client = LangGateLocal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-18 02:23:46\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-08-18 02:23:46\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m4\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m5\u001b[0m\n",
      "Available LLMs: 5\n",
      "- openai/gpt-5-chat: ChatGPT-5\n",
      "- openai/gpt-5: GPT-5\n",
      "- openai/gpt-5-high: GPT-5 high\n",
      "- anthropic/claude-sonnet-4: Claude-4 Sonnet\n",
      "- anthropic/claude-sonnet-4-reasoning: Claude-4 Sonnet R\n",
      "\n",
      "==================================================\n",
      "\n",
      "Available Image Models: 4\n",
      "- openai/gpt-image-1: GPT Image 1\n",
      "- openai/dall-e-3: DALL-E 3\n",
      "- black-forest-labs/flux-dev: FLUX.1 [dev]\n",
      "- stability-ai/sd-3.5-large: SD 3.5 Large\n"
     ]
    }
   ],
   "source": [
    "# List available LLMs\n",
    "llms = await client.list_llms()\n",
    "print(f\"Available LLMs: {len(llms)}\")\n",
    "for model in llms[:5]:\n",
    "    print(f\"- {model.id}: {model.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# List available image models\n",
    "image_models = await client.list_image_models()\n",
    "print(f\"Available Image Models: {len(image_models)}\")\n",
    "for model in image_models[:5]:\n",
    "    print(f\"- {model.id}: {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample user parameters\n",
    "input_params = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 20_000,\n",
    "    \"stream\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info and Transform Parameters\n",
    "\n",
    "Using the combined client, we can get model information and transform parameters from a unified interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ChatGPT-5\n",
      "Provider: OpenAI\n",
      "Description: ChatGPT-5 from OpenAI is designed for advanced, natural, multimodal, and context-aware conversations.\n",
      "\n",
      "Transformed parameters:\n",
      "('openai',\n",
      " {'api_key': SecretStr('**********'),\n",
      "  'base_url': 'https://api.openai.com/v1',\n",
      "  'max_tokens': 20000,\n",
      "  'model': 'gpt-5-chat-latest',\n",
      "  'stream': True})\n"
     ]
    }
   ],
   "source": [
    "if llms:\n",
    "    model_id = llms[0].id\n",
    "\n",
    "    # Get model info\n",
    "    model_info = await client.get_llm_info(model_id)\n",
    "    print(f\"Model: {model_info.name}\")\n",
    "    print(f\"Provider: {model_info.provider.name}\")\n",
    "    print(f\"Description: {model_info.description}\")\n",
    "\n",
    "    # Transform parameters\n",
    "    transformed = await client.get_params(model_id, input_params)\n",
    "    print(\"\\nTransformed parameters:\")\n",
    "    pp(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Model-Specific Configurations\n",
    "\n",
    "Different models may have different parameter transformations defined in `langgate_config.yaml`.\n",
    "You can define \"virtual models\" with IDs referring to a model + a configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Claude-4 Sonnet R\n",
      "Provider: Anthropic\n",
      "Description: Claude-4 Sonnet with reasoning capabilities.\n",
      "\n",
      "Transformed parameters:\n",
      "('anthropic',\n",
      " {'api_key': SecretStr('**********'),\n",
      "  'base_url': 'https://api.anthropic.com',\n",
      "  'max_tokens': 20000,\n",
      "  'model': 'claude-sonnet-4-0',\n",
      "  'stream': True,\n",
      "  'thinking': {'budget_tokens': 1024, 'type': 'enabled'}})\n"
     ]
    }
   ],
   "source": [
    "# Using a model with a specific configuration.\n",
    "# Anthropic has no actual model with this ID. We define it ourselves,\n",
    "# to reference claude-sonnet-4-0 with a specific reasoning configuration.\n",
    "model_id = \"anthropic/claude-sonnet-4-reasoning\"\n",
    "\n",
    "# Get model info\n",
    "model_info = await client.get_llm_info(model_id)\n",
    "print(f\"Model: {model_info.name}\")\n",
    "print(f\"Provider: {model_info.provider.name}\")\n",
    "print(f\"Description: {model_info.description}\")\n",
    "\n",
    "# Transform parameters\n",
    "transformed = await client.get_params(model_id, input_params)\n",
    "print(\"\\nTransformed parameters:\")\n",
    "pp(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Model Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capabilities for ChatGPT-5:\n",
      "{'supports_files': True,\n",
      " 'supports_max_tokens': True,\n",
      " 'supports_prompt_caching': True,\n",
      " 'supports_reasoning': True,\n",
      " 'supports_response_schema': True,\n",
      " 'supports_seed': True,\n",
      " 'supports_tools': False,\n",
      " 'supports_vision': True}\n",
      "\n",
      "Transformed parameters with tools:\n",
      "('openai',\n",
      " {'api_key': SecretStr('**********'),\n",
      "  'base_url': 'https://api.openai.com/v1',\n",
      "  'max_tokens': 20000,\n",
      "  'model': 'gpt-5-chat-latest',\n",
      "  'stream': True})\n"
     ]
    }
   ],
   "source": [
    "# Check model capabilities\n",
    "if llms:\n",
    "    model_id = llms[0].id\n",
    "    model_info = await client.get_llm_info(model_id)\n",
    "\n",
    "    print(f\"Capabilities for {model_info.name}:\")\n",
    "    pp(model_info.capabilities.model_dump(exclude_none=True))\n",
    "\n",
    "    # Adjust parameters based on capabilities\n",
    "    custom_params = input_params.copy()\n",
    "\n",
    "    # Example: Add tools if supported\n",
    "    if model_info.capabilities.supports_tools:\n",
    "        custom_params[\"tools\"] = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_weather\",\n",
    "                    \"description\": \"Get the current weather in a location\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and country, e.g., Cork, Ireland\",\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"location\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    # Transform the custom parameters\n",
    "    transformed = await client.get_params(model_id, custom_params)\n",
    "    print(\"\\nTransformed parameters with tools:\")\n",
    "    pp(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Image Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image model parameters\n",
    "image_params = {\n",
    "    \"prompt\": \"A beautiful sunset over the ocean\",\n",
    "    \"size\": \"1024x1024\",\n",
    "    \"quality\": \"medium\",\n",
    "    \"n\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Model: GPT Image 1\n",
      "Provider: OpenAI\n",
      "Description: GPT-Image-1 is a multimodal model that accepts both text and image inputs, and produces image outputs.\n",
      "\n",
      "Transformed image generation parameters:\n",
      "('openai',\n",
      " {'api_key': SecretStr('**********'),\n",
      "  'base_url': 'https://api.openai.com/v1',\n",
      "  'model': 'gpt-image-1',\n",
      "  'n': 1,\n",
      "  'prompt': 'A beautiful sunset over the ocean',\n",
      "  'quality': 'medium',\n",
      "  'size': '1024x1024'})\n"
     ]
    }
   ],
   "source": [
    "# Transform parameters for an image model - the same pattern as for LLMs\n",
    "if image_models:\n",
    "    image_model_id = image_models[0].id\n",
    "\n",
    "    # Get image model info\n",
    "    image_info = await client.get_image_model_info(image_model_id)\n",
    "    print(f\"Image Model: {image_info.name}\")\n",
    "    print(f\"Provider: {image_info.provider.name}\")\n",
    "    print(f\"Description: {image_info.description}\")\n",
    "\n",
    "    # Transform parameters\n",
    "    transformed = await client.get_params(image_model_id, image_params)\n",
    "    print(\"\\nTransformed image generation parameters:\")\n",
    "    pp(transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
