Metadata-Version: 2.1
Name: brevitas
Version: 0.12.1
Summary: Quantization-aware training in PyTorch
Home-page: https://github.com/Xilinx/brevitas
Author: AMD Research and Advanced Development
Author-email: brevitas-external@amd.com
Requires-Python: >=3.9, <3.13
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: dependencies==2.0.1
Requires-Dist: numpy<=1.26.4
Requires-Dist: packaging
Requires-Dist: setuptools<70.0
Requires-Dist: sympy
Requires-Dist: torch>=1.12
Requires-Dist: typing-extensions>=3.7.4
Requires-Dist: unfoldNd
Provides-Extra: notebook
Requires-Dist: jupyter; extra == "notebook"
Requires-Dist: nbmake; extra == "notebook"
Provides-Extra: dev
Requires-Dist: pre-commit; extra == "dev"
Provides-Extra: docs
Requires-Dist: m2r2==0.3.3.post2; extra == "docs"
Requires-Dist: nbsphinx==0.9.3; extra == "docs"
Requires-Dist: nbsphinx-link==1.3.0; extra == "docs"
Requires-Dist: pydata-sphinx-theme==0.15.3; extra == "docs"
Requires-Dist: sphinx==5.3.0; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints==1.21.8; extra == "docs"
Requires-Dist: sphinx-gallery==0.10.1; extra == "docs"
Requires-Dist: sphinxcontrib-napoleon==0.7; extra == "docs"
Requires-Dist: sphinxemoji==0.2.0; extra == "docs"
Requires-Dist: lxml==4.9.3; extra == "docs"
Provides-Extra: export
Requires-Dist: onnx==1.17.0; extra == "export"
Requires-Dist: onnxoptimizer; extra == "export"
Requires-Dist: onnxscript; extra == "export"
Requires-Dist: qonnx; extra == "export"
Provides-Extra: hadamard
Requires-Dist: scipy; extra == "hadamard"
Provides-Extra: test
Requires-Dist: hypothesis; extra == "test"
Requires-Dist: mock; extra == "test"
Requires-Dist: psutil; extra == "test"
Requires-Dist: pytest<8.4; extra == "test"
Requires-Dist: pytest-mock; extra == "test"
Requires-Dist: pytest-xdist; extra == "test"
Requires-Dist: pytest_cases; extra == "test"
Requires-Dist: scipy; extra == "test"
Requires-Dist: torchvision; extra == "test"
Requires-Dist: tqdm; extra == "test"
Provides-Extra: tts
Requires-Dist: librosa; extra == "tts"
Requires-Dist: numpy; extra == "tts"
Requires-Dist: pillow; extra == "tts"
Requires-Dist: pyyaml; extra == "tts"
Requires-Dist: scipy; extra == "tts"
Requires-Dist: soundfile; extra == "tts"
Requires-Dist: tqdm; extra == "tts"
Provides-Extra: stt
Requires-Dist: inflect==5.6.2; extra == "stt"
Requires-Dist: librosa; extra == "stt"
Requires-Dist: numba; extra == "stt"
Requires-Dist: pillow>=4.3.0; extra == "stt"
Requires-Dist: requests; extra == "stt"
Requires-Dist: ruamel.yaml; extra == "stt"
Requires-Dist: soundfile; extra == "stt"
Requires-Dist: sox; extra == "stt"
Requires-Dist: torch-stft; extra == "stt"
Requires-Dist: tqdm; extra == "stt"
Requires-Dist: unidecode; extra == "stt"
Provides-Extra: llm
Requires-Dist: accelerate; extra == "llm"
Requires-Dist: datasets; extra == "llm"
Requires-Dist: gguf==0.17.0; extra == "llm"
Requires-Dist: lm_eval; extra == "llm"
Requires-Dist: onnx; extra == "llm"
Requires-Dist: onnxruntime<1.21; extra == "llm"
Requires-Dist: optimum; extra == "llm"
Requires-Dist: pandas; extra == "llm"
Requires-Dist: torch>=2.3; extra == "llm"
Requires-Dist: tqdm; extra == "llm"
Requires-Dist: transformers[sentencepiece]==4.50.0; extra == "llm"
Provides-Extra: diffusion
Requires-Dist: accelerate; extra == "diffusion"
Requires-Dist: diffusers<=0.34; extra == "diffusion"
Requires-Dist: pandas; extra == "diffusion"
Requires-Dist: torch>=2.0; extra == "diffusion"
Requires-Dist: torchmetrics[image]; extra == "diffusion"
Requires-Dist: tqdm; extra == "diffusion"
Requires-Dist: transformers; extra == "diffusion"
Provides-Extra: vision
Requires-Dist: accelerate; extra == "vision"
Requires-Dist: torch>=1.13; extra == "vision"
Requires-Dist: torchvision; extra == "vision"
Requires-Dist: tqdm; extra == "vision"
Provides-Extra: finn-integration
Requires-Dist: bitstring; extra == "finn-integration"
Requires-Dist: onnx==1.17.0; extra == "finn-integration"
Requires-Dist: onnxoptimizer; extra == "finn-integration"
Requires-Dist: onnxruntime<1.21,>=1.15.0; extra == "finn-integration"
Requires-Dist: onnxscript; extra == "finn-integration"
Requires-Dist: qonnx; extra == "finn-integration"
Requires-Dist: toposort; extra == "finn-integration"
Provides-Extra: ort-integration
Requires-Dist: onnx==1.17.0; extra == "ort-integration"
Requires-Dist: onnxoptimizer; extra == "ort-integration"
Requires-Dist: onnxruntime<1.21,>=1.15.0; extra == "ort-integration"
Requires-Dist: onnxscript; extra == "ort-integration"
Requires-Dist: qonnx; extra == "ort-integration"

# Brevitas

[![Downloads](https://pepy.tech/badge/brevitas)](https://pepy.tech/project/brevitas)
[![Pytest](https://github.com/Xilinx/brevitas/actions/workflows/pytest.yml/badge.svg?branch=master)](https://github.com/Xilinx/brevitas/actions/workflows/pytest.yml)
[![Examples Pytest](https://github.com/Xilinx/brevitas/actions/workflows/examples_pytest.yml/badge.svg?branch=master)](https://github.com/Xilinx/brevitas/actions/workflows/examples_pytest.yml)
[![DOI](https://zenodo.org/badge/140494324.svg)](https://zenodo.org/badge/latestdoi/140494324)

Brevitas is a PyTorch library for neural network quantization, with support for both *post-training quantization (PTQ)* and *quantization-aware training (QAT)*.

**Please note that Brevitas is a research project and not an official Xilinx product.**

If you like this project please consider ⭐ this repo, as it is the simplest and best way to support it.

## Requirements

* Python >= 3.9, <3.13
* [Pytorch](https://pytorch.org) >= 1.12, <= 2.8 (more recent versions would be untested).
* Windows, Linux or macOS.
* GPU training-time acceleration (*Optional* but recommended).

## Installation

You can install the latest release from PyPI:
```bash
pip install brevitas
```

## Getting Started

Brevitas currently offers quantized implementations of the most common PyTorch layers used in DNN under `brevitas.nn`, such as `QuantConv1d`, `QuantConv2d`, `QuantConvTranspose1d`, `QuantConvTranspose2d`, `QuantMultiheadAttention`, `QuantRNN`, `QuantLSTM` etc., for adoption within PTQ and/or QAT.
For each one of these layers, quantization of different tensors (inputs, weights, bias, outputs, etc) can be individually tuned according to a wide range of quantization settings.

As a reference for PTQ, Brevitas provides an example user flow for ImageNet classification models under [`brevitas_examples.imagenet_classification.ptq`](https://github.com/Xilinx/brevitas/blob/master/src/brevitas_examples/imagenet_classification/ptq/ptq_evaluate.py) that quantizes an input torchvision model using PTQ under different quantization configurations (e.g. bit-width, granularity of scale, etc).

For more info, checkout our [documentation](https://xilinx.github.io/brevitas/).

## Cite as

If you adopt Brevitas in your work, please cite it as:
```
@software{brevitas,
  author       = {Franco, Giuseppe and Pappalardo, Alessandro and Fraser, Nicholas J},
  title        = {Xilinx/brevitas},
  year         = {2025},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.3333552},
  url          = {https://doi.org/10.5281/zenodo.3333552}
}
```

## History

- *2025/08/28* - Release version 0.12.1, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.12.1).
- *2025/05/09* - Release version 0.12.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.12.0).
- *2024/10/10* - Release version 0.11.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.11.0).
- *2024/07/23* - Minor release version 0.10.3, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.10.3).
- *2024/02/19* - Minor release version 0.10.2, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.10.2).
- *2024/02/15* - Minor release version 0.10.1, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.10.1).
- *2023/12/08* - Release version 0.10.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.10.0).
- *2023/04/28* - Minor release version 0.9.1, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.9.1).
- *2023/04/21* - Release version 0.9.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.9.0).
- *2023/01/10* - Release version 0.8.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.8.0).
- *2021/12/13* - Release version 0.7.1, fix a bunch of issues. Added TVMCon 2021 tutorial notebook.
- *2021/11/03* - Re-release version 0.7.0 (build 1) on PyPI to fix a packaging issue.
- *2021/10/29* - Release version 0.7.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.7.0).
- *2021/06/04* - Release version 0.6.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.6.0).
- *2021/05/24* - Release version 0.5.1, fix a bunch of minor issues. See [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.1).
- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0).
- *2021/03/15* - Release version 0.4.0, add support for \_\_torch_function\_\_ to QuantTensor.
- *2021/03/04* - Release version 0.3.1, fix bug w/ act initialization from statistics w/ IGNORE_MISSING_KEYS=1.
- *2021/03/01* - Release version 0.3.0, implements enum and shape solvers within extended dependency injectors. This allows declarative quantizers to be self-contained.
- *2021/02/04* - Release version 0.2.1, includes various bugfixes of QuantTensor w/ zero-point.
- *2021/01/30* - First release version 0.2.0 on PyPI.
