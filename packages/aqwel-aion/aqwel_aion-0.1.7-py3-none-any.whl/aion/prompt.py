#!/usr/bin/env python3
"""
ðŸ“ Aqwel-Aion v0.1.7 - AI Prompt Engineering & Management Module
================================================================

ðŸš€ COMPLETELY REWRITTEN IN v0.1.7 - PROFESSIONAL PROMPT ENGINEERING:
This module was entirely rewritten for v0.1.7 to provide state-of-the-art
prompt engineering capabilities specifically designed for AI researchers.

ðŸŽ¯ WHAT WAS ADDED IN v0.1.7:
- âœ… show_prompt(): Enhanced prompt display with 11+ professional templates
- âœ… get_prompt_templates(): Comprehensive template library for AI research
- âœ… create_custom_prompt(): Dynamic prompt generation with variable substitution
- âœ… build_conversation_prompt(): Multi-turn conversation management
- âœ… extract_code_from_prompt(): Intelligent code extraction from AI responses
- âœ… validate_prompt_length(): Token counting and length optimization
- âœ… optimize_prompt_for_ai(): Automatic prompt improvement and refinement

ðŸ”¬ PROFESSIONAL TEMPLATE LIBRARY:
- System prompts for AI assistants and research tools
- Code review and analysis prompts for quality assessment
- Data analysis prompts for statistical and ML workflows
- Research prompts for academic and scientific applications
- Creative and brainstorming prompts for ideation
- Educational prompts for learning and teaching scenarios

ðŸ’¡ PERFECT FOR AI RESEARCHERS:
This module provides research-grade prompt engineering tools that help
researchers get better results from AI models, improve reproducibility,
and standardize prompt workflows across projects.

ðŸ† ADVANCED FEATURES:
- Variable substitution with type checking and validation
- Conversation context management for multi-turn interactions
- Automatic prompt optimization using best practices
- Code extraction with syntax validation
- Length optimization for different AI model constraints

Author: Aksel Aghajanyan
License: Apache-2.0
Copyright: 2025 Aqwel AI  
Version: 0.1.7 (Complete rewrite - was basic display function in v0.1.6)
"""

import re
from typing import List, Dict, Tuple

def show_prompt(prompt_type: str) -> str:
    """
    Display and return a formatted prompt based on the specified prompt type.
    
    Args:
        prompt_type: The type of prompt to display
        
    Returns:
        The formatted prompt string
    """
    prompts = get_prompt_templates()
    
    if prompt_type in prompts:
        prompt = prompts[prompt_type]
        print(f"ðŸ› ï¸ {prompt_type.title()} prompt:\n{prompt}")
        return prompt
    else:
        default_prompt = "Hello, I need assistance with a task."
        print(f"ðŸ—£ï¸ Default prompt:\n{default_prompt}")
        return default_prompt


def get_prompt_templates() -> Dict[str, str]:
    """
    Get collection of pre-defined AI prompt templates for common tasks.
    
    Returns
    -------
    dict
        Dictionary mapping template names to prompt strings
    """
    return {
        "system": "You are Aion, an advanced AI assistant specialized in helping with software development, data science, and AI/ML tasks. You provide accurate, helpful, and detailed responses.",
        
        "code_review": "Please review the following code and provide feedback on:\n1. Code quality and best practices\n2. Potential bugs or issues\n3. Performance optimizations\n4. Readability improvements\n\nCode to review:",
        
        "debugging": "I'm encountering an issue with my code. Please help me debug it by:\n1. Identifying the problem\n2. Explaining why it occurs\n3. Providing a solution\n4. Suggesting prevention strategies\n\nCode and error:",
        
        "optimization": "Please help optimize this code for better performance, readability, or maintainability. Consider:\n1. Algorithm efficiency\n2. Memory usage\n3. Code structure\n4. Best practices\n\nCode to optimize:",
        
        "explanation": "Please explain the following code in detail:\n1. What it does\n2. How it works\n3. Key concepts used\n4. Potential use cases\n\nCode to explain:",
        
        "documentation": "Please help create comprehensive documentation for this code including:\n1. Function/class descriptions\n2. Parameter explanations\n3. Return value descriptions\n4. Usage examples\n\nCode to document:",
        
        "testing": "Please help create unit tests for this code. Include:\n1. Test cases for normal operation\n2. Edge case testing\n3. Error condition testing\n4. Mock data if needed\n\nCode to test:",
        
        "refactoring": "Please suggest refactoring improvements for this code:\n1. Extract methods/classes\n2. Improve naming\n3. Reduce complexity\n4. Follow SOLID principles\n\nCode to refactor:",
        
        "data_analysis": "Please help analyze this dataset or data-related code:\n1. Data exploration insights\n2. Statistical analysis\n3. Visualization suggestions\n4. Quality assessment\n\nData/code to analyze:",
        
        "ml_model": "Please help with this machine learning code:\n1. Model architecture review\n2. Training strategy\n3. Evaluation metrics\n4. Improvement suggestions\n\nML code:",
        
        "api_design": "Please help design or review this API:\n1. Endpoint structure\n2. Request/response formats\n3. Error handling\n4. Documentation\n\nAPI code/specification:"
    }


def create_custom_prompt(template: str, **kwargs) -> str:
    """
    Create a custom prompt from a template with variable substitution.
    
    Args:
        template: Prompt template string with {variable} placeholders
        **kwargs: Variables to substitute in the template
        
    Returns:
        Formatted prompt string
        
    Examples:
        >>> template = "Analyze {language} code for {purpose}"
        >>> prompt = create_custom_prompt(template, language="Python", purpose="bugs")
        >>> print(prompt)
        Analyze Python code for bugs
    """
    try:
        return template.format(**kwargs)
    except KeyError as e:
        raise ValueError(f"Missing required variable: {e}")


def build_conversation_prompt(messages: List[Dict[str, str]], system_prompt: str = None) -> str:
    """
    Build a conversation-style prompt from a list of messages.
    
    Args:
        messages: List of message dictionaries with 'role' and 'content' keys
        system_prompt: Optional system prompt to prepend
        
    Returns:
        Formatted conversation prompt
        
    Examples:
        >>> messages = [
        ...     {"role": "user", "content": "Hello"},
        ...     {"role": "assistant", "content": "Hi there!"},
        ...     {"role": "user", "content": "How are you?"}
        ... ]
        >>> prompt = build_conversation_prompt(messages)
    """
    conversation = []
    
    if system_prompt:
        conversation.append(f"System: {system_prompt}")
    
    for message in messages:
        role = message.get("role", "user").title()
        content = message.get("content", "")
        conversation.append(f"{role}: {content}")
    
    return "\n\n".join(conversation)


def extract_code_from_prompt(prompt: str) -> List[str]:
    """
    Extract code blocks from a prompt text.
    
    Args:
        prompt: Prompt text that may contain code blocks
        
    Returns:
        List of extracted code strings
    """
    # Extract code blocks marked with triple backticks
    code_blocks = re.findall(r'```(?:\w+)?\n?(.*?)\n?```', prompt, re.DOTALL)
    
    # Also extract inline code with single backticks
    inline_code = re.findall(r'`([^`]+)`', prompt)
    
    all_code = code_blocks + inline_code
    return [code.strip() for code in all_code if code.strip()]


def validate_prompt_length(prompt: str, max_tokens: int = 4000) -> Tuple[bool, int]:
    """
    Validate if a prompt is within token limits.
    
    Args:
        prompt: Prompt text to validate
        max_tokens: Maximum allowed tokens (approximate)
        
    Returns:
        Tuple of (is_valid, estimated_tokens)
    """
    # Rough estimation: 1 token â‰ˆ 4 characters for English text
    estimated_tokens = len(prompt) // 4
    is_valid = estimated_tokens <= max_tokens
    
    return is_valid, estimated_tokens


def optimize_prompt_for_ai(prompt: str) -> str:
    """
    Optimize a prompt for better AI model performance.
    
    Args:
        prompt: Original prompt text
        
    Returns:
        Optimized prompt text
    """
    # Add structure and clarity
    optimized = prompt.strip()
    
    # Ensure clear instructions
    if not any(word in optimized.lower() for word in ["please", "help", "analyze", "explain", "create"]):
        optimized = f"Please help with the following: {optimized}"
    
    # Add specific output format request if not present
    if "format" not in optimized.lower() and "structure" not in optimized.lower():
        optimized += "\n\nPlease provide a clear, structured response."
    
    return optimized