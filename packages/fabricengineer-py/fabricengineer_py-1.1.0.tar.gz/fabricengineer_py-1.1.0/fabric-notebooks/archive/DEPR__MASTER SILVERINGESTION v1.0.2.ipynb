{"cells":[{"cell_type":"markdown","source":["# MASTER SILVER INGESTION v1.0.1\n","\n","## Version change log\n","\n","##### **v1.0.1**\n","\n","- df_bronze can be passed to the etl via param\n","\n","##### **v1.0.2**\n","\n","- \"*\" as transformations wildcard; transformation will applied to all tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"33aa1149-60f7-4b9e-a52e-5537840cf7f4"},{"cell_type":"code","source":["# flake8: noqa\n","\n","import datetime\n","\n","from typing import Callable\n","from uuid import uuid4\n","from dataclasses import dataclass\n","\n","from pyspark.sql import SparkSession, DataFrame\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","\n","@F.udf(returnType=StringType())\n","def generate_uuid():\n","    \"\"\"Generiert eine UUID4\"\"\"\n","    return str(uuid4())\n","\n","\n","@dataclass(frozen=True)\n","class ConstantColumn:\n","    \"\"\"Class for adding a column with constant value to etl\"\"\"\n","    name: str\n","    value: str\n","    part_of_nk: bool = False\n","\n","    def __post_init__(self):\n","        \"\"\"\n","        Nach initialisierung wird der name in UPPERCASE umgewandelt.\n","        \"\"\"\n","        object.__setattr__(self, \"name\", self.name.upper())\n","\n","\n","class SilverIngestionService:\n","    _is_initialized: bool = False\n","\n","    def init(\n","        self,\n","        *,\n","        spark: SparkSession,\n","        src_lakehouse: str,\n","        src_schema: str,\n","        src_tablename: str,\n","        dist_lakehouse: str,\n","        dist_schema: str,\n","        dist_tablename: str,\n","        nk_columns: list[str],\n","        constant_columns: list[ConstantColumn],\n","        is_delta_load: bool,\n","        delta_load_use_broadcast: bool,\n","        transformations: dict,\n","        exclude_comparing_columns: list[str] | None = None,\n","        include_comparing_columns: list[str] | None = None,\n","        historize: bool = True,\n","        partition_by_columns: list[str] = None,\n","        df_bronze: DataFrame = None,\n","        \n","        pk_column: str = \"PK\",\n","        nk_column: str = \"NK\",\n","        nk_column_concate_str: str = \"_\",\n","        row_is_current_column: str = \"ROW_IS_CURRENT\",\n","        row_update_dts_column: str = \"ROW_UPDATE_DTS\",\n","        row_delete_dts_column: str = \"ROW_DELETE_DTS\",\n","        ldts_column: str = \"LDTS\",\n","    ) -> None:\n","        self._current_timestamp = F.from_utc_timestamp(F.current_timestamp(), \"Europe/Berlin\")\n","\n","        self._spark = spark\n","        self._df_bronze = df_bronze\n","        self._historize = historize\n","        self._is_delta_load = is_delta_load\n","        self._delta_load_use_broadcast = delta_load_use_broadcast\n","        self._src_lakehouse = src_lakehouse\n","        self._src_schema = src_schema\n","        self._src_tablename = src_tablename\n","        self._dist_lakehouse = dist_lakehouse\n","        self._dist_schema = dist_schema\n","        self._dist_tablename = dist_tablename\n","        self._nk_columns = nk_columns\n","        self._include_comparing_columns = include_comparing_columns\n","\n","        self._exclude_comparing_columns = exclude_comparing_columns if isinstance(exclude_comparing_columns, list) else []\n","        self._transformations: dict[str, Callable] = transformations if isinstance(transformations, dict) else {}\n","        self._constant_columns: list[ConstantColumn] = constant_columns if isinstance(constant_columns, list) else []        \n","        self._partition_by: list[str] = partition_by_columns if isinstance(partition_by_columns, list) else []\n","\n","        self._pk_column = pk_column\n","        self._nk_column = nk_column\n","        self._nk_column_concate_str = nk_column_concate_str\n","        self._row_is_current_column = row_is_current_column\n","        self._row_update_dts_column = row_update_dts_column\n","        self._row_delete_dts_column = row_delete_dts_column\n","        self._ldts_column = ldts_column\n","\n","        self._sql_src_table = f\"{src_lakehouse}.{src_schema}.{src_tablename}\"\n","        self._sql_dist_table = f\"{dist_lakehouse}.{dist_schema}.{dist_tablename}\"\n","        \n","        self._validate_parameters()\n","        self._set_spark_config()\n","\n","        self._dw_columns = [\n","            self._pk_column,\n","            self._nk_column,\n","            self._row_is_current_column,\n","            self._row_update_dts_column,\n","            self._row_delete_dts_column,\n","            self._ldts_column\n","        ]\n","\n","        self._exclude_comparing_columns = set(\n","            [self._pk_column]\n","            + self._nk_columns\n","            + self._dw_columns\n","            + self._exclude_comparing_columns\n","            + [column.name for column in self._constant_columns]\n","        )\n","\n","        self._spark.catalog.clearCache()\n","        self._is_initialized = True\n","\n","    def __str__(self) -> str:\n","        if not self._is_initialized:\n","            return super.__str__(self)\n","    \n","        return str({\n","            \"historize\": self._historize,\n","            \"is_delta_load\": self._is_delta_load,\n","            \"delta_load_use_broadcast\": self._delta_load_use_broadcast,\n","            \"src_lakehouse\": self._src_lakehouse,\n","            \"src_schema\": self._src_schema,\n","            \"src_tablename\": self._src_tablename,\n","            \"dist_lakehouse\": self._dist_lakehouse,\n","            \"dist_schema\": self._dist_schema,\n","            \"dist_tablename\": self._dist_tablename,\n","            \"nk_columns\": self._nk_columns,\n","            \"include_comparing_columns\": self._include_comparing_columns,\n","            \"exclude_comparing_columns\": self._exclude_comparing_columns,\n","            \"transformations\": self._transformations,\n","            \"constant_columns\": self._constant_columns,\n","            \"partition_by\": self._partition_by,\n","            \"pk_column\": self._pk_column,\n","            \"nk_column\": self._nk_column,\n","            \"nk_column_concate_str\": self._nk_column_concate_str,\n","            \"row_is_current_column\": self._row_is_current_column,\n","            \"row_update_dts_column\": self._row_update_dts_column,\n","            \"row_delete_dts_column\": self._row_delete_dts_column,\n","            \"ldts_column\": self._ldts_column,\n","            \"sql_src_table\": self._sql_src_table,\n","            \"sql_dist_table\": self._sql_dist_table,\n","            \"dw_columns\": self._dw_columns,\n","            \"exclude_comparing_columns\": self._exclude_comparing_columns\n","        })\n","\n","    def _validate_parameters(self) -> None:\n","        \"\"\"Validates the in constructor setted parameters, so the etl can run.\n","\n","        Raises:\n","            ValueError: when a valueerror occurs\n","            TypeError: when a typerror occurs\n","            Exception: generic exception\n","        \"\"\"\n","\n","        if self._df_bronze is not None:\n","            self._validate_param_isinstance(self._df_bronze, \"df_bronze\", DataFrame)\n","    \n","        self._validate_param_isinstance(self._spark, \"spark\", SparkSession)\n","        self._validate_param_isinstance(self._historize, \"historize\", bool)\n","        self._validate_param_isinstance(self._is_delta_load, \"is_delta_load\", bool)\n","        self._validate_param_isinstance(self._delta_load_use_broadcast, \"delta_load_use_broadcast\", bool)\n","        self._validate_param_isinstance(self._transformations, \"transformations\", dict)\n","        self._validate_min_length(self._pk_column, \"pk_column\", 2)\n","        self._validate_min_length(self._src_lakehouse, \"src_lakehouse\", 3)\n","        self._validate_min_length(self._src_schema, \"src_schema\", 3)\n","        self._validate_min_length(self._src_tablename, \"src_tablename\", 3)\n","        self._validate_min_length(self._dist_lakehouse, \"dist_lakehouse\", 3)\n","        self._validate_min_length(self._dist_schema, \"dist_schema\", 3)\n","        self._validate_min_length(self._dist_tablename, \"dist_tablename\", 3)\n","        self._validate_min_length(self._nk_columns, \"nk_columns\", 1)\n","        self._validate_min_length(self._nk_column, \"nk_column\", 2)\n","        self._validate_min_length(self._nk_column_concate_str, \"nk_column_concate_str\", 1)\n","        self._validate_param_isinstance(self._exclude_comparing_columns, \"exclude_columns_from_comparing\", list)\n","\n","        self._validate_min_length(self._row_delete_dts_column, \"row_delete_dts_columnname\", 3)\n","        self._validate_min_length(self._ldts_column, \"ldts_column\", 3)\n","        self._validate_param_isinstance(self._constant_columns, \"constant_columns\", list)\n","\n","    def _validate_param_isinstance(self, param, param_name: str, obj_class) -> None:\n","        \"\"\"Validates a parameter to be the expected class instance\n","\n","        Args:\n","            param (any): parameter\n","            param_name (str): parametername\n","            obj_class (_type_): class\n","\n","        Raises:\n","            TypeError: when actual type is different from expected type\n","        \"\"\"\n","        if not isinstance(param, obj_class):\n","            err_msg = f\"The param '{param_name}' should be type of {obj_class.__name__}, but was {str(param.__class__)}\"\n","            raise TypeError(err_msg)\n","\n","    def _validate_min_length(self, param, param_name: str, min_length: int) -> None:\n","        \"\"\"Validates a string or list to be not none and has a minimum length\n","\n","        Args:\n","            param (_type_): parameter\n","            param_name (str): parametername\n","            min_length (int): minimum lenght\n","\n","        Raises:\n","            TypeError: when actual type is different from expected type\n","            ValueError: when parametervalue is to short\n","        \"\"\"\n","        if not isinstance(param, str) and not isinstance(param, list):\n","            err_msg = f\"The param '{param_name}' should be type of string or list, but was {str(param.__class__)}\"\n","            raise TypeError(err_msg)\n","\n","        param_length = len(param)\n","        if param_length < min_length:\n","            err_msg = f\"Param length to short. The minimum length of the param '{param_name}' is {min_length} but was {param_length}\"\n","            raise ValueError(err_msg)\n","\n","    def _validate_constant_columns(self) -> None:\n","        \"\"\"Validates the given constant columns to be an instance of ConstantColumns and\n","        list contains only one part_of_nk=True, because of the following filtering of the dataframe.\n","\n","        It should have just one part_of_nk=True, because the dataframe will filtered later by the\n","        constant_column.name, if part_of_nk=True.\n","        If part_of_nk=True should be supported more then once, then we need to implement\n","        an \"and\" filtering.\n","\n","        Raises:\n","            TypeError: when an item of the list is not an instance of ConstantColumn\n","            ValueError: when list contains more then one ConstantColumn with part_of_nk=True\n","        \"\"\"\n","        nk_count = 0\n","        for column in self._constant_columns:\n","            if column.part_of_nk:\n","                nk_count += 1\n","\n","            if not isinstance(column, ConstantColumn):\n","                err_msg = f\"Invalid items in constant_columns found. All items should be instance of ConstantColumn\"\n","                raise TypeError(err_msg)\n","\n","            if nk_count > 1:\n","                err_msg = \"In constant_columns are more then one part_of_nk=True, what is not supported!\"\n","                raise ValueError(err_msg)\n","\n","    def _validate_nk_columns_in_df(self, df: DataFrame) -> None:\n","        \"\"\"Validates the given dataframe. The given dataframe should contain all natural key columns,\n","        because all natural key columns will selected and used for concatitation.\n","\n","        Args:\n","            df (DataFrame): dataframe to validate\n","\n","        Raises:\n","            ValueError: when dataframe does not contain all natural key columns\n","        \"\"\"\n","        df_columns = set(df.columns)\n","        for column in self._nk_columns:\n","            if column in df_columns:\n","                continue\n","\n","            err_msg = f\"The NK Column '{column}' does not exist in df columns: {df_columns}\"\n","            raise ValueError(err_msg)\n","\n","    def _validate_include_comparing_columns(self, df: DataFrame) -> None:\n","        \"\"\"\n","        TODO: add doku\n","        \"\"\"\n","        self._validate_param_isinstance(self._include_comparing_columns, \"include_comparing_columns\", list)\n","\n","        if len(self._include_comparing_columns) == 0:\n","            err_msg = \"The param 'include_comparing_columns' is present, but don't contains any columns.\"\n","            raise ValueError(err_msg)\n","        \n","        for include_column in self._include_comparing_columns:\n","            if include_column in df.columns:\n","                continue\n","    \n","            err_msg = f\"The column '{include_column}' should be compared, but is not given in df.\"\n","            raise ValueError(err_msg)\n","\n","    def _validate_partition_by_columns(self, df: DataFrame) -> None:\n","        self._validate_param_isinstance(self._partition_by, \"partition_by\", list)\n","        \n","        for partition_column in self._partition_by:\n","            if partition_column in df.columns:\n","                continue\n","            \n","            err_msg = f\"The column '{partition_column}' should be partitioned, but is not given in df.\"\n","            raise ValueError(err_msg)\n","\n","    def _set_spark_config(self) -> None:\n","        \"\"\"Sets additional spark configurations\n","\n","        spark.sql.parquet.vorder.enabled: Setting \"spark.sql.parquet.vorder.enabled\" to \"true\" in PySpark config enables a feature called vectorized parquet decoding.\n","                                                  This optimizes the performance of reading Parquet files by leveraging vectorized instructions and processing multiple values at once, enhancing overall processing speed.\n","        \n","        Setting \"spark.sql.legacy.parquet.int96RebaseModeInRead\" and \"spark.sql.legacy.parquet.int96RebaseModeInWrite\" to \"CORRECTED\" ensures that Int96 values (a specific timestamp representation used in Parquet files) are correctly rebased during both reading and writing operations.\n","        This is crucial for maintaining consistency and accuracy, especially when dealing with timestamp data across different systems or time zones.\n","        Similarly, configuring \"spark.sql.legacy.parquet.datetimeRebaseModeInRead\" and \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" to \"CORRECTED\" ensures correct handling of datetime values during Parquet file operations.\n","        By specifying this rebasing mode, potential discrepancies or errors related to datetime representations are mitigated, resulting in more reliable data processing and analysis workflows.\n","        \"\"\"\n","        self._spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","\n","        self._spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n","        self._spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n","        self._spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","        self._spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","\n","    def ingest(self):\n","        # 1.\n","        df_bronze, df_silver = self._generate_dataframes()\n","\n","        target_columns_ordered = self._get_columns_ordered(df_bronze)\n","\n","        do_overwrite = (\n","            df_silver is None or\n","            (\n","                not self._historize and \n","                not self._is_delta_load\n","                # If we are not historizing but performing a delta load,\n","                # we need to update the silver-layer data.\n","                # We should not overwrite the silver-layer data,\n","                # because the delta load (bronze layer) do not contain all the data!\n","            )\n","        )\n","        if do_overwrite:\n","            df_inital_load = df_bronze.select(target_columns_ordered)\n","            self._write_df(df_inital_load, \"overwrite\")\n","            return\n","\n","        # 2.\n","        columns_to_compare = self._get_columns_to_compare(df_bronze)\n","\n","        join_condition = (df_bronze[self._nk_column] == df_silver[self._nk_column])\n","        df_joined = df_bronze.join(df_silver, join_condition, \"outer\")\n","\n","        # 3.\n","        _, neq_condition = self._get_compare_condition(df_bronze, df_silver, columns_to_compare)\n","        updated_filter_condition = self._get_updated_filter(df_bronze, df_silver, neq_condition)\n","\n","        df_new_records = self._filter_new_records(df_joined, df_bronze, df_silver)\n","        df_updated_records = self._filter_updated_records(df_joined, df_bronze, updated_filter_condition)\n","\n","        # 4.\n","        df_new_data = df_new_records.unionByName(df_updated_records).select(target_columns_ordered).dropDuplicates([\"PK\"])\n","        self._write_df(df_new_data, \"append\")\n","\n","        # 5.\n","        df_expired_records = self._filter_expired_records(df_joined, df_silver, updated_filter_condition)\n","        self._exec_merge_into(df_expired_records, \"df_expired_records\")\n","\n","        # 6.\n","        if self._is_delta_load:\n","            return\n","\n","        df_deleted_records = self._filter_deleted_records(df_joined, df_bronze, df_silver)\n","        self._exec_merge_into(df_deleted_records, \"df_deleted_records\")\n","\n","    def _generate_dataframes(self) -> tuple[DataFrame, DataFrame]:\n","        df_bronze = self._create_bronze_df()\n","        df_bronze = self._apply_transformations(df_bronze)\n","\n","        df_silver = self._create_silver_df()\n","\n","        if df_silver is None:\n","            return df_bronze, df_silver\n","\n","        df_bronze = self._add_missing_columns(df_bronze, df_silver)\n","        df_silver = self._add_missing_columns(df_silver, df_bronze)\n","\n","        if self._is_delta_load and self._delta_load_use_broadcast:\n","            df_bronze = F.broadcast(df_bronze)\n","\n","        return df_bronze, df_silver\n","    \n","    def _get_compare_condition(self, df_bronze: DataFrame, df_silver: DataFrame, columns_to_compare: list[str]):\n","        eq_condition = (\n","            (df_bronze[columns_to_compare[0]] == df_silver[columns_to_compare[0]]) |\n","            (df_bronze[columns_to_compare[0]].isNull() & df_silver[columns_to_compare[0]].isNull())\n","        )\n","\n","        if len(columns_to_compare) == 1:\n","            return eq_condition, ~eq_condition\n","        \n","\n","        for compare_column in columns_to_compare[1:]:\n","            eq_condition &= (\n","                (df_bronze[compare_column] == df_silver[compare_column]) |\n","                (df_bronze[compare_column].isNull() & df_silver[compare_column].isNull())\n","            )\n","        \n","        return eq_condition, ~eq_condition\n","\n","    def _get_updated_filter(self, df_bronze: DataFrame, df_silver: DataFrame, neq_condition):\n","        updated_filter = (\n","            (df_bronze[self._nk_column].isNotNull()) &\n","            (df_silver[self._nk_column].isNotNull()) &\n","            (df_silver[self._row_is_current_column] == 1) &\n","            (neq_condition)\n","        )\n","\n","        return updated_filter\n","\n","    def _filter_new_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n","        new_records_filter = (df_silver[self._nk_column].isNull())\n","        df_new_records = df_joined.filter(new_records_filter) \\\n","                                  .select(df_bronze[\"*\"])\n","        \n","        return df_new_records\n","\n","    def _filter_updated_records(self, df_joined: DataFrame, df_bronze: DataFrame, updated_filter) -> DataFrame:\n","        # Select not matching bronze columns\n","        df_updated_records = df_joined.filter(updated_filter) \\\n","                                      .select(df_bronze[\"*\"])\n","        \n","        return df_updated_records\n","\n","    def _filter_expired_records(self, df_joined: DataFrame, df_silver: DataFrame, updated_filter) -> DataFrame:\n","        # Select not matching silver columns\n","        df_expired_records = df_joined.filter(updated_filter) \\\n","                                      .select(df_silver[\"*\"]) \\\n","                                      .withColumn(self._row_update_dts_column, self._current_timestamp) \\\n","                                      .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\")) \\\n","                                      .withColumn(self._row_is_current_column, F.lit(0))\n","       \n","        return df_expired_records\n","\n","    def _filter_deleted_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n","        df_deleted_records = df_joined.filter(df_bronze[self._nk_column].isNull()) \\\n","                                      .select(df_silver[\"*\"]) \\\n","                                      .withColumn(self._row_update_dts_column, self._current_timestamp) \\\n","                                      .withColumn(self._row_delete_dts_column, self._current_timestamp) \\\n","                                      .withColumn(self._row_is_current_column, F.lit(0))\n","\n","        return df_deleted_records\n","\n","    def _create_bronze_df(self) -> DataFrame:\n","        sql_select_source = f\"SELECT * FROM {self._sql_src_table}\"\n","        df = self._df_bronze if self._df_bronze is not None else self._spark.sql(sql_select_source)\n","\n","        self._validate_nk_columns_in_df(df)\n","\n","        for constant_column in self._constant_columns:\n","            if constant_column.name not in df.columns:\n","                df = df.withColumn(constant_column.name, F.lit(constant_column.value))\n","\n","        df = df.withColumn(self._pk_column, generate_uuid())  \\\n","               .withColumn(self._nk_column, F.concat_ws(self._nk_column_concate_str, *self._nk_columns)) \\\n","               .withColumn(self._row_update_dts_column, F.lit(None).cast(\"timestamp\")) \\\n","               .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\")) \\\n","               .withColumn(self._row_is_current_column, F.lit(1)) \\\n","               .withColumn(self._ldts_column, F.lit(self._current_timestamp))\n","\n","        return df\n","\n","    def _create_silver_df(self) -> DataFrame:\n","        if not self._spark.catalog.tableExists(self._sql_dist_table):\n","            return None\n","\n","        sql_select_destination = f\"SELECT * FROM {self._sql_dist_table}\"\n","        df = self._spark.sql(sql_select_destination)\n","\n","        self._validate_nk_columns_in_df(df)\n","\n","        for constant_column in self._constant_columns:\n","            if constant_column.name not in df.columns:\n","                df = df.withColumn(constant_column.name, F.lit(None))\n","\n","            if constant_column.part_of_nk:\n","                df = df.filter(F.col(constant_column.name) == constant_column.value)\n","\n","        df = df.withColumn(self._nk_column, F.concat_ws(self._nk_column_concate_str, *self._nk_columns))\n","\n","        return df\n","\n","    def _add_missing_columns(self, df_target: DataFrame, df_source: DataFrame) -> DataFrame:\n","        missing_columns = [missing_column for missing_column in df_source.columns if missing_column not in df_target.columns]\n","\n","        for missing_column in missing_columns:\n","            df_target = df_target.withColumn(missing_column, F.lit(None))\n","\n","        return df_target\n","\n","    def _get_columns_to_compare(self, df: DataFrame) -> list[str]:\n","        if isinstance(self._include_comparing_columns, list) and len(self._include_comparing_columns) >= 1:\n","            self._validate_include_comparing_columns(df)\n","            return self._include_comparing_columns\n","\n","        comparison_columns = [column for column in df.columns if column not in self._exclude_comparing_columns]\n","\n","        return comparison_columns\n","\n","    def _get_columns_ordered(self, df: DataFrame) -> list[str]:\n","        all_columns = [column for column in df.columns if column not in self._dw_columns]\n","\n","        return [self._pk_column, self._nk_column] + all_columns + [self._ldts_column, self._row_update_dts_column, self._row_delete_dts_column, self._row_is_current_column]\n","\n","    def _apply_transformations(self, df: DataFrame) -> DataFrame:\n","        transform_fn: Callable = self._transformations.get(self._src_tablename)\n","        transform_fn_all: Callable = self._transformations.get(\"*\")\n","\n","        if transform_fn_all is not None:\n","            df = transform_fn_all(df, self)\n","\n","        if transform_fn is None:\n","            return df\n","        \n","        return transform_fn(df, self)\n","\n","    def _exec_merge_into(self, df_source: DataFrame, view_name: str) -> None:\n","        df_source.createOrReplaceTempView(view_name)\n","        self._spark.sql(f\"\"\"\n","            MERGE INTO {self._sql_dist_table} AS target\n","            USING {view_name} AS source\n","            ON target.{self._pk_column} = source.{self._pk_column}\n","            WHEN MATCHED THEN\n","            UPDATE SET\n","                {self._row_update_dts_column} = source.{self._row_update_dts_column},\n","                {self._row_delete_dts_column} = source.{self._row_delete_dts_column},\n","                {self._row_is_current_column} = source.{self._row_is_current_column}\n","        \"\"\")\n","\n","    def _write_df(self, df: DataFrame, write_mode: str) -> None:\n","        df.write \\\n","            .format(\"delta\") \\\n","            .mode(write_mode) \\\n","            .option(\"mergeSchema\", \"true\") \\\n","            .partitionBy(*self._partition_by) \\\n","            .saveAsTable(self._sql_dist_table)\n","\n","\n","etl = SilverIngestionService()\n","\n","print(\"MASTER NOTEBOOK PASSES 'etl':\", str(etl))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7dbdedb7-8c2e-4c6f-8c05-08af196b8cae"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}