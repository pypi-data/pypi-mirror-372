{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9e391f-c2b1-40ab-8e7e-946e7811eeef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Export fabricengineer-py Modules\n",
    "\n",
    "This notebook retrieves the fabricengineer-py module source code directly from GitHub, allowing you to work without relying on slow-loading Fabric environments in notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bfe4d-d9a6-4da8-89ce-2a97a9f5466a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8f42e4-b628-4b1c-9496-9185a845d59c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:08:18.4087901Z",
       "execution_start_time": "2025-08-14T10:08:17.9191491Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8b3b5d86-1124-4dd6-95e1-9d8646c353bb",
       "queued_time": "2025-08-14T10:08:06.8957008Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": "2025-08-14T10:08:06.8966968Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IS_DEV: bool = True\n",
    "VERSION: str = None\n",
    "LAKEHOUSE_CODE_ABFSS = None\n",
    "MODULES: str = None  # transform.mlv | transform.silver.insertonly | transform.silver.scd2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148778f5-edf9-4c95-bd34-4d5cce44f4eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f5a6b4-0abe-4a17-a13d-a7641a811e4a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:08:18.8156223Z",
       "execution_start_time": "2025-08-14T10:08:18.4105493Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "58a9ce86-5ac8-49cd-ac3f-dc33858d7c9a",
       "queued_time": "2025-08-14T10:08:08.1920656Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e195ff-626b-4955-9c59-b89ed1e4b7fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:08:19.2301809Z",
       "execution_start_time": "2025-08-14T10:08:18.8180534Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b30b3891-4830-4e92-b5b3-c237d50172fe",
       "queued_time": "2025-08-14T10:08:10.5165465Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLV_MODULE_NAME = \"transform.mlv\"\n",
    "ETL_INSERTONLY_MODULE_NAME = \"transform.silver.insertonly\"\n",
    "ETL_SCD2_MODULE_NAME = \"transform.silver.scd2\"\n",
    "\n",
    "ALLOWED_MODULES = {\n",
    "    MLV_MODULE_NAME,\n",
    "    ETL_INSERTONLY_MODULE_NAME,\n",
    "    ETL_SCD2_MODULE_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3ec6a-9e8d-4f91-8338-c37c9053d8b2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:09:02.0555043Z",
       "execution_start_time": "2025-08-14T10:09:01.6832784Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4a537723-fcad-43c9-ac8b-d8ace392356c",
       "queued_time": "2025-08-14T10:09:01.6820437Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transform.mlv']\n"
     ]
    }
   ],
   "source": [
    "if IS_DEV:\n",
    "    VERSION = \"1.0.1\"\n",
    "    MODULES = \"transform.mlv\"\n",
    "    LAKEHOUSE_CODE_ABFSS = f\"abfss://b50e79c4-1140-479c-b5c1-225982ea6783@onelake.dfs.fabric.microsoft.com/c7ab73ce-7e20-45c4-9bbf-004d37143759\"\n",
    "\n",
    "if \",\" in MODULES:\n",
    "    MODULES = MODULES.split(\",\")\n",
    "else:\n",
    "    MODULES = [MODULES]\n",
    "\n",
    "assert all(module in ALLOWED_MODULES for module in MODULES), f\"All modules should be in {allowed_modules}\"\n",
    "\n",
    "print(MODULES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4beaa-faeb-4d77-a738-ec26a17645da",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080d134c-53f4-4571-9699-312a82f31ba9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:11:21.6372091Z",
       "execution_start_time": "2025-08-14T10:11:21.2655529Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b0362c27-2b75-4df0-89cd-0e889d555e6e",
       "queued_time": "2025-08-14T10:11:21.2644239Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_module_code(name: str, code: str) -> None:\n",
    "    path = f\"{LAKEHOUSE_CODE_ABFSS}/Files/py-packages/fabricengineer/{VERSION}/{name}.py.txt\"\n",
    "    notebookutils.fs.put(path, code, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5b5c0a-ade5-4b25-aff7-2b7de4a7e0c4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:11:23.5527053Z",
       "execution_start_time": "2025-08-14T10:11:23.1840921Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6af8bb11-6a26-4536-bd79-09d6386f0f62",
       "queued_time": "2025-08-14T10:11:23.1831081Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"https://raw.githubusercontent.com/enricogoerlitz/fabricengineer-py/refs/tags/{VERSION}/src/fabricengineer/import_module/import_module.py\"\n",
    "resp = requests.get(url)\n",
    "code = resp.text\n",
    "\n",
    "exec(code, globals())  # This provides the 'import_module' function\n",
    "assert code.startswith(\"import requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fee6a4-403b-4ddb-9d39-576eb8b5afac",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:12:39.5127993Z",
       "execution_start_time": "2025-08-14T10:12:36.9210096Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b8ffff0c-41f8-41ba-bcc4-bc84ed2d7198",
       "queued_time": "2025-08-14T10:12:36.9198791Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlv_module = None\n",
    "scd2_module = None\n",
    "insertonly_module = None\n",
    "\n",
    "if MLV_MODULE_NAME in MODULES:\n",
    "    mlv_module = import_module(MLV_MODULE_NAME, VERSION)\n",
    "    save_module_code(MLV_MODULE_NAME, mlv_module)\n",
    "\n",
    "if ETL_INSERTONLY_MODULE_NAME in MODULES:\n",
    "    insertonly_module = import_module(ETL_INSERTONLY_MODULE_NAME, VERSION)\n",
    "    save_module_code(ETL_INSERTONLY_MODULE_NAME, insertonly_module)\n",
    "\n",
    "if ETL_SCD2_MODULE_NAME in MODULES:\n",
    "    scd2_module = import_module(ETL_SCD2_MODULE_NAME, VERSION)\n",
    "    save_module_code(ETL_SCD2_MODULE_NAME, scd2_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94548e35-5c46-4839-98c0-0167246ca142",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c2414f-264e-4db2-9a86-043e4741a447",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:12:44.1516587Z",
       "execution_start_time": "2025-08-14T10:12:43.7872866Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2c21b63c-c495-457a-b66b-abc31045c5c2",
       "queued_time": "2025-08-14T10:12:43.786203Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 17,
       "statement_ids": [
        17
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 17, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IS_DEV:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47631fb4-5551-48cd-859b-0238a5ef90f0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Export payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5a63c9-8e6b-4d70-b37d-9bedea96514e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-14T10:12:49.2449443Z",
       "execution_start_time": "2025-08-14T10:12:48.9075444Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d5308a0b-25ba-4b0b-a4f7-621ad7f15cad",
       "queued_time": "2025-08-14T10:12:48.9063994Z",
       "session_id": "85ef6a09-31a9-4c32-8746-9d93bec4ad32",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 18,
       "statement_ids": [
        18
       ]
      },
      "text/plain": [
       "StatementMeta(, 85ef6a09-31a9-4c32-8746-9d93bec4ad32, 18, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modules': {'transform.mlv': 'from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\n\\n\\ndef to_spark_sql(sql: str) -> str:\\n    return sql \\\\\\n            .replace(\"[\", \"`\") \\\\\\n            .replace(\"]\", \"`\")\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=kwargs.get(\"is_testing_mock\", False)\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> \\'MaterializedLakeView\\':\\n        \"\"\"Initializes the MaterializedLakeView instance.\\n\\n        Args:\\n            lakehouse (str): The lakehouse name.\\n            schema (str): The schema name.\\n            table (str): The table name.\\n            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\\n            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\\n            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\\n            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\\n\\n        Returns:\\n            MaterializedLakeView: The initialized MaterializedLakeView instance.\\n        \"\"\"\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = kwargs.get(\"is_testing_mock\", False)\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        \"\"\"Initializes the SparkSession instance.\\n        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global \\'spark\\' variable.\\n\\n        Args:\\n            spark_ (SparkSession): The SparkSession instance.\\n\\n        Returns:\\n            SparkSession | None: The initialized SparkSession instance or None.\\n        \"\"\"\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        \"\"\"Initializes the NotebookUtils instance.\\n        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global \\'notebookutils\\' variable.\\n\\n        Args:\\n            notebookutils_ (Any): The NotebookUtils instance.\\n\\n        Returns:\\n            Any | None: The initialized NotebookUtils instance or None.\\n        \"\"\"\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        \"\"\"Reads the content of the SQL file from the specified lakehouse.\\n        If the file does not exist, it returns None.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be read.\\n\\n        Returns:\\n            str | None: The content of the file or None if it doesn\\'t exist.\\n        \"\"\"\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \"r\") as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        \"\"\"Writes the SQL content to the specified file in a lakehouse.\\n        If the file already exists, it will be overwritten.\\n        If the file cannot be written, it raises a RuntimeError.\\n\\n        Args:\\n            sql (str): The SQL content to write.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be written.\\n\\n        Returns:\\n            bool: True if the file was written successfully, False otherwise.\\n        \"\"\"\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> DataFrame | None:\\n        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        logger.info(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame | None:\\n        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\\n        self.create_schema()\\n\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n        logger.info(f\"CREATE MLV: {self.table_path}\")\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        logger.info(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\\n\\n        Args:\\n            sql (str): The SQL query to create the MLV.\\n            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\\n\\n        Returns:\\n            DataFrame: The result of the create or replace operation.\\n        \"\"\"\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            logger.info(\"Nothing has changed.\")\\n            return None\\n\\n        logger.info(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        logger.info(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()', 'transform.silver.scd2': None, 'transform.silver.insertonly': None}}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"modules\": {\n",
    "        MLV_MODULE_NAME: mlv_module,\n",
    "        ETL_SCD2_MODULE_NAME: scd2_module,\n",
    "        ETL_INSERTONLY_MODULE_NAME: insertonly_module\n",
    "    }\n",
    "}\n",
    "\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f153b863-d7a3-470a-abb3-fe62d9c2906f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-08T13:02:12.7208349Z",
       "execution_start_time": "2025-08-08T13:02:11.2519111Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7b0d2bb1-ed0c-4d63-bcaf-dec8ac585985",
       "queued_time": "2025-08-08T13:01:57.0827827Z",
       "session_id": "a94a6196-327b-4b4d-863a-2bc5c2c39026",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, a94a6196-327b-4b4d-863a-2bc5c2c39026, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExitValue: {'modules': {'transform.mlv': 'from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\n\\n\\ndef to_spark_sql(sql: str) -> str:\\n    return sql \\\\\\n            .replace(\"[\", \"`\") \\\\\\n            .replace(\"]\", \"`\")\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=kwargs.get(\"is_testing_mock\", False)\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        **kwargs\\n    ) -> \\'MaterializedLakeView\\':\\n        \"\"\"Initializes the MaterializedLakeView instance.\\n\\n        Args:\\n            lakehouse (str): The lakehouse name.\\n            schema (str): The schema name.\\n            table (str): The table name.\\n            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\\n            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\\n            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\\n            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\\n\\n        Returns:\\n            MaterializedLakeView: The initialized MaterializedLakeView instance.\\n        \"\"\"\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = kwargs.get(\"is_testing_mock\", False)\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        \"\"\"Initializes the SparkSession instance.\\n        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global \\'spark\\' variable.\\n\\n        Args:\\n            spark_ (SparkSession): The SparkSession instance.\\n\\n        Returns:\\n            SparkSession | None: The initialized SparkSession instance or None.\\n        \"\"\"\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        \"\"\"Initializes the NotebookUtils instance.\\n        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global \\'notebookutils\\' variable.\\n\\n        Args:\\n            notebookutils_ (Any): The NotebookUtils instance.\\n\\n        Returns:\\n            Any | None: The initialized NotebookUtils instance or None.\\n        \"\"\"\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        \"\"\"Reads the content of the SQL file from the specified lakehouse.\\n        If the file does not exist, it returns None.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be read.\\n\\n        Returns:\\n            str | None: The content of the file or None if it doesn\\'t exist.\\n        \"\"\"\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \"r\") as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        \"\"\"Writes the SQL content to the specified file in a lakehouse.\\n        If the file already exists, it will be overwritten.\\n        If the file cannot be written, it raises a RuntimeError.\\n\\n        Args:\\n            sql (str): The SQL content to write.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be written.\\n\\n        Returns:\\n            bool: True if the file was written successfully, False otherwise.\\n        \"\"\"\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> DataFrame | None:\\n        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        logger.info(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame | None:\\n        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\\n        self.create_schema()\\n\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n        logger.info(f\"CREATE MLV: {self.table_path}\")\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        logger.info(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\\n\\n        Args:\\n            sql (str): The SQL query to create the MLV.\\n            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\\n\\n        Returns:\\n            DataFrame: The result of the create or replace operation.\\n        \"\"\"\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            logger.info(\"Nothing has changed.\")\\n            return None\\n\\n        logger.info(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        logger.info(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()', 'transform.silver.scd2': None, 'transform.silver.insertonly': None}}"
     ]
    }
   ],
   "source": [
    "payload_str = json.dumps(payload)\n",
    "notebookutils.notebook.exit(payload_str)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c7ab73ce-7e20-45c4-9bbf-004d37143759",
    "default_lakehouse_name": "GlobalUtilsLakehouse",
    "default_lakehouse_workspace_id": "b50e79c4-1140-479c-b5c1-225982ea6783",
    "known_lakehouses": [
     {
      "id": "c7ab73ce-7e20-45c4-9bbf-004d37143759"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
