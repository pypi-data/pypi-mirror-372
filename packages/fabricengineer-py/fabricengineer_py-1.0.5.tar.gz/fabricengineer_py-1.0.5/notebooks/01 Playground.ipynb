{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81b363a",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648232ba",
   "metadata": {},
   "source": [
    "## Initialize globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0c3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from uuid import uuid4\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from fabricengineer.transform.mlv.mlv import MaterializedLakeView\n",
    "from fabricengineer.transform.silver import (\n",
    "    SilverIngestionInsertOnlyService,\n",
    "    SilverIngestionSCD2Service\n",
    ")\n",
    "from fabricengineer.transform.silver.utils import (\n",
    "    LakehouseTable,\n",
    "    get_mock_table_path\n",
    ")\n",
    "from fabricengineer.logging import TimeLogger, logger\n",
    "\n",
    "mlv: MaterializedLakeView\n",
    "timer: TimeLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965e651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:42:40 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.7 instead (on interface en0)\n",
      "25/08/06 13:42:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/enricogoerlitz/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/enricogoerlitz/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-61793a18-a2e8-425a-962d-ccda52e479ef;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: resolution report :: resolve 84ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-61793a18-a2e8-425a-962d-ccda52e479ef\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/08/06 13:42:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "class NotebookUtilsFSMock:\n",
    "    def _get_path(self, file: str) -> str:\n",
    "        return os.path.join(os.getcwd(), file)\n",
    "\n",
    "    def exists(self, path: str) -> bool:\n",
    "        return os.path.exists(self._get_path(path))\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        file: str,\n",
    "        content: str,\n",
    "        overwrite: bool = False\n",
    "    ) -> None:\n",
    "        path = self._get_path(file)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        if os.path.exists(path) and not overwrite:\n",
    "            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "class NotebookUtilsMock:\n",
    "    def __init__(self):\n",
    "        self.fs = NotebookUtilsFSMock()\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"TestSession\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "global spark\n",
    "spark: SparkSession = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "global notebookutils\n",
    "notebookutils = NotebookUtilsMock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6173fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "from typing import Any, Callable\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def capture_logs(logger: logging.Logger):\n",
    "    log_stream = io.StringIO()\n",
    "    handler = logging.StreamHandler(log_stream)\n",
    "    handler.setLevel(logging.DEBUG)  # Fang alles ab\n",
    "    formatter = logging.Formatter(\"[%(asctime)s] [%(levelname)s] %(filename)s %(message)s\", \"%d.%m.%Y %H:%M:%S,%f\")\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(handler)\n",
    "    try:\n",
    "        yield log_stream\n",
    "    finally:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "def sniff_logs(logger: logging.Logger, fn: Callable[[], Any]) -> tuple[Any, list[str]]:\n",
    "    with capture_logs(logger) as log_stream:\n",
    "        result = fn()\n",
    "    logs = log_stream.getvalue().splitlines()\n",
    "    return result, logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8d5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def cleanup_fs():\n",
    "    path_Files = notebookutils.fs._get_path(\"Files\")\n",
    "    path_tmp = notebookutils.fs._get_path(\"tmp\")\n",
    "    path_tmp_2 = \"../tmp\"\n",
    "    path_tmp_3 = \"../Files\"\n",
    "\n",
    "    rm_paths = [path_Files, path_tmp, path_tmp_2, path_tmp_3]\n",
    "    for path in rm_paths:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "cleanup_fs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d2e7c",
   "metadata": {},
   "source": [
    "## TimeLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c8a6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeLogger(start_time=None, end_time=None, elapsed_time=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../src/fabricengineer/logging/timer.py\") as f:\n",
    "    code = f.read()\n",
    "exec(code, globals())\n",
    "\n",
    "timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d295e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 10:50:47] [INFO] fabricengineer TIMER-START:\t2025-08-05 10:50:47\n",
      "[05.08.2025 10:50:48] [INFO] fabricengineer TIMER-END:\t2025-08-05 10:50:48, ELAPSED: 1.0036s\n"
     ]
    }
   ],
   "source": [
    "timer.start().log()\n",
    "time.sleep(1)\n",
    "timer.stop().log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076f2b1",
   "metadata": {},
   "source": [
    "## MaterializedLakeView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba1bbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lakehouse': 'Lakehouse',\n",
       " 'schema': 'schema',\n",
       " 'table': 'table',\n",
       " 'table_path': 'Lakehouse.schema.table'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../src/fabricengineer/transform/mlv/mlv.py\") as f:\n",
    "    code = f.read()\n",
    "exec(code, globals())\n",
    "\n",
    "\n",
    "mlv.init(\n",
    "    lakehouse=\"Lakehouse\",\n",
    "    schema=\"schema\",\n",
    "    table=\"table\",\n",
    "    table_suffix=None,\n",
    "    is_testing_mock=True\n",
    ")\n",
    "\n",
    "mlv.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d270d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 10:50:48] [INFO] fabricengineer CREATE SCHEMA IF NOT EXISTS b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema\n",
      "[05.08.2025 10:50:48] [INFO] fabricengineer CREATE MLV: b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[05.08.2025 10:50:48,f] [INFO] <string> CREATE SCHEMA IF NOT EXISTS b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema',\n",
       " '[05.08.2025 10:50:48,f] [INFO] <string> CREATE MLV: b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 10:50:48] [INFO] fabricengineer Nothing has changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[05.08.2025 10:50:48,f] [INFO] <string> Nothing has changed.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 10:50:48] [INFO] fabricengineer REPLACE MLV: b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table\n",
      "[05.08.2025 10:50:48] [INFO] fabricengineer DROP MATERIALIZED LAKE VIEW IF EXISTS b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table\n",
      "[05.08.2025 10:50:48] [INFO] fabricengineer CREATE SCHEMA IF NOT EXISTS b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema\n",
      "[05.08.2025 10:50:48] [INFO] fabricengineer CREATE MLV: b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs-3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[05.08.2025 10:50:48,f] [INFO] <string> REPLACE MLV: b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table',\n",
       " '[05.08.2025 10:50:48,f] [INFO] <string> DROP MATERIALIZED LAKE VIEW IF EXISTS b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table',\n",
       " '[05.08.2025 10:50:48,f] [INFO] <string> CREATE SCHEMA IF NOT EXISTS b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema',\n",
       " '[05.08.2025 10:50:48,f] [INFO] <string> CREATE MLV: b889b89c-d0a4-4ae2-be2b-aa31db3f0afc.schema.table']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 10:50:48] [INFO] fabricengineer Nothing has changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[05.08.2025 10:50:48,f] [INFO] <string> Nothing has changed.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 10:51:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "mlv.init(\n",
    "    lakehouse=str(uuid4()),\n",
    "    schema=\"schema\",\n",
    "    table=\"table\",\n",
    "    table_suffix=None,\n",
    "    is_testing_mock=True\n",
    ")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT * FROM Lakehouse.schema.table\n",
    "\"\"\"\n",
    "is_existing = False\n",
    "for i in range(0, 4):\n",
    "    if i > 0:\n",
    "        is_existing = True\n",
    "    if i == 2:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM Lakehouse.schema.table WHERE 1=0\n",
    "        \"\"\"\n",
    "    result, logs = sniff_logs(\n",
    "        logger,\n",
    "        lambda: mlv.create_or_replace(sql, mock_is_existing=is_existing),\n",
    "    )\n",
    "    print(f\"Logs-{i+1}\")\n",
    "    display(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb398d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlv.file_path\n",
    "os.remove(mlv.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922dca13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WARN: file=None, is_existing=True. RECREATE.',\n",
       " 'DROP MATERIALIZED LAKE VIEW IF EXISTS d0f584a9-ad2b-41b5-92f8-e4d8dddce28a.schema.table',\n",
       " 'CREATE SCHEMA IF NOT EXISTS d0f584a9-ad2b-41b5-92f8-e4d8dddce28a.schema',\n",
       " 'CREATE MLV: d0f584a9-ad2b-41b5-92f8-e4d8dddce28a.schema.table']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, logs = sniff_logs(\n",
    "    lambda: mlv.create_or_replace(sql, mock_is_existing=True)\n",
    ")\n",
    "\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5235b494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REFRESH MATERIALIZED LAKE VIEW d0f584a9-ad2b-41b5-92f8-e4d8dddce28a.schema.table FULL']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, logs = sniff_logs(\n",
    "    lambda: mlv.refresh(full_refresh=True)\n",
    ")\n",
    "\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110dd14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REFRESH MATERIALIZED LAKE VIEW d0f584a9-ad2b-41b5-92f8-e4d8dddce28a.schema.table ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, logs = sniff_logs(\n",
    "    lambda: mlv.refresh(full_refresh=False)\n",
    ")\n",
    "\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6be1e",
   "metadata": {},
   "source": [
    "## Clean up the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8f3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_fs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578946d",
   "metadata": {},
   "source": [
    "## SilverIngestionInsertOnlyService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170d66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_table = LakehouseTable(\n",
    "    lakehouse=\"BronzeLakehouse\",\n",
    "    schema=\"schema\",\n",
    "    table=\"table1\"\n",
    ")\n",
    "dest_table = LakehouseTable(\n",
    "    lakehouse=\"SilverLakehouse\",\n",
    "    schema=src_table.schema,\n",
    "    table=src_table.table\n",
    ")\n",
    "\n",
    "etl = SilverIngestionInsertOnlyService()\n",
    "etl.init(\n",
    "    spark_=spark,\n",
    "    source_table=src_table,\n",
    "    destination_table=dest_table,\n",
    "    nk_columns=[\"id\"],\n",
    "    constant_columns=[],\n",
    "    is_delta_load=False,\n",
    "    delta_load_use_broadcast=True,\n",
    "    transformations={},\n",
    "    exclude_comparing_columns=None,\n",
    "    include_comparing_columns=None,\n",
    "    historize=True,\n",
    "    partition_by_columns=None,\n",
    "    is_testing_mock=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6ae959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:17:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+-------------------+-------------------+\n",
      "|id |name     |department_id|created_at         |updated_at         |\n",
      "+---+---------+-------------+-------------------+-------------------+\n",
      "|1  |Alice    |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|2  |u-Bob    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|3  |u-Charlie|3            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|4  |David    |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|5  |Eve      |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|6  |Frank    |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|8  |Heidi    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|9  |Ivan     |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|10 |Judy     |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|11 |Judy-2   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|12 |Judy-3   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|13 |Judy-4   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "+---+---------+-------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:17:06 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"department_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"created_at\", T.StringType(), False),\n",
    "    T.StructField(\"updated_at\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 1, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (2, \"u-Bob\", 2, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (3, \"u-Charlie\", 3, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (4, \"David\", 1, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (5, \"Eve\", 2, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (6, \"Frank\", 3, \"2023-01-01\", \"2023-01-01\"),\n",
    "    # (7, \"Grace\", 1, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (8, \"Heidi\", 2, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (9, \"Ivan\", 3, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (10, \"Judy\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "    ,(11, \"Judy-2\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "    ,(12, \"Judy-3\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "    ,(13, \"Judy-4\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "]\n",
    "\n",
    "df_bronze = spark.createDataFrame(data, schema)\n",
    "df_bronze = df_bronze \\\n",
    "    .withColumn(\"created_at\", F.to_timestamp(\"created_at\")) \\\n",
    "    .withColumn(\"updated_at\",F.to_timestamp(\"updated_at\"))\n",
    "\n",
    "df_bronze.show(truncate=False)\n",
    "bronze_path = get_mock_table_path(etl._src_table)\n",
    "df_bronze.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2c6083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06.08.2025 13:17:11] [INFO] fabricengineer DROP MATERIALIZED LAKE VIEW IF EXISTS SilverLakehouse.schema.table1_h\n",
      "[06.08.2025 13:17:11] [INFO] fabricengineer MLV: CREATE MLV SilverLakehouse.schema.table1_h\n",
      "[06.08.2025 13:17:11] [INFO] fabricengineer CREATE SCHEMA IF NOT EXISTS SilverLakehouse.schema\n",
      "[06.08.2025 13:17:11] [INFO] fabricengineer CREATE MLV: SilverLakehouse.schema.table1_h\n",
      "[06.08.2025 13:17:11] [INFO] fabricengineer REFRESH MATERIALIZED LAKE VIEW SilverLakehouse.schema.table1_h \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---+---+---------+-------------+-------------------+-------------------+--------------------------+--------------+\n",
      "|PK                                  |NK |id |name     |department_id|created_at         |updated_at         |ROW_LOAD_DTS              |ROW_DELETE_DTS|\n",
      "+------------------------------------+---+---+---------+-------------+-------------------+-------------------+--------------------------+--------------+\n",
      "|2d2c76c7-deea-454d-8b41-1d1b64882410|1  |1  |Alice    |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|b6f90e50-df64-4638-a8f4-c41faba62ef8|2  |2  |u-Bob    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|2c7d45ec-2d25-4404-b5fb-12d9d269d5b9|3  |3  |u-Charlie|3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|081047ac-f760-4dbe-aafa-d789cbabeb43|4  |4  |David    |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|b4255f0f-db74-4a44-b960-8af8cee5453c|5  |5  |Eve      |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|e5a25450-ba1c-421f-b505-965100f45068|6  |6  |Frank    |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|99536465-1851-4890-bb44-89c52bdee8ad|8  |8  |Heidi    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|a051518b-a35f-4a9b-9aa7-e5c24b555286|9  |9  |Ivan     |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|e8deb1cc-9572-45ce-bf4c-9908f735c553|10 |10 |Judy     |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|1489a746-0b24-4170-97a9-74bf5dcd7421|11 |11 |Judy-2   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|557a2e2b-7987-415c-9718-1a0aba1b1757|12 |12 |Judy-3   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "|6c1c42f5-2d94-4add-934b-0119da6ea34a|13 |13 |Judy-4   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:17:09.164361|NULL          |\n",
      "+------------------------------------+---+---+---------+-------------+-------------------+-------------------+--------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = etl.ingest()\n",
    "new_data.orderBy(\"id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---+---+---------+-------------+-------------------+-------------------+--------------------------+--------------+\n",
      "|PK                                  |NK |id |name     |department_id|created_at         |updated_at         |ROW_LOAD_DTS              |ROW_DELETE_DTS|\n",
      "+------------------------------------+---+---+---------+-------------+-------------------+-------------------+--------------------------+--------------+\n",
      "|d85b9efb-4c1b-47af-9ab6-00a68518278c|1  |1  |Alice    |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|a6ab88ad-8d0c-490f-9ef5-6c4581543a1d|2  |2  |u-Bob    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|2e2885c8-d923-4034-8a1a-5e62fb4a4d5d|3  |3  |u-Charlie|3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|8b5020b8-7226-4673-a167-77b121001f64|4  |4  |David    |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|27898553-3042-4605-ab19-f59690877958|5  |5  |Eve      |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|36b5199c-b053-4281-b43f-ea38c2e3ac79|6  |6  |Frank    |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|4b7a264d-11a0-469c-929f-2c3fd155b0b1|8  |8  |Heidi    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|12ea9c11-84dd-452a-bc21-66aa692f704a|9  |9  |Ivan     |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|776e1ef4-137e-46bc-baaf-25fcb27e1e74|10 |10 |Judy     |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|458186d7-89af-4243-a0b4-25611f7d4e29|11 |11 |Judy-2   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|add75f67-55bf-4ccf-b298-7cd3d52609ed|12 |12 |Judy-3   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "|f1628125-b6a0-4e03-b9f3-cf1929187d63|13 |13 |Judy-4   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-01 14:51:19.276059|NULL          |\n",
      "+------------------------------------+---+---+---------+-------------+-------------------+-------------------+--------------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 14:51:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "silver_path = get_mock_table_path(etl._dest_table)\n",
    "df = spark.read.format(\"parquet\").load(silver_path).orderBy(F.col(\"id\").asc(), F.col(\"ROW_LOAD_DTS\").asc())\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858c34c",
   "metadata": {},
   "source": [
    "## SilverIngestionSCD2Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5074ca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'historize': True, 'is_delta_load': False, 'delta_load_use_broadcast': True, 'src_table_path': 'BronzeLakehouse.schema.tablescd2', 'dist_table_path': 'SilverLakehouse.schema.tablescd2', 'nk_columns': ['id'], 'include_comparing_columns': [], 'exclude_comparing_columns': {'ROW_LOAD_DTS', 'id', 'PK', 'ROW_UPDATE_DTS', 'ROW_IS_CURRENT', 'NK', 'ROW_DELETE_DTS'}, 'transformations': {}, 'constant_columns': [], 'partition_by': [], 'pk_column': 'PK', 'nk_column': 'NK', 'nk_column_concate_str': '_', 'row_load_dts_column': 'ROW_LOAD_DTS', 'row_update_dts_column': 'ROW_UPDATE_DTS', 'row_delete_dts_column': 'ROW_DELETE_DTS', 'dw_columns': ['PK', 'NK', 'ROW_IS_CURRENT', 'ROW_UPDATE_DTS', 'ROW_DELETE_DTS', 'ROW_LOAD_DTS']}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_table = LakehouseTable(\n",
    "    lakehouse=\"BronzeLakehouse\",\n",
    "    schema=\"schema\",\n",
    "    table=\"tablescd2\"\n",
    ")\n",
    "dest_table = LakehouseTable(\n",
    "    lakehouse=\"SilverLakehouse\",\n",
    "    schema=src_table.schema,\n",
    "    table=src_table.table\n",
    ")\n",
    "\n",
    "etl = SilverIngestionSCD2Service()\n",
    "etl.init(\n",
    "    spark_=spark,\n",
    "    source_table=src_table,\n",
    "    destination_table=dest_table,\n",
    "    nk_columns=[\"id\"],\n",
    "    constant_columns=[],\n",
    "    is_delta_load=False,\n",
    "    delta_load_use_broadcast=True,\n",
    "    transformations={},\n",
    "    exclude_comparing_columns=None,\n",
    "    include_comparing_columns=None,\n",
    "    historize=True,\n",
    "    partition_by_columns=None,\n",
    "    is_testing_mock=True\n",
    ")\n",
    "\n",
    "str(etl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a77588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+-------------------+-------------------+\n",
      "|id |name   |department_id|created_at         |updated_at         |\n",
      "+---+-------+-------------+-------------------+-------------------+\n",
      "|1  |Alice  |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|2  |Bob    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|3  |Charlie|3            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|4  |David  |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|5  |Eve    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|6  |Frank  |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|7  |Grace  |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|8  |Heidi  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|9  |Ivan   |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "|10 |Judy   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|\n",
      "+---+-------+-------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:42:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"department_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"created_at\", T.StringType(), False),\n",
    "    T.StructField(\"updated_at\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 1, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (2, \"Bob\", 2, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (3, \"Charlie\", 3, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (4, \"David\", 1, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (5, \"Eve\", 2, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (6, \"Frank\", 3, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (7, \"Grace\", 1, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (8, \"Heidi\", 2, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (9, \"Ivan\", 3, \"2023-01-01\", \"2023-01-01\"),\n",
    "    (10, \"Judy\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "    # ,(11, \"Judy-2\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "    # ,(12, \"Judy-3\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "    # ,(13, \"Judy-4\", 1, \"2023-01-01\", \"2023-01-01\")\n",
    "]\n",
    "\n",
    "df_bronze = spark.createDataFrame(data, schema)\n",
    "df_bronze = df_bronze \\\n",
    "    .withColumn(\"created_at\", F.to_timestamp(\"created_at\")) \\\n",
    "    .withColumn(\"updated_at\",F.to_timestamp(\"updated_at\"))\n",
    "\n",
    "df_bronze.show(truncate=False)\n",
    "bronze_path = get_mock_table_path(etl._src_table)\n",
    "df_bronze.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b5aeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:42:53 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "etl.ingest()\n",
    "\n",
    "df_silver = etl.read_silver_df()\n",
    "# df_silver.orderBy(F.col(\"id\").asc(), F.col(\"ROW_LOAD_DTS\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c6f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---+---+-------+-------------+-------------------+-------------------+--------------------------+--------------------------+--------------------------+--------------+\n",
      "|PK                                  |NK |id |name   |department_id|created_at         |updated_at         |ROW_LOAD_DTS              |ROW_UPDATE_DTS            |ROW_DELETE_DTS            |ROW_IS_CURRENT|\n",
      "+------------------------------------+---+---+-------+-------------+-------------------+-------------------+--------------------------+--------------------------+--------------------------+--------------+\n",
      "|409e88bb-b18f-4d67-b5f9-0aa8f274aea0|1  |1  |Alice  |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|e6c435e4-8bf3-4543-bd08-025bd8dc35f7|2  |2  |Bob    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|43a53451-cd1f-4032-ab60-7f461392a1f3|2  |2  |u-Bob  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:58.198339|NULL                      |NULL                      |1             |\n",
      "|a06c3710-6ff1-4799-92f9-f6cd0f08405e|2  |2  |u-Bob  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:35:04.181623|NULL                      |NULL                      |1             |\n",
      "|aff5d097-dda2-4454-96b2-464488bb7060|2  |2  |u-Bob  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:35:16.464314|NULL                      |NULL                      |1             |\n",
      "|13eddc8e-a1c3-4fdf-af76-178b661346c9|3  |3  |Charlie|3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|2dc4b1c6-950f-48d3-a989-f475350623a6|4  |4  |David  |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|2f5c7402-dd8a-47b1-a666-3ed5d5e37233|5  |5  |Eve    |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|b64eabbd-5069-4132-ae47-1157c43d7f47|5  |5  |u-Eve  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:58.198339|NULL                      |NULL                      |1             |\n",
      "|7df7101a-8adf-41a7-825e-c0763557aef3|5  |5  |u-Eve  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:35:04.181623|NULL                      |NULL                      |1             |\n",
      "|7361d950-8932-4687-aa9f-991d08fd0a48|5  |5  |u-Eve  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:35:16.464314|NULL                      |NULL                      |1             |\n",
      "|caf086a6-a822-463b-92ba-6bf1416322ac|6  |6  |Frank  |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|8612f46f-1ef7-45e7-98af-2ac90aff0f3d|7  |7  |Grace  |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|fea87cbe-c633-4c7d-ab07-f1073a801393|8  |8  |Heidi  |2            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|01779193-074d-40a3-af73-49da9fc710e3|9  |9  |Ivan   |3            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|2025-08-06 13:34:58.198339|2025-08-06 13:34:58.198339|0             |\n",
      "|271c833b-ed8a-4946-9f0e-6c5d1d07cf97|10 |10 |Judy   |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:30.134193|NULL                      |NULL                      |1             |\n",
      "|519310c6-e316-4adb-afe5-45b9e2146724|11 |11 |Judy-2 |1            |2023-01-01 00:00:00|2023-01-01 00:00:00|2025-08-06 13:34:58.198339|NULL                      |NULL                      |1             |\n",
      "+------------------------------------+---+---+-------+-------------+-------------------+-------------------+--------------------------+--------------------------+--------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver.orderBy(F.col(\"id\").asc(), F.col(\"ROW_LOAD_DTS\").asc()).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
