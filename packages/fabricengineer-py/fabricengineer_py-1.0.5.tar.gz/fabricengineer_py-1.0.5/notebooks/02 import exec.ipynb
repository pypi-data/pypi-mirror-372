{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43180ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from fabricengineer.import_module.import_module import import_module\n",
    "\n",
    "VERSION = \"0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dbac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 13:30:05 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.7 instead (on interface en0)\n",
      "25/08/07 13:30:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/07 13:30:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/07 13:30:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class NotebookUtilsFSMock:\n",
    "    def _get_path(self, file: str) -> str:\n",
    "        return os.path.join(os.getcwd(), file)\n",
    "\n",
    "    def exists(self, path: str) -> bool:\n",
    "        return os.path.exists(self._get_path(path))\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        file: str,\n",
    "        content: str,\n",
    "        overwrite: bool = False\n",
    "    ) -> None:\n",
    "        path = self._get_path(file)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        if os.path.exists(path) and not overwrite:\n",
    "            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "class NotebookUtilsMock:\n",
    "    def __init__(self):\n",
    "        self.fs = NotebookUtilsFSMock()\n",
    "\n",
    "global spark\n",
    "spark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\n",
    "\n",
    "global notebookutils\n",
    "notebookutils = NotebookUtilsMock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3602f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import logging\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import Any, Callable\n",
      "from abc import ABC, abstractmethod\n",
      "from dataclasses import dataclass\n",
      "from uuid import uuid4\n",
      "\n",
      "from pyspark.sql import (\n",
      "    SparkSession,\n",
      "    DataFrame,\n",
      "    functions as F,\n",
      "    types as T,\n",
      "    Window\n",
      ")\n",
      "\n",
      "\n",
      "logging.getLogger(\"py4j\").propagate = False\n",
      "\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO,\n",
      "    format=\"[%(asctime)s] [%(levelname)s] %(name)s %(message)s\",\n",
      "    datefmt=\"%d.%m.%Y %H:%M:%S\"\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"fabricengineer\")\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class LakehouseTable:\n",
      "    lakehouse: str\n",
      "    schema: str\n",
      "    table: str\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        return f\"{self.lakehouse}.{self.schema}.{self.table}\"\n",
      "\n",
      "\n",
      "@F.udf(returnType=T.StringType())\n",
      "def generate_uuid():\n",
      "    \"\"\"Generate a UUID4 for spark column.\"\"\"\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class ConstantColumn:\n",
      "    \"\"\"Class for adding a column with constant value to etl\"\"\"\n",
      "    name: str\n",
      "    value: str\n",
      "    part_of_nk: bool = False\n",
      "\n",
      "    def __post_init__(self):\n",
      "        \"\"\"\n",
      "        Nach initialisierung wird der name in UPPERCASE umgewandelt.\n",
      "        \"\"\"\n",
      "        object.__setattr__(self, \"name\", self.name.upper())\n",
      "\n",
      "\n",
      "def get_mock_table_path(table: LakehouseTable) -> str:\n",
      "    \"\"\"Returns the mock table path for testing purposes.\"\"\"\n",
      "    if table is None:\n",
      "        raise ValueError(\"Table is not initialized.\")\n",
      "    table_path = table.table_path.replace(\".\", \"/\")\n",
      "    full_table_path = f\"tmp/lakehouse/{table_path}\"\n",
      "    return full_table_path\n",
      "\n",
      "\n",
      "class BaseSilverIngestionService(ABC):\n",
      "\n",
      "    @abstractmethod\n",
      "    def init(self, **kwargs): pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def run(self, **kwargs): pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def read_silver_df(self) -> DataFrame: pass\n",
      "\n",
      "\n",
      "class BaseSilverIngestionServiceImpl(BaseSilverIngestionService, ABC):\n",
      "    \"\"\"TODO:\n",
      "    1. validate parameters hier rein\n",
      "        - dann muss man nur seine eigenen validieren\n",
      "    2. InsertOnly base impl\n",
      "    3. InsertOnly base testen\n",
      "    4. SCD2 base impl\n",
      "    5. SCD2 base tests schreiben\n",
      "    6. SCD2 base testen\n",
      "\n",
      "    \"\"\"\n",
      "    _is_initialized: bool = False\n",
      "\n",
      "    @abstractmethod\n",
      "    def run(self, **kwargs): pass\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        *,\n",
      "        spark_: SparkSession,\n",
      "        source_table: LakehouseTable,\n",
      "        destination_table: LakehouseTable,\n",
      "        nk_columns: list[str],\n",
      "        constant_columns: list[ConstantColumn],\n",
      "        is_delta_load: bool,\n",
      "        delta_load_use_broadcast: bool,\n",
      "        transformations: dict,\n",
      "        exclude_comparing_columns: list[str] | None = None,\n",
      "        include_comparing_columns: list[str] | None = None,\n",
      "        historize: bool = True,\n",
      "        partition_by_columns: list[str] = None,\n",
      "        df_bronze: DataFrame = None,\n",
      "        create_historized_mlv: bool = True,\n",
      "        dw_columns: list[str] = None,\n",
      "\n",
      "        pk_column_name: str = \"PK\",\n",
      "        nk_column_name: str = \"NK\",\n",
      "        nk_column_concate_str: str = \"_\",\n",
      "        row_is_current_column: str = \"ROW_IS_CURRENT\",\n",
      "        row_hist_number_column: str = \"ROW_HIST_NUMBER\",\n",
      "        row_update_dts_column: str = \"ROW_UPDATE_DTS\",\n",
      "        row_delete_dts_column: str = \"ROW_DELETE_DTS\",\n",
      "        row_load_dts_column: str = \"ROW_LOAD_DTS\",\n",
      "\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "\n",
      "        self._spark = spark_\n",
      "        self._df_bronze = df_bronze\n",
      "        self._historize = historize\n",
      "        self._is_create_hist_mlv = create_historized_mlv\n",
      "        self._is_delta_load = is_delta_load\n",
      "        self._delta_load_use_broadcast = delta_load_use_broadcast\n",
      "        self._src_table = source_table\n",
      "        self._dest_table = destination_table\n",
      "        self._nk_columns = nk_columns\n",
      "        self._include_comparing_columns = include_comparing_columns\n",
      "\n",
      "        self._exclude_comparing_columns: list[str] = exclude_comparing_columns or []\n",
      "        self._transformations: dict[str, Callable] = transformations or {}\n",
      "        self._constant_columns: list[ConstantColumn] = constant_columns or []\n",
      "        self._partition_by: list[str] = partition_by_columns or []\n",
      "        self._dw_columns = dw_columns\n",
      "\n",
      "        self._pk_column_name = pk_column_name\n",
      "        self._nk_column_name = nk_column_name\n",
      "        self._nk_column_concate_str = nk_column_concate_str\n",
      "        self._row_hist_number_column = row_hist_number_column\n",
      "        self._row_is_current_column = row_is_current_column\n",
      "        self._row_update_dts_column = row_update_dts_column\n",
      "        self._row_delete_dts_column = row_delete_dts_column\n",
      "        self._row_load_dts_column = row_load_dts_column\n",
      "\n",
      "        self._validate_parameters()\n",
      "        self._set_spark_config()\n",
      "\n",
      "        self._exclude_comparing_columns = set(\n",
      "            [self._pk_column_name]\n",
      "            + self._nk_columns\n",
      "            + self._dw_columns\n",
      "            + self._exclude_comparing_columns\n",
      "            + [column.name for column in self._constant_columns]\n",
      "        )\n",
      "\n",
      "        self._spark.catalog.clearCache()\n",
      "\n",
      "    def _validate_parameters(self) -> None:\n",
      "        \"\"\"Validates the in constructor setted parameters, so the etl can run.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: when a valueerror occurs\n",
      "            TypeError: when a typerror occurs\n",
      "            Exception: generic exception\n",
      "        \"\"\"\n",
      "        if self._df_bronze is not None:\n",
      "            self._validate_param_isinstance(self._df_bronze, \"df_bronze\", DataFrame)\n",
      "\n",
      "        self._validate_param_isinstance(self._spark, \"spark\", SparkSession)\n",
      "        self._validate_param_isinstance(self._historize, \"historize\", bool)\n",
      "        self._validate_param_isinstance(self._is_create_hist_mlv, \"create_historized_mlv\", bool)\n",
      "        self._validate_param_isinstance(self._is_delta_load, \"is_delta_load\", bool)\n",
      "        self._validate_param_isinstance(self._delta_load_use_broadcast, \"delta_load_use_broadcast\", bool)\n",
      "        self._validate_param_isinstance(self._transformations, \"transformations\", dict)\n",
      "        self._validate_param_isinstance(self._src_table, \"src_table\", LakehouseTable)\n",
      "        self._validate_param_isinstance(self._dest_table, \"dest_table\", LakehouseTable)\n",
      "        self._validate_param_isinstance(self._include_comparing_columns, \"include_columns_from_comparing\", list)\n",
      "        self._validate_param_isinstance(self._exclude_comparing_columns, \"exclude_columns_from_comparing\", list)\n",
      "        self._validate_param_isinstance(self._partition_by, \"partition_by_columns\", list)\n",
      "        self._validate_param_isinstance(self._pk_column_name, \"pk_column\", str)\n",
      "        self._validate_param_isinstance(self._nk_column_name, \"nk_column\", str)\n",
      "        self._validate_param_isinstance(self._nk_columns, \"nk_columns\", list)\n",
      "        self._validate_param_isinstance(self._nk_column_concate_str, \"nk_column_concate_str\", str)\n",
      "        self._validate_param_isinstance(self._constant_columns, \"constant_columns\", list)\n",
      "        self._validate_param_isinstance(self._row_load_dts_column, \"row_load_dts_column\", str)\n",
      "        self._validate_param_isinstance(self._row_hist_number_column, \"row_hist_number_column\", str)\n",
      "        self._validate_param_isinstance(self._row_is_current_column, \"row_is_current_column\", str)\n",
      "        self._validate_param_isinstance(self._row_update_dts_column, \"row_update_dts_column\", str)\n",
      "        self._validate_param_isinstance(self._row_delete_dts_column, \"row_delete_dts_column\", str)\n",
      "\n",
      "        self._validate_min_length(self._pk_column_name, \"pk_column\", 2)\n",
      "        self._validate_min_length(self._nk_column_name, \"nk_column\", 2)\n",
      "        self._validate_min_length(self._src_table.lakehouse, \"src_lakehouse\", 3)\n",
      "        self._validate_min_length(self._src_table.schema, \"src_schema\", 1)\n",
      "        self._validate_min_length(self._src_table.table, \"src_tablename\", 3)\n",
      "        self._validate_min_length(self._dest_table.lakehouse, \"dest_lakehouse\", 3)\n",
      "        self._validate_min_length(self._dest_table.schema, \"dest_schema\", 1)\n",
      "        self._validate_min_length(self._dest_table.table, \"dest_tablename\", 3)\n",
      "        self._validate_min_length(self._nk_columns, \"nk_columns\", 1)\n",
      "        self._validate_min_length(self._nk_column_concate_str, \"nk_column_concate_str\", 1)\n",
      "        self._validate_min_length(self._row_load_dts_column, \"row_load_dts_column\", 3)\n",
      "        self._validate_min_length(self._row_hist_number_column, \"row_hist_number_column\", 3)\n",
      "        self._validate_min_length(self._row_is_current_column, \"row_is_current_column\", 3)\n",
      "        self._validate_min_length(self._row_update_dts_column, \"row_update_dts_column\", 3)\n",
      "        self._validate_min_length(self._row_delete_dts_column, \"row_delete_dts_column\", 3)\n",
      "\n",
      "        self._validate_transformations()\n",
      "        self._validate_constant_columns()\n",
      "\n",
      "    def read_silver_df(self, fformat: str = \"delta\") -> DataFrame:\n",
      "        \"\"\"Reads the silver layer DataFrame.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The silver layer DataFrame.\n",
      "        \"\"\"\n",
      "        if self._is_testing_mock:\n",
      "            if not os.path.exists(get_mock_table_path(self._dest_table)):\n",
      "                return None\n",
      "        elif not self._spark.catalog.tableExists(self._dest_table.table_path):\n",
      "            return None\n",
      "\n",
      "        sql_select_destination = f\"SELECT * FROM {self._dest_table.table_path}\"\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            df = self._spark.read.format(fformat).load(get_mock_table_path(self._dest_table))\n",
      "            return df\n",
      "\n",
      "        df = self._spark.sql(sql_select_destination)\n",
      "        return df\n",
      "\n",
      "    def _write_df(self, df: DataFrame, write_mode: str) -> None:\n",
      "        \"\"\"Writes the DataFrame to the specified location.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): The DataFrame to write.\n",
      "            write_mode (str): The write mode (e.g., \"overwrite\", \"append\").\n",
      "        \"\"\"\n",
      "        writer = df.write \\\n",
      "            .format(\"delta\") \\\n",
      "            .mode(write_mode) \\\n",
      "            .option(\"mergeSchema\", \"true\") \\\n",
      "            .partitionBy(*self._partition_by)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            writer.save(get_mock_table_path(self._dest_table))\n",
      "            return\n",
      "\n",
      "        writer.saveAsTable(self._dest_table.table_path)\n",
      "\n",
      "    def _get_columns_ordered(self, df: DataFrame, last_columns: list[str]) -> list[str]:\n",
      "        \"\"\"Get the columns in the desired order for processing.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): The DataFrame to analyze.\n",
      "\n",
      "        Returns:\n",
      "            list[str]: The columns in the desired order.\n",
      "        \"\"\"\n",
      "        data_columns = [\n",
      "            column\n",
      "            for column in df.columns\n",
      "            if column not in self._dw_columns\n",
      "        ]\n",
      "\n",
      "        return (\n",
      "            [self._pk_column_name, self._nk_column_name] +\n",
      "            data_columns +\n",
      "            last_columns\n",
      "        )\n",
      "\n",
      "    def _apply_transformations(self, df: DataFrame) -> DataFrame:\n",
      "        \"\"\"Applies transformations to the DataFrame.\n",
      "        Uses the source table name to find the appropriate transformation function.\n",
      "        Or uses a wildcard transformation function if available.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): The DataFrame to transform.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The transformed DataFrame.\n",
      "        \"\"\"\n",
      "        transform_fn: Callable = self._transformations.get(self._src_table.table)\n",
      "        transform_fn_all: Callable = self._transformations.get(\"*\")\n",
      "\n",
      "        if transform_fn_all is not None:\n",
      "            df = transform_fn_all(df, self)\n",
      "\n",
      "        if transform_fn is None:\n",
      "            return df\n",
      "\n",
      "        return transform_fn(df, self)\n",
      "\n",
      "    def _get_columns_to_compare(self, df: DataFrame) -> list[str]:\n",
      "        \"\"\"Get the columns to compare in the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): The DataFrame to analyze.\n",
      "\n",
      "        Returns:\n",
      "            list[str]: The columns to compare.\n",
      "        \"\"\"\n",
      "        if (\n",
      "            isinstance(self._include_comparing_columns, list) and\n",
      "            len(self._include_comparing_columns) >= 1\n",
      "        ):\n",
      "            self._validate_include_comparing_columns(df)\n",
      "            return self._include_comparing_columns\n",
      "\n",
      "        comparison_columns = [\n",
      "            column\n",
      "            for column in df.columns\n",
      "            if column not in self._exclude_comparing_columns\n",
      "        ]\n",
      "\n",
      "        return comparison_columns\n",
      "\n",
      "    def _add_missing_columns(self, df_target: DataFrame, df_source: DataFrame) -> DataFrame:\n",
      "        \"\"\"Adds missing columns from the source DataFrame to the target DataFrame.\n",
      "\n",
      "        Args:\n",
      "            df_target (DataFrame): The target DataFrame to which missing columns will be added.\n",
      "            df_source (DataFrame): The source DataFrame from which missing columns will be taken.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The target DataFrame with missing columns added.\n",
      "        \"\"\"\n",
      "        missing_columns = [\n",
      "            missing_column\n",
      "            for missing_column in df_source.columns\n",
      "            if missing_column not in df_target.columns\n",
      "        ]\n",
      "\n",
      "        for missing_column in missing_columns:\n",
      "            df_target = df_target.withColumn(missing_column, F.lit(None))\n",
      "\n",
      "        return df_target\n",
      "\n",
      "    def _compare_condition(\n",
      "        self,\n",
      "        df_bronze: DataFrame,\n",
      "        df_silver: DataFrame,\n",
      "        columns_to_compare: list[str]\n",
      "    ) -> tuple[F.Column, F.Column]:\n",
      "        \"\"\"Compares the specified columns of the bronze and silver DataFrames.\n",
      "\n",
      "        Args:\n",
      "            df_bronze (DataFrame): The bronze DataFrame.\n",
      "            df_silver (DataFrame): The silver DataFrame.\n",
      "            columns_to_compare (list[str]): The columns to compare.\n",
      "\n",
      "        Returns:\n",
      "            tuple[F.Column, F.Column]: The equality and inequality conditions.\n",
      "        \"\"\"\n",
      "        eq_condition = (\n",
      "            (df_bronze[columns_to_compare[0]] == df_silver[columns_to_compare[0]]) |\n",
      "            (df_bronze[columns_to_compare[0]].isNull() & df_silver[columns_to_compare[0]].isNull())\n",
      "        )\n",
      "\n",
      "        if len(columns_to_compare) == 1:\n",
      "            return eq_condition, ~eq_condition\n",
      "\n",
      "        for compare_column in columns_to_compare[1:]:\n",
      "            eq_condition &= (\n",
      "                (df_bronze[compare_column] == df_silver[compare_column]) |\n",
      "                (df_bronze[compare_column].isNull() & df_silver[compare_column].isNull())\n",
      "            )\n",
      "\n",
      "        return eq_condition, ~eq_condition\n",
      "\n",
      "    def _validate_transformations(self) -> None:\n",
      "        \"\"\"Validates the transformation functions.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: If any transformation function is not callable.\n",
      "        \"\"\"\n",
      "        for key, fn in self._transformations.items():\n",
      "            logger.info(f\"Transformation function for key '{key}': {fn}\")\n",
      "            if not callable(fn):\n",
      "                err_msg = f\"The transformation function for key '{key}' is not callable.\"\n",
      "                raise TypeError(err_msg)\n",
      "\n",
      "    def _validate_param_isinstance(self, param, param_name: str, obj_class) -> None:\n",
      "        \"\"\"Validates a parameter to be the expected class instance\n",
      "\n",
      "        Args:\n",
      "            param (any): parameter\n",
      "            param_name (str): parametername\n",
      "            obj_class (_type_): class\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when actual type is different from expected type\n",
      "        \"\"\"\n",
      "        if not isinstance(param, obj_class):\n",
      "            err_msg = f\"The param '{param_name}' should be type of {obj_class.__name__}, but was {str(param.__class__)}\"\n",
      "            raise TypeError(err_msg)\n",
      "\n",
      "    def _validate_min_length(self, param, param_name: str, min_length: int) -> None:\n",
      "        \"\"\"Validates a string or list to be not none and has a minimum length\n",
      "\n",
      "        Args:\n",
      "            param (_type_): parameter\n",
      "            param_name (str): parametername\n",
      "            min_length (int): minimum lenght\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when actual type is different from expected type\n",
      "            ValueError: when parametervalue is to short\n",
      "        \"\"\"\n",
      "        if not isinstance(param, str) and not isinstance(param, list):\n",
      "            err_msg = f\"The param '{param_name}' should be type of string or list, but was {str(param.__class__)}\"\n",
      "            raise TypeError(err_msg)\n",
      "\n",
      "        param_length = len(param)\n",
      "        if param_length < min_length:\n",
      "            err_msg = f\"Param length to short. The minimum length of the param '{param_name}' is {min_length} but was {param_length}\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_constant_columns(self) -> None:\n",
      "        \"\"\"Validates the given constant columns to be an instance of ConstantColumns and\n",
      "        list contains only one part_of_nk=True, because of the following filtering of the dataframe.\n",
      "\n",
      "        It should have just one part_of_nk=True, because the dataframe will filtered later by the\n",
      "        constant_column.name, if part_of_nk=True.\n",
      "        If part_of_nk=True should be supported more then once, then we need to implement\n",
      "        an \"and\" filtering.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when an item of the list is not an instance of ConstantColumn\n",
      "            ValueError: when list contains more then one ConstantColumn with part_of_nk=True\n",
      "        \"\"\"\n",
      "        nk_count = 0\n",
      "        for constant_column in self._constant_columns:\n",
      "            self._validate_param_isinstance(constant_column, \"constant_column\", ConstantColumn)\n",
      "\n",
      "            if constant_column.part_of_nk:\n",
      "                nk_count += 1\n",
      "\n",
      "            if nk_count > 1:\n",
      "                err_msg = \"In constant_columns are more then one part_of_nk=True, what is not supported!\"\n",
      "                raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_nk_columns_in_df(self, df: DataFrame) -> None:\n",
      "        \"\"\"Validates the given dataframe. The given dataframe should contain all natural key columns,\n",
      "        because all natural key columns will selected and used for concatitation.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): dataframe to validate\n",
      "\n",
      "        Raises:\n",
      "            ValueError: when dataframe does not contain all natural key columns\n",
      "        \"\"\"\n",
      "        df_columns = set(df.columns)\n",
      "        for column in self._nk_columns:\n",
      "            if column in df_columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The NK Column '{column}' does not exist in df columns: {df_columns}\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_include_comparing_columns(self, df: DataFrame) -> None:\n",
      "        \"\"\"Validates the include_comparing_columns.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): The dataframe to validate against.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: If include_comparing_columns is empty or if any column in include_comparing_columns\n",
      "            ValueError: If any column in include_comparing_columns is not present in the dataframe.\n",
      "        \"\"\"\n",
      "        self._validate_param_isinstance(self._include_comparing_columns, \"include_comparing_columns\", list)\n",
      "\n",
      "        if len(self._include_comparing_columns) == 0:\n",
      "            err_msg = \"The param 'include_comparing_columns' is present, but don't contains any columns.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "        for include_column in self._include_comparing_columns:\n",
      "            if include_column in df.columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The column '{include_column}' should be compared, but is not given in df.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_partition_by_columns(self, df: DataFrame) -> None:\n",
      "        \"\"\"Validates the partition by columns.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): The dataframe to validate against.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: If partition_by is not a list.\n",
      "            ValueError: If any partition_column is not present in the dataframe.\n",
      "        \"\"\"\n",
      "        self._validate_param_isinstance(self._partition_by, \"partition_by\", list)\n",
      "\n",
      "        for partition_column in self._partition_by:\n",
      "            if partition_column in df.columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The column '{partition_column}' should be partitioned, but is not given in df.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _set_spark_config(self) -> None:\n",
      "        \"\"\"Sets additional spark configurations\n",
      "\n",
      "        spark.sql.parquet.vorder.enabled: Setting \"spark.sql.parquet.vorder.enabled\" to \"true\" in PySpark config enables a feature called vectorized parquet decoding.\n",
      "                                                  This optimizes the performance of reading Parquet files by leveraging vectorized instructions and processing multiple values at once, enhancing overall processing speed.\n",
      "\n",
      "        Setting \"spark.sql.parquet.int96RebaseModeInRead\" and \"spark.sql.legacy.parquet.int96RebaseModeInWrite\" to \"CORRECTED\" ensures that Int96 values (a specific timestamp representation used in Parquet files) are correctly rebased during both reading and writing operations.\n",
      "        This is crucial for maintaining consistency and accuracy, especially when dealing with timestamp data across different systems or time zones.\n",
      "        Similarly, configuring \"spark.sql.parquet.datetimeRebaseModeInRead\" and \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" to \"CORRECTED\" ensures correct handling of datetime values during Parquet file operations.\n",
      "        By specifying this rebasing mode, potential discrepancies or errors related to datetime representations are mitigated, resulting in more reliable data processing and analysis workflows.\n",
      "        \"\"\"\n",
      "        self._spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
      "\n",
      "        self._spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        if not self._is_initialized:\n",
      "            return super().__str__(self)\n",
      "\n",
      "        return str({\n",
      "            \"historize\": self._historize,\n",
      "            \"is_delta_load\": self._is_delta_load,\n",
      "            \"delta_load_use_broadcast\": self._delta_load_use_broadcast,\n",
      "            \"src_table_path\": self._src_table.table_path,\n",
      "            \"dist_table_path\": self._dest_table.table_path,\n",
      "            \"nk_columns\": self._nk_columns,\n",
      "            \"include_comparing_columns\": self._include_comparing_columns,\n",
      "            \"exclude_comparing_columns\": self._exclude_comparing_columns,\n",
      "            \"transformations\": self._transformations,\n",
      "            \"constant_columns\": self._constant_columns,\n",
      "            \"partition_by\": self._partition_by,\n",
      "            \"pk_column\": self._pk_column_name,\n",
      "            \"nk_column\": self._nk_column_name,\n",
      "            \"nk_column_concate_str\": self._nk_column_concate_str,\n",
      "            \"row_load_dts_column\": self._row_load_dts_column,\n",
      "            \"row_update_dts_column\": self._row_update_dts_column,\n",
      "            \"row_delete_dts_column\": self._row_delete_dts_column,\n",
      "            \"dw_columns\": self._dw_columns\n",
      "        })\n",
      "\n",
      "\n",
      "class SilverIngestionSCD2Service(BaseSilverIngestionServiceImpl):\n",
      "    _is_initialized: bool = False\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        *,\n",
      "        spark_: SparkSession,\n",
      "        source_table: LakehouseTable,\n",
      "        destination_table: LakehouseTable,\n",
      "        nk_columns: list[str],\n",
      "        constant_columns: list[ConstantColumn],\n",
      "        is_delta_load: bool,\n",
      "        delta_load_use_broadcast: bool,\n",
      "        transformations: dict,\n",
      "        exclude_comparing_columns: list[str] | None = None,\n",
      "        include_comparing_columns: list[str] | None = None,\n",
      "        historize: bool = True,\n",
      "        partition_by_columns: list[str] = None,\n",
      "        df_bronze: DataFrame = None,\n",
      "\n",
      "        pk_column_name: str = \"PK\",\n",
      "        nk_column_name: str = \"NK\",\n",
      "        nk_column_concate_str: str = \"_\",\n",
      "        row_is_current_column: str = \"ROW_IS_CURRENT\",\n",
      "        row_update_dts_column: str = \"ROW_UPDATE_DTS\",\n",
      "        row_delete_dts_column: str = \"ROW_DELETE_DTS\",\n",
      "        row_load_dts_column: str = \"ROW_LOAD_DTS\",\n",
      "\n",
      "        **kwargs\n",
      "    ) -> None:\n",
      "        dw_columns = [\n",
      "            pk_column_name,\n",
      "            nk_column_name,\n",
      "            row_is_current_column,\n",
      "            row_update_dts_column,\n",
      "            row_delete_dts_column,\n",
      "            row_load_dts_column\n",
      "        ]\n",
      "\n",
      "        super().init(\n",
      "            spark_=spark_,\n",
      "            source_table=source_table,\n",
      "            destination_table=destination_table,\n",
      "            nk_columns=nk_columns,\n",
      "            constant_columns=constant_columns,\n",
      "            is_delta_load=is_delta_load,\n",
      "            delta_load_use_broadcast=delta_load_use_broadcast,\n",
      "            transformations=transformations,\n",
      "            exclude_comparing_columns=exclude_comparing_columns or [],\n",
      "            include_comparing_columns=include_comparing_columns or [],\n",
      "            historize=historize,\n",
      "            partition_by_columns=partition_by_columns or [],\n",
      "            df_bronze=df_bronze,\n",
      "            dw_columns=dw_columns,\n",
      "\n",
      "            pk_column_name=pk_column_name,\n",
      "            nk_column_name=nk_column_name,\n",
      "            nk_column_concate_str=nk_column_concate_str,\n",
      "            row_is_current_column=row_is_current_column,\n",
      "            row_update_dts_column=row_update_dts_column,\n",
      "            row_delete_dts_column=row_delete_dts_column,\n",
      "            row_load_dts_column=row_load_dts_column,\n",
      "\n",
      "            is_testing_mock=kwargs.get(\"is_testing_mock\", False)\n",
      "        )\n",
      "\n",
      "        self._validate_scd2_params()\n",
      "\n",
      "        self._is_initialized = True\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        return super().__str__()\n",
      "\n",
      "    def _validate_scd2_params(self) -> None:\n",
      "        pass\n",
      "\n",
      "    def run(self) -> None:\n",
      "        \"\"\"Ingest data from bronze to silver layer.\n",
      "        This method performs the following steps:\n",
      "        1. Create a DataFrame from the bronze layer.\n",
      "        2. Create a DataFrame from the silver layer.\n",
      "        3. If the silver layer is empty or we are not historizing, overwrite the silver layer with the bronze layer.\n",
      "        4. If the silver layer is not empty and we are historizing, perform the following steps:\n",
      "            a. Compare the bronze and silver DataFrames to find new and updated records.\n",
      "            b. Insert new records into the silver layer.\n",
      "            c. Update existing records in the silver layer with the new data from the bronze layer.\n",
      "            d. Set the ROW_UPDATE_DTS and ROW_IS_CURRENT columns for updated records.\n",
      "            e. Set the ROW_DELETE_DTS and ROW_IS_CURRENT columns for deleted records.\n",
      "        5. If the silver layer is not empty and we are not historizing, perform the following steps:\n",
      "            a. Compare the bronze and silver DataFrames to find new, updated, and deleted records.\n",
      "            b. Insert new records into the silver layer.\n",
      "            c. Update existing records in the silver layer with the new data from the bronze layer.\n",
      "            d. Set the ROW_UPDATE_DTS and ROW_IS_CURRENT columns for updated records.\n",
      "            e. Set the ROW_DELETE_DTS and ROW_IS_CURRENT columns for deleted records.\n",
      "        6. If the silver layer is not empty and we are performing a delta load, merge the expired and deleted records into the silver layer.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the service is not initialized before calling this method.\n",
      "            ValueError: If the required columns are not present in the DataFrame.\n",
      "            TypeError: If the DataFrame is not of the expected type.\n",
      "            Exception: If any other error occurs during the ingestion process.\n",
      "        \"\"\"\n",
      "        if not self._is_initialized:\n",
      "            raise RuntimeError(\"The SilverIngestionInsertOnlyService is not initialized. Call the init method first.\")\n",
      "\n",
      "        self._current_timestamp = datetime.now()\n",
      "        df_bronze, df_silver = self._generate_dataframes()\n",
      "\n",
      "        # 1.\n",
      "        target_columns_ordered = self._get_columns_ordered(df_bronze, last_columns=[\n",
      "            self._row_load_dts_column,\n",
      "            self._row_update_dts_column,\n",
      "            self._row_delete_dts_column,\n",
      "            self._row_is_current_column\n",
      "        ])\n",
      "\n",
      "        do_overwrite = (\n",
      "            df_silver is None or\n",
      "            (\n",
      "                not self._historize and\n",
      "                not self._is_delta_load\n",
      "                # If we are not historizing but performing a delta load,\n",
      "                # we need to update the silver-layer data.\n",
      "                # We should not overwrite the silver-layer data,\n",
      "                # because the delta load (bronze layer) do not contain all the data!\n",
      "            )\n",
      "        )\n",
      "        if do_overwrite:\n",
      "            df_inital_load = df_bronze.select(target_columns_ordered)\n",
      "            self._write_df(df_inital_load, \"overwrite\")\n",
      "            return\n",
      "\n",
      "        # 2.\n",
      "        columns_to_compare = self._get_columns_to_compare(df_bronze)\n",
      "\n",
      "        join_condition = (df_bronze[self._nk_column_name] == df_silver[self._nk_column_name])\n",
      "        df_joined = df_bronze.join(df_silver, join_condition, \"outer\")\n",
      "\n",
      "        # 3.\n",
      "        _, neq_condition = self._compare_condition(df_bronze, df_silver, columns_to_compare)\n",
      "        updated_filter_condition = self._updated_filter(df_bronze, df_silver, neq_condition)\n",
      "\n",
      "        # New records\n",
      "        df_new_records = self._filter_new_records(df_joined, df_bronze, df_silver)\n",
      "\n",
      "        # Updated records to insert (from bronze)\n",
      "        df_updated_records = self._filter_updated_records(df_joined, df_bronze, updated_filter_condition)\n",
      "\n",
      "        # 4.\n",
      "        df_new_data = df_new_records.unionByName(df_updated_records) \\\n",
      "                                    .select(target_columns_ordered) \\\n",
      "                                    .dropDuplicates([\"PK\"])\n",
      "\n",
      "        self._write_df(df_new_data, \"append\")\n",
      "\n",
      "        # 5.\n",
      "        # Expired records are the records in silver layer, which was updated. Merge/Update them and\n",
      "        # add current timestamp to ROW_UPDATE_DTS and set ROW_IS_CURRENT to 0.\n",
      "        df_expired_records = self._filter_expired_records(df_joined, df_silver, updated_filter_condition)\n",
      "\n",
      "        # 6.\n",
      "        if self._is_delta_load:\n",
      "            self._exec_merge_into(df_expired_records)\n",
      "            return\n",
      "\n",
      "        df_deleted_records = self._filter_deleted_records(df_joined, df_bronze, df_silver)\n",
      "\n",
      "        df_merge_into_records = df_expired_records.unionByName(df_deleted_records) \\\n",
      "                                                  .select(target_columns_ordered)\n",
      "        self._exec_merge_into(df_merge_into_records)\n",
      "\n",
      "    def _generate_dataframes(self) -> tuple[DataFrame, DataFrame]:\n",
      "        \"\"\"Generates the bronze and silver DataFrames.\n",
      "        This method creates the bronze DataFrame from the source table and the silver DataFrame from the destination table.\n",
      "        If the silver DataFrame does not exist, it returns None for the silver DataFrame.\n",
      "        If the bronze DataFrame does not exist, it raises an error.\n",
      "        Raises:\n",
      "            RuntimeError: If the bronze DataFrame cannot be created.\n",
      "            ValueError: If the required columns are not present in the DataFrame.\n",
      "            TypeError: If the DataFrame is not of the expected type.\n",
      "            Exception: If any other error occurs during the ingestion process.\n",
      "\n",
      "        Returns:\n",
      "            tuple[DataFrame, DataFrame]: The bronze and silver DataFrames.\n",
      "        \"\"\"\n",
      "        df_bronze = self._create_bronze_df()\n",
      "        df_bronze = self._apply_transformations(df_bronze)\n",
      "\n",
      "        df_silver = self._create_silver_df()\n",
      "\n",
      "        if df_silver is None:\n",
      "            return df_bronze, df_silver\n",
      "\n",
      "        df_bronze = self._add_missing_columns(df_bronze, df_silver)\n",
      "        df_silver = self._add_missing_columns(df_silver, df_bronze)\n",
      "\n",
      "        if self._is_delta_load and self._delta_load_use_broadcast:\n",
      "            df_bronze = F.broadcast(df_bronze)\n",
      "\n",
      "        return df_bronze, df_silver\n",
      "\n",
      "    def _updated_filter(self, df_bronze: DataFrame, df_silver: DataFrame, neq_condition):\n",
      "        \"\"\"Generates the filter condition for updated records.\n",
      "        This method creates a filter condition that checks for updated records based on the non-key columns.\n",
      "        It checks if the non-key columns in the bronze DataFrame are not null, the non-key columns in the silver DataFrame are not null,\n",
      "        the ROW_IS_CURRENT column in the silver DataFrame is equal to 1, and the non-equal condition is met.\n",
      "        This filter condition is used to identify records that have been updated in the bronze DataFrame compared to the silver DataFrame.\n",
      "\n",
      "        Args:\n",
      "            df_bronze (DataFrame): The bronze DataFrame containing the source data.\n",
      "            df_silver (DataFrame): The silver DataFrame containing the target data.\n",
      "            neq_condition (Column): The non-equal condition to check for updates.\n",
      "\n",
      "        Returns:\n",
      "            Column: The filter condition for updated records.\n",
      "        \"\"\"\n",
      "        updated_filter = (\n",
      "            (df_bronze[self._nk_column_name].isNotNull()) &\n",
      "            (df_silver[self._nk_column_name].isNotNull()) &\n",
      "            (df_silver[self._row_is_current_column] == 1) &\n",
      "            (neq_condition)\n",
      "        )\n",
      "\n",
      "        return updated_filter\n",
      "\n",
      "    def _filter_new_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n",
      "        \"\"\"Filters new records from the joined DataFrame.\n",
      "        This method filters the joined DataFrame to find records that are new, meaning they do not exist in the silver DataFrame.\n",
      "        It checks if the NK column in the silver DataFrame is null, indicating that these records are not present in the silver layer.\n",
      "        The new records are selected from the bronze DataFrame, which contains the source data.\n",
      "\n",
      "        Args:\n",
      "            df_joined (DataFrame): The joined DataFrame containing all records.\n",
      "            df_bronze (DataFrame): The bronze DataFrame containing the source data.\n",
      "            df_silver (DataFrame): The silver DataFrame containing the target data.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: A DataFrame containing the new records.\n",
      "        \"\"\"\n",
      "        new_records_filter = (df_silver[self._nk_column_name].isNull())\n",
      "        df_new_records = df_joined.filter(new_records_filter) \\\n",
      "                                  .select(df_bronze[\"*\"])\n",
      "\n",
      "        return df_new_records\n",
      "\n",
      "    def _filter_updated_records(self, df_joined: DataFrame, df_bronze: DataFrame, updated_filter) -> DataFrame:\n",
      "        \"\"\"Filters updated records from the joined DataFrame.\n",
      "        This method filters the joined DataFrame to find records that have been updated in the bronze DataFrame compared to the silver DataFrame.\n",
      "        It uses the updated filter condition to select records that have been modified.\n",
      "\n",
      "        Args:\n",
      "            df_joined (DataFrame): The joined DataFrame containing all records.\n",
      "            df_bronze (DataFrame): The bronze DataFrame containing the source data.\n",
      "            updated_filter (Column): The filter condition for updated records.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: A DataFrame containing the updated records.\n",
      "        \"\"\"\n",
      "        # Select not matching bronze columns\n",
      "        df_updated_records = df_joined.filter(updated_filter) \\\n",
      "                                      .select(df_bronze[\"*\"])\n",
      "\n",
      "        return df_updated_records\n",
      "\n",
      "    def _filter_expired_records(self, df_joined: DataFrame, df_silver: DataFrame, updated_filter) -> DataFrame:\n",
      "        \"\"\"Filters expired records from the joined DataFrame.\n",
      "        This method filters the joined DataFrame to find records that have been updated in the silver DataFrame.\n",
      "        It selects records that match the updated filter condition and updates the ROW_UPDATE_DTS column with the current timestamp.\n",
      "        It also sets the ROW_DELETE_DTS column to None and the ROW_IS_CURRENT column to 0, indicating that these records are no longer current.\n",
      "        This is used to mark records in the silver layer as expired when they have been updated in the bronze layer.\n",
      "\n",
      "        Args:\n",
      "            df_joined (DataFrame): The joined DataFrame containing all records.\n",
      "            df_silver (DataFrame): The silver DataFrame containing the target data.\n",
      "            updated_filter (Column): The filter condition for updated records.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: A DataFrame containing the expired records.\n",
      "        \"\"\"\n",
      "        # Select not matching silver columns\n",
      "        df_expired_records = df_joined.filter(updated_filter) \\\n",
      "                                      .select(df_silver[\"*\"]) \\\n",
      "                                      .withColumn(self._row_update_dts_column, F.lit(self._current_timestamp)) \\\n",
      "                                      .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\")) \\\n",
      "                                      .withColumn(self._row_is_current_column, F.lit(0))\n",
      "\n",
      "        return df_expired_records\n",
      "\n",
      "    def _filter_deleted_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n",
      "        \"\"\"Filters deleted records from the joined DataFrame.\n",
      "        This method filters the joined DataFrame to find records that exist in the silver DataFrame but do not exist in the bronze DataFrame.\n",
      "        It checks if the NK column in the bronze DataFrame is null, indicating that these records have been deleted.\n",
      "        The deleted records are selected from the silver DataFrame, which contains the target data.\n",
      "        It updates the ROW_UPDATE_DTS and ROW_DELETE_DTS columns with the current timestamp\n",
      "        and sets the ROW_IS_CURRENT column to 0, indicating that these records are no longer current.\n",
      "\n",
      "        Args:\n",
      "            df_joined (DataFrame): The joined DataFrame containing all records.\n",
      "            df_bronze (DataFrame): The bronze DataFrame containing the source data.\n",
      "            df_silver (DataFrame): The silver DataFrame containing the target data.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: A DataFrame containing the deleted records.\n",
      "        \"\"\"\n",
      "        df_deleted_records = df_joined.filter(df_bronze[self._nk_column_name].isNull()) \\\n",
      "                                      .select(df_silver[\"*\"]) \\\n",
      "                                      .withColumn(self._row_update_dts_column, F.lit(self._current_timestamp)) \\\n",
      "                                      .withColumn(self._row_delete_dts_column, F.lit(self._current_timestamp)) \\\n",
      "                                      .withColumn(self._row_is_current_column, F.lit(0))\n",
      "\n",
      "        return df_deleted_records\n",
      "\n",
      "    def _create_bronze_df(self) -> DataFrame:\n",
      "        \"\"\"Creates the bronze DataFrame.\n",
      "        This method reads data from the source table and applies necessary transformations to create the bronze DataFrame.\n",
      "        It selects all columns from the source table, adds primary key (PK) and natural key (NK) columns,\n",
      "        and sets the row update, delete, and load timestamps.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The bronze DataFrame containing the source data.\n",
      "        \"\"\"\n",
      "        sql_select_source = f\"SELECT * FROM {self._src_table.table_path}\"\n",
      "        if isinstance(self._df_bronze, DataFrame):\n",
      "            df = self._df_bronze\n",
      "        elif not self._is_testing_mock:\n",
      "            df = self._spark.sql(sql_select_source)\n",
      "        else:\n",
      "            df = self._spark.read.format(\"parquet\").load(get_mock_table_path(self._src_table))\n",
      "\n",
      "        self._validate_nk_columns_in_df(df)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.name not in df.columns:\n",
      "                df = df.withColumn(constant_column.name, F.lit(constant_column.value))\n",
      "\n",
      "        df = df.withColumn(self._pk_column_name, generate_uuid())  \\\n",
      "               .withColumn(self._nk_column_name, F.concat_ws(self._nk_column_concate_str, *self._nk_columns)) \\\n",
      "               .withColumn(self._row_update_dts_column, F.lit(None).cast(\"timestamp\")) \\\n",
      "               .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\")) \\\n",
      "               .withColumn(self._row_is_current_column, F.lit(1)) \\\n",
      "               .withColumn(self._row_load_dts_column, F.lit(self._current_timestamp))\n",
      "\n",
      "        return df\n",
      "\n",
      "    def _create_silver_df(self) -> DataFrame:\n",
      "        \"\"\"Creates the silver DataFrame.\n",
      "        This method reads data from the destination table and applies necessary transformations to create the silver DataFrame.\n",
      "        It filters the DataFrame to include only current records (ROW_IS_CURRENT = 1)\n",
      "        and ensures that the natural key (NK) columns are present.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The silver DataFrame containing the target data.\n",
      "        \"\"\"\n",
      "        if self._is_testing_mock:\n",
      "            if not os.path.exists(get_mock_table_path(self._dest_table)):\n",
      "                return None\n",
      "        elif not self._spark.catalog.tableExists(self._dest_table.table_path):\n",
      "            return None\n",
      "\n",
      "        df = self.read_silver_df()\n",
      "        df = df.filter(F.col(self._row_is_current_column) == 1)\n",
      "\n",
      "        self._validate_nk_columns_in_df(df)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.name not in df.columns:\n",
      "                df = df.withColumn(constant_column.name, F.lit(None))\n",
      "\n",
      "            if constant_column.part_of_nk:\n",
      "                df = df.filter(F.col(constant_column.name) == constant_column.value)\n",
      "\n",
      "        df = df.withColumn(self._nk_column_name, F.concat_ws(self._nk_column_concate_str, *self._nk_columns))\n",
      "\n",
      "        return df\n",
      "\n",
      "    def _exec_merge_into(self, df_source: DataFrame) -> None:\n",
      "        \"\"\"Executes the MERGE INTO statement.\n",
      "        This method merges the source DataFrame into the destination table using the Delta Lake MERGE INTO operation.\n",
      "        It matches records based on the primary key (PK) column and updates the ROW_UPDATE_DTS, ROW_DELETE_DTS, and ROW_IS_CURRENT columns.\n",
      "\n",
      "        Args:\n",
      "            df_source (DataFrame): The source DataFrame to merge.\n",
      "            view_name (str): The name of the view to merge into.\n",
      "        \"\"\"\n",
      "        if self._is_testing_mock:\n",
      "            target_path = get_mock_table_path(self._dest_table)\n",
      "            delta_table = DeltaTable.forPath(self._spark, target_path)\n",
      "        else:\n",
      "            table_name = self._dest_table.table_path\n",
      "            delta_table = DeltaTable.forName(self._spark, table_name)\n",
      "\n",
      "        delta_table.alias(\"target\") \\\n",
      "            .merge(\n",
      "                df_source.alias(\"source\"),\n",
      "                f\"target.{self._pk_column_name} = source.{self._pk_column_name}\"\n",
      "            ) \\\n",
      "            .whenMatchedUpdate(set={\n",
      "                self._row_update_dts_column: f\"source.{self._row_update_dts_column}\",\n",
      "                self._row_delete_dts_column: f\"source.{self._row_delete_dts_column}\",\n",
      "                self._row_is_current_column: f\"source.{self._row_is_current_column}\"\n",
      "            }) \\\n",
      "            .execute()\n",
      "\n",
      "\n",
      "etl = SilverIngestionSCD2Service()\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.scd2\", VERSION)\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./code.py\", \"w\") as f:\n",
    "#     f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e928341",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3e8d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SilverIngestionSCD2Service at 0x10563fcb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d48cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = LakehouseTable(\n",
    "    lakehouse=\"test_lakehouse\",\n",
    "    schema=\"test_schema\",\n",
    "    table=\"test_table\"\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ff41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Any\n",
      "from pyspark.sql import DataFrame, SparkSession\n",
      "from fabricengineer.logging.logger import logger\n",
      "\n",
      "\n",
      "def to_spark_sql(sql: str) -> str:\n",
      "    return sql \\\n",
      "            .replace(\"[\", \"`\") \\\n",
      "            .replace(\"]\", \"`\")\n",
      "\n",
      "\n",
      "class MaterializedLakeView:\n",
      "    def __init__(\n",
      "        self,\n",
      "        lakehouse: str = None,\n",
      "        schema: str = None,\n",
      "        table: str = None,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self.init(\n",
      "            lakehouse=lakehouse,\n",
      "            schema=schema,\n",
      "            table=table,\n",
      "            table_suffix=table_suffix,\n",
      "            spark_=spark_,\n",
      "            notebookutils_=notebookutils_,\n",
      "            is_testing_mock=is_testing_mock\n",
      "        )\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        lakehouse: str,\n",
      "        schema: str,\n",
      "        table: str,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> 'MaterializedLakeView':\n",
      "        \"\"\"Initializes the MaterializedLakeView instance.\n",
      "\n",
      "        Args:\n",
      "            lakehouse (str): The lakehouse name.\n",
      "            schema (str): The schema name.\n",
      "            table (str): The table name.\n",
      "            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\n",
      "            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\n",
      "            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\n",
      "            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\n",
      "\n",
      "        Returns:\n",
      "            MaterializedLakeView: The initialized MaterializedLakeView instance.\n",
      "        \"\"\"\n",
      "        self._lakehouse = lakehouse\n",
      "        self._schema = schema\n",
      "        self._table = table\n",
      "        self._table_suffix = table_suffix\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "\n",
      "        # 'spark' and 'notebookutils' are available in Fabric notebook\n",
      "        self._spark = self._get_init_spark(spark_)\n",
      "        self._notebookutils = self._get_init_notebookutils(notebookutils_)\n",
      "        return self\n",
      "\n",
      "    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\n",
      "        \"\"\"Initializes the SparkSession instance.\n",
      "        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global 'spark' variable.\n",
      "\n",
      "        Args:\n",
      "            spark_ (SparkSession): The SparkSession instance.\n",
      "\n",
      "        Returns:\n",
      "            SparkSession | None: The initialized SparkSession instance or None.\n",
      "        \"\"\"\n",
      "        if isinstance(spark_, SparkSession):\n",
      "            return spark_\n",
      "        try:\n",
      "            if spark is not None:  # noqa: F821 # type: ignore\n",
      "                return spark  # noqa: F821 # type: ignore\n",
      "            return spark_\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\n",
      "        \"\"\"Initializes the NotebookUtils instance.\n",
      "        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global 'notebookutils' variable.\n",
      "\n",
      "        Args:\n",
      "            notebookutils_ (Any): The NotebookUtils instance.\n",
      "\n",
      "        Returns:\n",
      "            Any | None: The initialized NotebookUtils instance or None.\n",
      "        \"\"\"\n",
      "        if notebookutils_ is not None:\n",
      "            return notebookutils_\n",
      "        try:\n",
      "            if notebookutils is not None:  # noqa: F821 # type: ignore\n",
      "                return notebookutils  # noqa: F821 # type: ignore\n",
      "            return None\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    @property\n",
      "    def lakehouse(self) -> str:\n",
      "        if self._lakehouse is None:\n",
      "            raise ValueError(\"Lakehouse is not initialized.\")\n",
      "        return self._lakehouse\n",
      "\n",
      "    @property\n",
      "    def schema(self) -> str:\n",
      "        if self._schema is None:\n",
      "            raise ValueError(\"Schema is not initialized.\")\n",
      "        return self._schema\n",
      "\n",
      "    @property\n",
      "    def table(self) -> str:\n",
      "        if self._table is None:\n",
      "            raise ValueError(\"Table is not initialized.\")\n",
      "        return self._table\n",
      "\n",
      "    @property\n",
      "    def table_suffix(self) -> str:\n",
      "        return self._table_suffix\n",
      "\n",
      "    @property\n",
      "    def spark(self) -> SparkSession:\n",
      "        if self._spark is None:\n",
      "            raise ValueError(\"SparkSession is not initialized.\")\n",
      "        return self._spark\n",
      "\n",
      "    @property\n",
      "    def notebookutils(self) -> Any:\n",
      "        if self._notebookutils is None:\n",
      "            raise ValueError(\"NotebookUtils is not initialized.\")\n",
      "        return self._notebookutils\n",
      "\n",
      "    @property\n",
      "    def table_name(self) -> str:\n",
      "        table_suffix = self.table_suffix or \"\"\n",
      "        return f\"{self.table}{table_suffix}\"\n",
      "\n",
      "    @property\n",
      "    def file_path(self) -> str:\n",
      "        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\n",
      "        return path\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\n",
      "        return table_path\n",
      "\n",
      "    @property\n",
      "    def schema_path(self) -> str:\n",
      "        schema_path = f\"{self.lakehouse}.{self.schema}\"\n",
      "        return schema_path\n",
      "\n",
      "    def read_file(self) -> str | None:\n",
      "        \"\"\"Reads the content of the SQL file from the specified lakehouse.\n",
      "        If the file does not exist, it returns None.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the file cannot be read.\n",
      "\n",
      "        Returns:\n",
      "            str | None: The content of the file or None if it doesn't exist.\n",
      "        \"\"\"\n",
      "        path = self.file_path\n",
      "        try:\n",
      "            if not self.notebookutils.fs.exists(path):\n",
      "                return None\n",
      "            if self._is_testing_mock:\n",
      "                with open(path, \"r\") as file:\n",
      "                    return file.read()\n",
      "            df = self.spark.read.text(path, wholetext=True)\n",
      "            mlv_code = df.collect()[0][0]\n",
      "            return mlv_code\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\n",
      "\n",
      "    def write_file(self, sql: str) -> bool:\n",
      "        \"\"\"Writes the SQL content to the specified file in a lakehouse.\n",
      "        If the file already exists, it will be overwritten.\n",
      "        If the file cannot be written, it raises a RuntimeError.\n",
      "\n",
      "        Args:\n",
      "            sql (str): The SQL content to write.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the file cannot be written.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the file was written successfully, False otherwise.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            result = self.notebookutils.fs.put(\n",
      "                file=self.file_path,\n",
      "                content=sql,\n",
      "                overwrite=True\n",
      "            )\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\n",
      "\n",
      "    def create_schema(self) -> DataFrame | None:\n",
      "        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\n",
      "        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\n",
      "        logger.info(create_schema)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_schema)\n",
      "\n",
      "    def create(self, sql: str) -> DataFrame | None:\n",
      "        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\n",
      "        self.create_schema()\n",
      "\n",
      "        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\nAS\\n{sql}\"\n",
      "        logger.info(f\"CREATE MLV: {self.table_path}\")\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_mlv)\n",
      "\n",
      "    def drop(self) -> str:\n",
      "        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\n",
      "        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\n",
      "        logger.info(drop_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(drop_mlv)\n",
      "\n",
      "    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\n",
      "        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\n",
      "\n",
      "        Args:\n",
      "            sql (str): The SQL query to create the MLV.\n",
      "            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The result of the create or replace operation.\n",
      "        \"\"\"\n",
      "        mlv_code_current = self.read_file()\n",
      "        is_existing = (\n",
      "            mock_is_existing\n",
      "            if mock_is_existing is not None\n",
      "            else self.spark.catalog.tableExists(self.table_path)\n",
      "        )\n",
      "\n",
      "        if mlv_code_current is None and not is_existing:\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif mlv_code_current is None and is_existing:\n",
      "            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\n",
      "            self.drop()\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif sql == mlv_code_current and is_existing:\n",
      "            logger.info(\"Nothing has changed.\")\n",
      "            return None\n",
      "\n",
      "        logger.info(f\"REPLACE MLV: {self.table_path}\")\n",
      "        self.drop()\n",
      "        res = self.create(sql)\n",
      "        self.write_file(sql)\n",
      "        return res\n",
      "\n",
      "    def refresh(self, full_refresh: bool) -> DataFrame:\n",
      "        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\n",
      "        full_refresh_str = \"FULL\" if full_refresh else \"\"\n",
      "        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\n",
      "        logger.info(refresh_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(refresh_mlv)\n",
      "\n",
      "    def to_dict(self) -> None:\n",
      "        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\n",
      "        return {\n",
      "            \"lakehouse\": self.lakehouse,\n",
      "            \"schema\": self.schema,\n",
      "            \"table\": self.table,\n",
      "            \"table_path\": self.table_path\n",
      "        }\n",
      "\n",
      "\n",
      "mlv = MaterializedLakeView() {'__name__': '__main__', '__doc__': '\\nNote: all executions are function-scoped as we do not assume the code below executes in an isolated kernel environment.\\n', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import sys\\n\\nsys.path.append(\"../src\")\\n\\nfrom fabricengineer.import_module.import_module import import_module\\n\\nVERSION = \"0.0.9\"', 'import os\\nfrom pyspark.sql import SparkSession\\n\\n\\nclass NotebookUtilsFSMock:\\n    def _get_path(self, file: str) -> str:\\n        return os.path.join(os.getcwd(), file)\\n\\n    def exists(self, path: str) -> bool:\\n        return os.path.exists(self._get_path(path))\\n\\n    def put(\\n        self,\\n        file: str,\\n        content: str,\\n        overwrite: bool = False\\n    ) -> None:\\n        path = self._get_path(file)\\n        os.makedirs(os.path.dirname(path), exist_ok=True)\\n\\n        if os.path.exists(path) and not overwrite:\\n            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\\n        with open(path, \\'w\\') as f:\\n            f.write(content)\\n\\n\\nclass NotebookUtilsMock:\\n    def __init__(self):\\n        self.fs = NotebookUtilsFSMock()\\n\\nglobal spark\\nspark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\\n\\nglobal notebookutils\\nnotebookutils = NotebookUtilsMock()', 'code = import_module(\"transform.silver.insertonly\", VERSION)\\n\\nprint(code)', 'with open(\"./code.py\", \"w\") as f:\\n    f.write(code)', 'exec(code)', 'etl', 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', 'code = import_module(\"transform.silver.mlv\", VERSION)\\n\\nprint(code, globals())'], '_oh': {6: <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, 7: LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')}, '_dh': [PosixPath('/Users/enricogoerlitz/Developer/fabricengineer-py/notebooks')], 'In': ['', 'import sys\\n\\nsys.path.append(\"../src\")\\n\\nfrom fabricengineer.import_module.import_module import import_module\\n\\nVERSION = \"0.0.9\"', 'import os\\nfrom pyspark.sql import SparkSession\\n\\n\\nclass NotebookUtilsFSMock:\\n    def _get_path(self, file: str) -> str:\\n        return os.path.join(os.getcwd(), file)\\n\\n    def exists(self, path: str) -> bool:\\n        return os.path.exists(self._get_path(path))\\n\\n    def put(\\n        self,\\n        file: str,\\n        content: str,\\n        overwrite: bool = False\\n    ) -> None:\\n        path = self._get_path(file)\\n        os.makedirs(os.path.dirname(path), exist_ok=True)\\n\\n        if os.path.exists(path) and not overwrite:\\n            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\\n        with open(path, \\'w\\') as f:\\n            f.write(content)\\n\\n\\nclass NotebookUtilsMock:\\n    def __init__(self):\\n        self.fs = NotebookUtilsFSMock()\\n\\nglobal spark\\nspark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\\n\\nglobal notebookutils\\nnotebookutils = NotebookUtilsMock()', 'code = import_module(\"transform.silver.insertonly\", VERSION)\\n\\nprint(code)', 'with open(\"./code.py\", \"w\") as f:\\n    f.write(code)', 'exec(code)', 'etl', 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', 'code = import_module(\"transform.silver.mlv\", VERSION)\\n\\nprint(code, globals())'], 'Out': {6: <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, 7: LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x107b89040>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x107b88e90>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x107b88e90>, 'open': <function open at 0x10678b560>, '_': LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table'), '__': <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, '___': '', 'logging': <module 'logging' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/logging/__init__.py'>, 'os': <module 'os' (frozen)>, 'uuid': <module 'uuid' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/uuid.py'>, 'json': <module 'json' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/json/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'sc': None, 'spark': <pyspark.sql.session.SparkSession object at 0x10c352540>, 'sqlContext': None, 'spark_session_id': '', 'create_spark_exception': None, 'script_version': '1.0.7', 'display': <function custom_display at 0x107be0540>, 'HTML': <class 'IPython.core.display.HTML'>, 'ipython_display': <function display at 0x1065b42c0>, 'fabric_envs': ['trident-spark-kernel', 'lighter', 'synapse-spark-kernel', 'fabric-synapse-runtime-1-1', 'fabric-synapse-runtime-1-2'], 'is_synapse_kernel': <function is_synapse_kernel at 0x107be04a0>, 'custom_display': <function custom_display at 0x107be0540>, 'textwrap': <module 'textwrap' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/textwrap.py'>, 'msal': <module 'msal' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/msal/__init__.py'>, 'requests': <module 'requests' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/requests/__init__.py'>, 'time': <module 'time' (built-in)>, 'Thread': <class 'threading.Thread'>, 'platform': <module 'platform' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/platform.py'>, 'os_name': 'Darwin', 'os_version': '24.5.0', 'extension_version': '0.2.0', 'refresh_token': None, 'DEFAULT_SCOPES': ['https://analysis.windows.net/powerbi/api/.default'], 'LOCAL_TOKEN_SERVER_PORT': '18888', 'LOCAL_TOKEN_SERVER_URL': 'http://localhost:18888/token', 'Config': <class '__main__.Config'>, 'PipConfig': <class '__main__.PipConfig'>, 'SessionStatus': <class '__main__.SessionStatus'>, 'SparkStatementStatus': <class '__main__.SparkStatementStatus'>, 'OutPut': <class '__main__.OutPut'>, 'Authentication': <class '__main__.Authentication'>, 'MWCClient': <class '__main__.MWCClient'>, 'where_json': <function where_json at 0x107be1080>, '__vsc_ipynb_file__': '/Users/enricogoerlitz/Developer/fabricengineer-py/notebooks/02 import exec.ipynb', '__DW_SCOPE__': {'output_cache': {}, 'output-f704a7d9-3c36-493c-bb2f-194a04425a39': {'enable_for_types': <function __DW_OUTPUT_FORMATTER__.<locals>.enable_formatter_for_types at 0x107e13a60>, 'disable': <function __DW_OUTPUT_FORMATTER__.<locals>.disable_formatter at 0x107e13b00>}}, '_i': 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', '_ii': 'etl', '_iii': 'exec(code)', '_i1': 'import sys\\n\\nsys.path.append(\"../src\")\\n\\nfrom fabricengineer.import_module.import_module import import_module\\n\\nVERSION = \"0.0.9\"', 'import_module': <function import_module at 0x1067a80e0>, 'VERSION': '0.0.9', '_i2': 'import os\\nfrom pyspark.sql import SparkSession\\n\\n\\nclass NotebookUtilsFSMock:\\n    def _get_path(self, file: str) -> str:\\n        return os.path.join(os.getcwd(), file)\\n\\n    def exists(self, path: str) -> bool:\\n        return os.path.exists(self._get_path(path))\\n\\n    def put(\\n        self,\\n        file: str,\\n        content: str,\\n        overwrite: bool = False\\n    ) -> None:\\n        path = self._get_path(file)\\n        os.makedirs(os.path.dirname(path), exist_ok=True)\\n\\n        if os.path.exists(path) and not overwrite:\\n            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\\n        with open(path, \\'w\\') as f:\\n            f.write(content)\\n\\n\\nclass NotebookUtilsMock:\\n    def __init__(self):\\n        self.fs = NotebookUtilsFSMock()\\n\\nglobal spark\\nspark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\\n\\nglobal notebookutils\\nnotebookutils = NotebookUtilsMock()', 'SparkSession': <class 'pyspark.sql.session.SparkSession'>, 'NotebookUtilsFSMock': <class '__main__.NotebookUtilsFSMock'>, 'NotebookUtilsMock': <class '__main__.NotebookUtilsMock'>, '__annotations__': {'spark': <class 'pyspark.sql.session.SparkSession'>}, 'notebookutils': <__main__.NotebookUtilsMock object at 0x10c2bca70>, '_i3': 'code = import_module(\"transform.silver.insertonly\", VERSION)\\n\\nprint(code)', 'code': 'from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\nfrom fabricengineer.logging.logger import logger\\n\\n\\ndef to_spark_sql(sql: str) -> str:\\n    return sql \\\\\\n            .replace(\"[\", \"`\") \\\\\\n            .replace(\"]\", \"`\")\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=is_testing_mock\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> \\'MaterializedLakeView\\':\\n        \"\"\"Initializes the MaterializedLakeView instance.\\n\\n        Args:\\n            lakehouse (str): The lakehouse name.\\n            schema (str): The schema name.\\n            table (str): The table name.\\n            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\\n            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\\n            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\\n            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\\n\\n        Returns:\\n            MaterializedLakeView: The initialized MaterializedLakeView instance.\\n        \"\"\"\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = is_testing_mock\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        \"\"\"Initializes the SparkSession instance.\\n        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global \\'spark\\' variable.\\n\\n        Args:\\n            spark_ (SparkSession): The SparkSession instance.\\n\\n        Returns:\\n            SparkSession | None: The initialized SparkSession instance or None.\\n        \"\"\"\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        \"\"\"Initializes the NotebookUtils instance.\\n        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global \\'notebookutils\\' variable.\\n\\n        Args:\\n            notebookutils_ (Any): The NotebookUtils instance.\\n\\n        Returns:\\n            Any | None: The initialized NotebookUtils instance or None.\\n        \"\"\"\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        \"\"\"Reads the content of the SQL file from the specified lakehouse.\\n        If the file does not exist, it returns None.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be read.\\n\\n        Returns:\\n            str | None: The content of the file or None if it doesn\\'t exist.\\n        \"\"\"\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \"r\") as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        \"\"\"Writes the SQL content to the specified file in a lakehouse.\\n        If the file already exists, it will be overwritten.\\n        If the file cannot be written, it raises a RuntimeError.\\n\\n        Args:\\n            sql (str): The SQL content to write.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be written.\\n\\n        Returns:\\n            bool: True if the file was written successfully, False otherwise.\\n        \"\"\"\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> DataFrame | None:\\n        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        logger.info(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame | None:\\n        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\\n        self.create_schema()\\n\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n        logger.info(f\"CREATE MLV: {self.table_path}\")\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        logger.info(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\\n\\n        Args:\\n            sql (str): The SQL query to create the MLV.\\n            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\\n\\n        Returns:\\n            DataFrame: The result of the create or replace operation.\\n        \"\"\"\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            logger.info(\"Nothing has changed.\")\\n            return None\\n\\n        logger.info(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        logger.info(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()', '_i4': 'with open(\"./code.py\", \"w\") as f:\\n    f.write(code)', 'f': <_io.TextIOWrapper name='./code.py' mode='w' encoding='UTF-8'>, '_i5': 'exec(code)', 'datetime': <class 'datetime.datetime'>, 'Callable': typing.Callable, 'ABC': <class 'abc.ABC'>, 'abstractmethod': <function abstractmethod at 0x104cfe840>, 'dataclass': <function dataclass at 0x1058fbb00>, 'uuid4': <function uuid4 at 0x1061dad40>, 'DataFrame': <class 'pyspark.sql.dataframe.DataFrame'>, 'F': <module 'pyspark.sql.functions' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pyspark/sql/functions.py'>, 'T': <module 'pyspark.sql.types' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pyspark/sql/types.py'>, 'Window': <class 'pyspark.sql.window.Window'>, 'logger': <Logger fabricengineer (INFO)>, 'LakehouseTable': <class '__main__.LakehouseTable'>, 'generate_uuid': <function generate_uuid at 0x107e844a0>, 'ConstantColumn': <class '__main__.ConstantColumn'>, 'get_mock_table_path': <function get_mock_table_path at 0x10c33bce0>, 'BaseSilverIngestionService': <class '__main__.BaseSilverIngestionService'>, 'SilverIngestionInsertOnlyService': <class '__main__.SilverIngestionInsertOnlyService'>, 'etl': <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, '_i6': 'etl', '_6': <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, '_i7': 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', 'table': LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table'), '_7': LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table'), '_i8': 'code = import_module(\"transform.silver.mlv\", VERSION)\\n\\nprint(code, globals())'}\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.mlv\", VERSION)\n",
    "\n",
    "print(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "637aeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b8cfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 13:04:02] [INFO] fabricengineer REPLACE MLV: TestLakehouse.schema.table_mlv\n",
      "[05.08.2025 13:04:02] [INFO] fabricengineer DROP MATERIALIZED LAKE VIEW IF EXISTS TestLakehouse.schema.table_mlv\n",
      "[05.08.2025 13:04:02] [INFO] fabricengineer CREATE SCHEMA IF NOT EXISTS TestLakehouse.schema\n",
      "[05.08.2025 13:04:02] [INFO] fabricengineer CREATE MLV: TestLakehouse.schema.table_mlv\n"
     ]
    }
   ],
   "source": [
    "mlv.init(\n",
    "    lakehouse=\"TestLakehouse\",\n",
    "    schema=\"schema\",\n",
    "    table=\"table\",\n",
    "    is_testing_mock=True\n",
    ")\n",
    "\n",
    "sql = \"SELECT * FROM TestLakehouse.schema.table2\"\n",
    "mlv.create_or_replace(sql, mock_is_existing=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
