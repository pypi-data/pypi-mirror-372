{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./static/pretraining_header.png\" width=\"289px\" style=\"height: auto;\"></img>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll look at how we can pretrain BabyBERT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì¶ Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing the dependencies we'll need for training BabyBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babybert.data import CollatorForMLM, LanguageModelingDataset, load_corpus\n",
    "from babybert.model import BabyBERT, BabyBERTConfig, BabyBERTForMLM\n",
    "from babybert.tokenizer import WordPieceTokenizer\n",
    "from babybert.trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ Loading our pretrained tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load a pretrained tokenizer. We'll use the one we trained in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPieceTokenizer.from_pretrained(\"./checkpoints/toy-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìö Assembling our training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly pretrain a language model, you need a vast corpus of diverse, unstructured texts. For the sake of this example, we'll use a text file containing around 1,000 raw English sentences.\n",
    "\n",
    "We'll also encode the corpus using our pretrained tokenizer, which converts each sentence into a list of token IDs and attention masks. These token IDs and masks will serve as training examples for our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\"./data/corpus.txt\")\n",
    "encoded = tokenizer.batch_encode(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset object to store the encoded corpus; thankfully, `LanguageModelingDataset` has a built-in `from_dict` method we can use to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LanguageModelingDataset.from_dict(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öôÔ∏è Instantiating the BabyBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the configuration settings for our BabyBERT model and instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = BabyBERTConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    block_size=dataset.seq_length,\n",
    ")\n",
    "\n",
    "model = BabyBERT(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pretraining, we need to add a masked language modeling head to BabyBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = BabyBERTForMLM(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí™ Instantiating the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a collator that we can use to automatically mask our input sequences for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = CollatorForMLM(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use that collator as part of the configuration for our trainer! The trainer will automatically perform masking for us when it creates a batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cfg = TrainerConfig(\n",
    "    collator=collator, batch_size=16, num_workers=4, num_samples=1000\n",
    ")\n",
    "\n",
    "trainer = Trainer(mlm_model, trainer_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãÔ∏è Training BabyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything ready to go now - let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[33m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 1008/1008 [03:37<00:00,  4.63samples/s, loss=6.0447]\n"
     ]
    }
   ],
   "source": [
    "trainer.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üíæ Saving the pretrained BabyBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our pretrained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./checkpoints/toy-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
