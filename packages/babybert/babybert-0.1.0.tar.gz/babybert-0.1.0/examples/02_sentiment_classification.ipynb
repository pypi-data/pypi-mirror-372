{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "        <img src=\"./static/sentiment_header.png\" width=\"570px\" style=\"height: auto;\"></img>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate how we can use a pretrained BabyBERT model to detect the sentiment expressed by a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì¶ Importing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import all the dependencies needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babybert.data import LanguageModelingDataset, load_dataset\n",
    "from babybert.model import BabyBERT, BabyBERTForSentimentClassification\n",
    "from babybert.tokenizer import WordPieceTokenizer\n",
    "from babybert.trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚¨ÜÔ∏è Loading our pretrained tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained tokenizer and BabyBERT model checkpoints from the previous two notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = \"./checkpoints/toy-model\"\n",
    "tokenizer = WordPieceTokenizer.from_pretrained(checkpoint_directory)\n",
    "model = BabyBERT.from_pretrained(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìö Building our training dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenizer and model prepared, we can start assembling our dataset. The sentiment classification dataset we will be using contains two rows for each sample. The first row contains the sample text, and the second row contains a binary label indicating whether the text has a positive or negative sentiment (0 for negative, 1 for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"./data/sentiment_classification.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the a few samples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love this movie so much!\n",
      "Label: 1 (positive)\n",
      "\n",
      "Sentence: This was the worst meal I've ever had.\n",
      "Label: 0 (negative)\n",
      "\n",
      "Sentence: What a fantastic day, everything went perfectly.\n",
      "Label: 1 (positive)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence, label in list(zip(*dataset.values()))[:3]:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Label: {label} ({'positive' if label else 'negative'})\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have of fine-tuning dataset loaded, let's encode it using our tokenizer to obtain token IDs and attention masks.\n",
    "\n",
    "Note that we pass in `model.config.block_size` for the `padding_length` argument. `block_size` specifies the length of the token sequences that our model expects, so we want to pad each sequence in our dataset to that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.batch_encode(\n",
    "    dataset[\"text\"], padding_length=model.config.block_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use `LanguageModelingDataset.from_dict` to create a dataset object from the token IDs, attention masks, and sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = LanguageModelingDataset.from_dict(\n",
    "    {**encoded, \"labels\": dataset[\"label\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí≠ Setting up sentiment analysis head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our `BabyBERT` model produces a contextual embedding for each token in the input sequence as output. However, for sentiment classification, we want class predictions as output - one for the negative class, one for the positive class.\n",
    "\n",
    "To do this, we can use the `BabyBERTForSentimentClassification` class like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_model = BabyBERTForSentimentClassification(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí™ Instantiating the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's configure and instantiate our trainer. For the sake of example, we'll train on a minute number of samples. In a production setting, we would want to train on far more samples than this over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cfg = TrainerConfig(batch_size=16, num_workers=4, num_samples=1000)\n",
    "\n",
    "trainer = Trainer(sentiment_analysis_model, trainer_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãÔ∏è Fine-tuning BabyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the pieces in place. Let's fine-tune our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[33m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 1008/1008 [03:40<00:00,  4.57samples/s, loss=0.6802]\n"
     ]
    }
   ],
   "source": [
    "trainer.run(training_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
