# This template uses a single node to download, run, and upload results
<% if USE_PVC.enabled %>
name: <<TEMPLATE_NAME>>-<<JOB_ID>>
<% else %>
name: <<TEMPLATE_NAME>>
<% endif %>
<% if TERMIN_GRACE_SEC > 0 %>
terminationGracePeriodSeconds: <<TERMIN_GRACE_SEC>>
lifecycle:
  preStop:
    exec:
      command: ["/bin/sh", "-c", "echo 'PreStop: Cleaning up'; sleep 10"]
<% endif %>
<% if ENABLE_ON_DEMAND %>
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: karpenter.sh/capacity-type
              operator: In
              values:
                - on-demand
            # - key: easi.csiro.au/node-class
            #   operator: In
            #   values:
            #     - general-purpose
<% endif %>
<% if RETRY > 0 %>
retryStrategy:
  retryPolicy: OnError
  limit: <<RETRY>>
<% endif %>
volumes:
  <% for secret in MOUNT_SECRETS %>
  - name: secret-volume-<< loop.index >>
    secret:
      secretName: << secret.name >>
  <% endfor %>
container:
  envFrom:
    <% for ENV_SECRET in ENV_SECRETS %>
    - secretRef:
        name: << ENV_SECRET >>
    <% endfor %>
    <% for ENV_CONFIG in ENV_CONFIGS %>
    - configMapRef:
        name: << ENV_CONFIG >>
    <% endfor %>
  volumeMounts:
    <% for secret in MOUNT_SECRETS %>
    - name: secret-volume-<< loop.index >>
      mountPath: << secret.mount_path >>
    <% endfor %>
    <% if USE_PVC.enabled %>
    - name: workspace-<< JOB_ID >>
      mountPath: <<APP_INPUT_PATH.rstrip("/")>>
      subPath: <<APP_INPUT_PATH.lstrip("/").rstrip("/")>>
    <% if APP_OUTPUT_PATH is not none %>
    - name: workspace-<< JOB_ID >>
      mountPath: <<APP_OUTPUT_PATH.rstrip("/")>>
      subPath: <<APP_OUTPUT_PATH.lstrip("/").rstrip("/")>>
    <% endif %>
    <% if APP_WORK_DIR is not none %>
    - name: workspace-<< JOB_ID >>
      mountPath: <<APP_WORK_DIR.rstrip("/")>>
      subPath: <<APP_WORK_DIR.lstrip("/").rstrip("/")>>
    <% endif %>
    <% endif %>
  env:
    <% for key, value in ENV_VARS.items() %>
      - name: << key >>
        value: << value >>
    <% endfor %>
  args:
  - |-
    <% if not IS_ERR_TOLER %>
    set -e;
    <% endif %>
    echo Step-Pod: "{{pod.name}}";

    function graceful_exit {
      echo "Cleaning up artifacts and logs..."
      copy_download_logs
    }
    trap graceful_exit SIGTERM
    trap graceful_exit EXIT

    # PRE - COMMAND
    # =============
    <<APP_PRE_COMMAND>>;

    # CHECK REQUIRED PACKAGE DEPENDENCIES
    # ===================================
    if ! command -v curl &> /dev/null; then
    echo "curl is not installed.";
    apt-get update && apt-get install -y curl;
    fi

    if ! command -v jq &> /dev/null; then
    echo "jq is not installed.";
    apt-get update && apt-get install -y jq;
    fi

    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    if ! command -v aws &> /dev/null; then
    echo "Installing AWS CLI v2...";
    apt-get update && apt-get install -y unzip;
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip";
    unzip awscliv2.zip;
    ./aws/install;
    rm -rf awscliv2.zip ./aws;
    fi
    <% endif %>

    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    if ! command -v gsutil &> /dev/null; then
    echo "Installing gsutil via Google Cloud SDK...";
    echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list;
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -;
    apt-get update && apt-get install -y google-cloud-sdk;
    fi
    <% endif %>


    # CONFIGURE IN/OUT PATHS
    # ======================
    echo Configuring in/out paths;
    path="<<APP_INPUT_PATH>>";
    outPath=<<APP_OUTPUT_PATH>>;
    if [ -d "$path" ]; then
      echo "The path exists."
    else
      echo "The path does not exist. Creating the path now..."
      mkdir -p "$path"

      if [ $? -eq 0 ]; then
        echo "Path successfully created."
      else
        echo "Failed to create path."
      fi
    fi;

    function copy_download_logs {
        echo "No files to download"
    };

    # CONFIGURE GCP STORAGE - DOWNLOAD
    # ================================
    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    echo Configuring gcp cloud storage to download input files...

    echo "$<<SECRET_KEY>>" > /tmp/firebase.json
    export GOOGLE_APPLICATION_CREDENTIALS=/tmp/firebase.json

    function copy_download_logs {
      <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS)<% endif %>
      gsutil -m cp -r /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
    };

    <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS);<% endif %>
    FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
    for public_url in "${FILES[@]}"; do
    file=$(basename $public_url)
    gsutil cp $public_url $path/$file;
    done
    <% endif %>

    # CONFIGURE S3 STORAGE - DOWNLOAD
    # ===============================
    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    echo Configuring s3 cloud storage to download input files...

    if [[ ! "<<STORAGE_ENDPOINT_URL>>" == *"s3"* ]]; then
      echo "Changing AWS_ENDPOINT_URL to: https://<<STORAGE_ENDPOINT_URL>> "
      export AWS_ENDPOINT_URL="https://<<STORAGE_ENDPOINT_URL>>"
    fi

    function copy_download_logs {
      aws s3 cp /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
    };

    FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
    for public_url in "${FILES[@]}"; do
    file=$(basename $public_url)
    aws s3 cp $public_url $path/$file;
    done
    <% endif %>


    # INPUT PARAMS: param.json
    # ====================================
    echo {{inputs.parameters.JOB_PARAMETERS}} > $path/param.json;

    # MAIN COMMAND
    # ============
    <<APP_MAIN_COMMAND>>;

    # POST COMMAND
    # ============
    <<APP_POST_COMMAND>>;


    # UPLOADS
    # ============

    <% if USE_PVC.enabled %>
    local_path="<<APP_OUTPUT_PATH>>"
    <% else %>
    local_path="/tmp/output"
    <% endif %>

    <% if HAS_UPLOAD %>

    # Install zip
    echo "Installing zip..."
    apt-get update && apt-get install -y zip

    upload_copy_paths=$(echo '{{inputs.parameters.upload-copy-paths}}' | jq -r '.[]');
    selected_files=(<% for file in JOB.uploads.selected_outputs %>"<< file >>"<% if not loop.last %> <% endif %><% endfor %>);

    # Check selected files or all
    if [ -z "${selected_files[*]}" ]; then
      echo $outPath;
    else
      mkdir -p $local_path/tmp/selected;

      # Iterate over selected files and copy them to the new directory
      for file in "${selected_files[@]}"; do
        if [ -f "$outPath/${file}" ]; then
          cp "$outPath/${file}" "$local_path/tmp/selected/"
        elif [ -d "$outPath/${file}" ]; then
          cp -r "$outPath/${file}" "$local_path/tmp/selected/"
        fi
      done

      outPath="$local_path/tmp/selected"
    fi

    <% if JOB.uploads.is_custom_upload %>
    targetPath={{inputs.parameters.upload-custom-path}};
    <% else %>
    targetPath={{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}};
    <% endif %>


    # Check if zip, zip outputs
    if {{inputs.parameters.zip-outputs}}; then
      cd $outPath;
      zip -r $local_path/tmp/output.zip .;
      cd -;
    fi



    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    echo Uploading results to GCP storage...

    if {{inputs.parameters.zip-outputs}}; then
      gsutil -m cp $local_path/tmp/output.zip $targetPath/output.zip;
    else
      gsutil -m cp -r $outPath/** $targetPath;
    fi

    for p in $upload_copy_paths; do
      gsutil -m cp -r  $targetPath/** $p;  # cloud-2-cloud copy
    done
    <% endif %>

    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    echo Uploading results to S3 storage...

    if {{inputs.parameters.zip-outputs}}; then
      aws s3 cp  $local_path/tmp/output.zip $targetPath/output.zip;
    else
      aws s3 cp $outPath/ $targetPath/ --recursive;
    fi

    for p in $upload_copy_paths; do
      aws s3 cp $targetPath/ $p --recursive;  # cloud-2-cloud copy
    done
    <% endif %>
    <% endif %>
    sleep 5

  command:
  - /bin/bash
  - -c
  resources:
    requests:
      memory: <<MEM_REQ>>
      cpu: <<CPU_REQ>>
      <% if EPS_REQ is not none %>
      ephemeral-storage: <<EPS_REQ>>
      <% endif %>
    limits:
      memory: <<MEM_LIM>>
      cpu: <<CPU_LIM>>
      <% if EPS_LIM is not none %>
      ephemeral-storage: <<EPS_LIM>>
      <% endif %>
  image: <<IMAGE_URL>>
  imagePullPolicy: <<IMG_PULL_POLICY>>
inputs:
  parameters:
  - name: files-json
  - name: upload-base-path
  - name: JOB_PARAMETERS
  - name: upload-copy-paths
  - name: zip-outputs
  <% for i in range(MAX_NUM) %>
  - name: dep-art-loc-<<i>>
    default: no-location
  <% endfor %>
  <% for i in range(MAX_NUM) %>
  - name: dep-art-<<i>>
    optional: true
    path: <<APP_INPUT_PATH>>/{{inputs.parameters.dep-art-loc-<<i>>}}/
  <% endfor %>
outputs:
  parameters:
  - name: pod-name
    value: '{{pod.name}}'
