<p align="right">
<a href="https://pypi.org/project/lm-proxy/" target="_blank"><img src="https://badge.fury.io/py/lm-proxy.svg" alt="PYPI Release"></a>
<a href="https://github.com/Nayjest/lm-proxy/actions/workflows/code-style.yml" target="_blank"><img src="https://github.com/Nayjest/lm-proxy/actions/workflows/code-style.yml/badge.svg" alt="Code Style"></a>
<a href="https://github.com/Nayjest/lm-proxy/actions/workflows/tests.yml" target="_blank"><img src="https://github.com/Nayjest/lm-proxy/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
<a href="https://github.com/Nayjest/lm-proxy/blob/main/LICENSE" target="_blank"><img src="https://img.shields.io/static/v1?label=license&message=MIT&color=d08aff" alt="License"></a>
</p>

# LLM Proxy Server

**LLM Proxy Server** is OpenAI-compatible http proxy server for inferencing various LLMs capable of working with Google, Anthropic, OpenAI APIs, local PyTorch inference, etc.

**Development Status**: bookmark it and go away, it is still in early development.

## ✨ Features

- @todo


## 🚀 Quickstart
```bash
# Install LLM Proxy Server via pip
pip install llm-proxy-server

```

## 🤝 Contributing

We ❤️ contributions! See [CONTRIBUTING.md](CONTRIBUTING.md).

## 📝 License

Licensed under the [MIT License](LICENSE).

© 2022&mdash;2025 [Vitalii Stepanenko](mailto:mail@vitaliy.in)
