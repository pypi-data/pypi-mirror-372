import pytest
from mcp_eval import Expect
from mcp_eval.session import TestAgent

{% for s in scenarios %}
@pytest.mark.asyncio
async def test_{{ s.name | py_ident }}(agent: TestAgent):
    response = await agent.generate_str({{ s.prompt | tojson }})
    {% for a in s.assertions %}
    {% if a.kind == 'response_contains' %}
    await agent.session.assert_that(Expect.content.contains({{ a.text | tojson }}, case_sensitive={{ a.case_sensitive|default(False) }}), response=response)
    {% elif a.kind == 'tool_was_called' %}
    await agent.session.assert_that(Expect.tools.was_called({{ a.tool_name | tojson }}, min_times={{ a.min_times|default(1) }}))
    {% elif a.kind == 'llm_judge' %}
    await agent.session.assert_that(Expect.judge.llm({{ a.rubric | tojson }}, min_score={{ a.min_score|default(0.8) }}), response=response)
    {% elif a.kind == 'tool_sequence' %}
    await agent.session.assert_that(Expect.tools.sequence({{ a.sequence | tojson }}, allow_other_calls={{ a.allow_other_calls|default(False) }}))
    {% elif a.kind == 'tool_called_with' %}
    await agent.session.assert_that(Expect.tools.called_with({{ a.tool_name | tojson }}, {{ a.arguments | py }}))
    {% elif a.kind == 'tool_output_matches' %}
    await agent.session.assert_that(Expect.tools.output_matches(tool_name={{ a.tool_name | tojson }}, expected_output={{ a.expected_output | py }}, field_path={{ a.field_path | py }}, match_type={{ a.match_type | tojson }}, case_sensitive={{ a.case_sensitive|default(True) }}, call_index={{ a.call_index|default(-1) }}))
    {% elif a.kind == 'max_iterations' %}
    await agent.session.assert_that(Expect.performance.max_iterations({{ a.max_iterations }}))
    {% elif a.kind == 'response_time_under' %}
    await agent.session.assert_that(Expect.performance.response_time_under({{ a.ms }}))
    {% endif %}
    {% endfor %}

{% endfor %}


