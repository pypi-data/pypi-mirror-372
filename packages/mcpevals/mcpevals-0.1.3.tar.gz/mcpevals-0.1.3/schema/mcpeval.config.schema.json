{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/lastmile-ai/mcp-eval/main/schema/mcpeval.config.schema.json",
  "title": "MCP-Eval Configuration",
  "description": "Configuration schema for MCP-Eval",
  "allOf": [
    {
      "$ref": "https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/schema/mcp-agent.config.schema.json"
    },
    {
      "type": "object",
      "properties": {
        "$schema": {
          "type": "string",
          "description": "Reference to this JSON schema for validation and autocomplete"
        },
        "eval_name": {
          "type": "string",
          "description": "Name of the evaluation test suite",
          "default": "MCP-Eval Test Suite"
        },
        "eval_description": {
          "type": "string",
          "description": "Description of what this test suite evaluates",
          "default": "Comprehensive evaluation of MCP servers"
        },
        "judge": {
          "$ref": "#/definitions/JudgeConfig",
          "description": "Configuration for the LLM judge that evaluates test results"
        },
        "metrics": {
          "$ref": "#/definitions/MetricsConfig",
          "description": "Configuration for metrics collection during test execution"
        },
        "reporting": {
          "$ref": "#/definitions/ReportingConfig",
          "description": "Configuration for test result reporting"
        },
        "execution": {
          "$ref": "#/definitions/ExecutionConfig",
          "description": "Configuration for test execution behavior"
        },
        "default_servers": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of default server names to use for tests",
          "default": []
        },
        "provider": {
          "type": "string",
          "description": "Default LLM provider for test execution",
          "default": "anthropic",
          "examples": [
            "anthropic",
            "openai",
            "bedrock",
            "azure",
            "google"
          ]
        },
        "model": {
          "type": "string",
          "description": "Default model to use for test execution",
          "default": "claude-sonnet-4-0",
          "examples": [
            "claude-3-5-haiku-20241022",
            "claude-sonnet-4-0",
            "gpt-4-turbo",
            "gpt-4o"
          ]
        },
        "default_agent": {
          "oneOf": [
            {
              "$ref": "https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/schema/mcp-agent.config.schema.json#/$defs/AgentSpec"
            },
            {
              "type": "string",
              "description": "Name of an AgentSpec defined in subagents"
            },
            {
              "type": "null"
            }
          ],
          "description": "Default agent configuration for tests"
        }
      }
    }
  ],
  "definitions": {
    "JudgeConfig": {
      "type": "object",
      "description": "Configuration for the LLM judge that evaluates test results",
      "properties": {
        "provider": {
          "type": [
            "string",
            "null"
          ],
          "description": "Judge-specific LLM provider (falls back to global provider)",
          "examples": [
            "anthropic",
            "openai",
            "bedrock",
            "azure",
            "google"
          ]
        },
        "model": {
          "type": [
            "string",
            "null"
          ],
          "description": "Model to use for judging (falls back to global model)",
          "default": null,
          "examples": [
            "claude-3-5-haiku-20241022",
            "claude-sonnet-4-0",
            "gpt-4-turbo"
          ]
        },
        "min_score": {
          "type": "number",
          "description": "Minimum score threshold for passing tests",
          "default": 0.8,
          "minimum": 0,
          "maximum": 1
        },
        "max_tokens": {
          "type": "integer",
          "description": "Maximum tokens for judge responses",
          "default": 1000,
          "minimum": 1
        },
        "system_prompt": {
          "type": "string",
          "description": "System prompt for the judge",
          "default": "You are an expert evaluator of AI assistant responses."
        }
      },
      "additionalProperties": false
    },
    "MetricsConfig": {
      "type": "object",
      "description": "Configuration for metrics collection during test execution",
      "properties": {
        "collect": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "response_time",
              "tool_coverage",
              "iteration_count",
              "token_usage",
              "cost_estimate",
              "error_recovery",
              "content_extraction_quality",
              "input_tokens",
              "output_tokens"
            ]
          },
          "description": "Metrics to collect during test execution",
          "default": [
            "response_time",
            "tool_coverage",
            "iteration_count",
            "token_usage",
            "cost_estimate"
          ]
        }
      },
      "additionalProperties": false
    },
    "ReportingConfig": {
      "type": "object",
      "description": "Configuration for test result reporting",
      "properties": {
        "formats": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "json",
              "markdown",
              "html",
              "junit"
            ]
          },
          "description": "Output formats for test reports",
          "default": [
            "json",
            "markdown"
          ]
        },
        "output_dir": {
          "type": "string",
          "description": "Directory for test report output",
          "default": "./test-reports"
        },
        "include_traces": {
          "type": "boolean",
          "description": "Include detailed execution traces in reports",
          "default": true
        }
      },
      "additionalProperties": false
    },
    "ExecutionConfig": {
      "type": "object",
      "description": "Configuration for test execution behavior",
      "properties": {
        "max_concurrency": {
          "type": "integer",
          "description": "Maximum number of concurrent test executions",
          "default": 5,
          "minimum": 1
        },
        "timeout_seconds": {
          "type": "integer",
          "description": "Timeout for individual test execution in seconds",
          "default": 300,
          "minimum": 1
        },
        "retry_failed": {
          "type": "boolean",
          "description": "Whether to retry failed tests",
          "default": false
        }
      },
      "additionalProperties": false
    }
  }
}