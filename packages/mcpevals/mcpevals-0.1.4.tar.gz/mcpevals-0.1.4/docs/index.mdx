---
title: "MCP-Eval Documentation"
description: "The comprehensive testing framework for MCP servers and tool-using agents."
icon: "house"
sidebarTitle: "Home"
---

<Note>
  **Your flight simulator for MCP servers and agents** — Connect agents to real MCP servers, run realistic scenarios, and calculate metrics for tool calls and more.
</Note>

<Info>
  [Model Context Protocol](https://modelcontextprotocol.io/docs/getting-started/intro)  standardizes how applications provide context to large language models (LLMs). Think of MCP like a USB-C port for AI applications. 
  
  
  **MCP-Eval** ensures your MCP servers, and agents built with them, work reliably in production.
</Info>

## Get Started in 30 Seconds

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:

<CodeGroup>
```bash uv
# Install MCP-Eval
uv add mcpevals

# Initialize your project
mcp-eval init

# Run your first test
mcp-eval run examples/fetch_test.py
```

```bash pip
# Install MCP-Eval
pip install mcpevals

# Initialize your project
mcp-eval init

# Run your first test
mcp-eval run examples/fetch_test.py
```
</CodeGroup>

<Check>You're ready to start testing! [Continue with the Quickstart →](./quickstart)</Check>

## What MCP-Eval Does for You

<Columns cols={2}>
  <Card title="Test MCP Servers" icon="server">
    Ensure your MCP servers respond correctly to agent requests and handle edge cases gracefully
  </Card>
  <Card title="Evaluate Agents" icon="robot">
    Measure how effectively agents use tools, follow instructions, and recover from errors
  </Card>
  <Card title="Track Performance" icon="chart-line">
    Monitor latency, token usage, cost, and success rates with OpenTelemetry-backed metrics
  </Card>
  <Card title="Assert Quality" icon="check-circle">
    Use structural checks, LLM judges, and path efficiency validators to ensure high quality
  </Card>
</Columns>

## Why Teams Choose MCP-Eval

- **Production-ready**: Built on OpenTelemetry for enterprise-grade observability
- **Multiple test styles**: Choose between decorators, pytest, or dataset-driven testing
- **Rich assertions**: Content checks, tool verification, performance gates, and LLM judges
- **CI/CD friendly**: GitHub Actions support, JSON/HTML reports, and regression detection
- **Language agnostic**: Test MCP servers written in any language

## Quick Navigation

<Columns cols={3}>
  <Card title="Quickstart" icon="rocket" href="./quickstart">
    Get up and running in 5 minutes
  </Card>
  <Card title="Common Workflows" icon="graduation-cap" href="./common-workflows">
    Step-by-step guides for typical tasks
  </Card>
  <Card title="API Reference" icon="code" href="./api-catalog">
    Complete assertion catalog and APIs
  </Card>
</Columns>

## Learning Path

<Tabs>
  <Tab title="Getting Started">
    <Columns cols={1}>
      <Card title="Overview" icon="map" href="./overview">
        Understand MCP-Eval's architecture and philosophy
      </Card>
      <Card title="Quickstart" icon="rocket" href="./quickstart">
        Your first test in 5 minutes
      </Card>
      <Card title="Concepts" icon="lightbulb" href="./concepts">
        Core concepts and terminology
      </Card>
    </Columns>
  </Tab>
  
  <Tab title="Writing Tests">
    <Columns cols={1}>
      <Card title="Assertions" icon="check" href="./assertions">
        The unified Expect API for all assertions
      </Card>
      <Card title="Common Workflows" icon="route" href="./common-workflows">
        Practical testing patterns
      </Card>
      <Card title="Test Generation" icon="wand-magic-sparkles" href="./test-generation">
        AI-powered test creation
      </Card>
    </Columns>
  </Tab>
  
  <Tab title="Evaluation Types">
    <Columns cols={1}>
      <Card title="Server Evaluation" icon="server" href="./server-evaluation">
        Testing MCP server implementations
      </Card>
      <Card title="Agent Evaluation" icon="robot" href="./agent-evaluation">
        Measuring agent effectiveness
      </Card>
      <Card title="Datasets" icon="database" href="./datasets">
        Systematic evaluation suites
      </Card>
    </Columns>
  </Tab>
  
  <Tab title="Configuration">
    <Columns cols={1}>
      <Card title="Configuration" icon="gear" href="./configuration">
        Settings and customization
      </Card>
      <Card title="CI/CD" icon="circle-play" href="./ci-cd">
        GitHub Actions and automation
      </Card>
      <Card title="Reports" icon="chart-bar" href="./reports">
        Understanding test outputs
      </Card>
    </Columns>
  </Tab>
  
  <Tab title="Reference">
    <Columns cols={1}>
      <Card title="CLI Reference" icon="terminal" href="./cli-reference">
        Complete command documentation
      </Card>
      <Card title="API Reference" icon="code" href="./api-catalog">
        Detailed API documentation
      </Card>
      <Card title="Troubleshooting" icon="wrench" href="./troubleshooting">
        Common issues and solutions
      </Card>
      <Card title="FAQ" icon="question-circle" href="./faq">
        Frequently asked questions
      </Card>
    </Columns>
  </Tab>
</Tabs>

## Example: Your First Test

<CodeGroup>
```python test_fetch.py
from mcp_eval import task, Expect

@task("Verify fetch server works correctly")
async def test_fetch(agent, session):
    # Ask the agent to fetch a webpage
    response = await agent.generate_str("Fetch https://example.com and summarize it")
    
    # Assert the right tool was called
    await session.assert_that(Expect.tools.was_called("fetch"))
    
    # Verify the content is correct
    await session.assert_that(Expect.content.contains("Example Domain"), response=response)
    
    # Check performance
    await session.assert_that(Expect.performance.response_time_under(5000))
```

```python pytest_style.py
import pytest
from mcp_eval import create_agent, Expect

@pytest.mark.asyncio
async def test_fetch_with_pytest():
    agent = await create_agent("claude-3-5-sonnet")
    response = await agent.generate_str("Fetch https://example.com")
    
    assert "Example Domain" in response
    assert agent.tools_called == ["fetch"]
```
</CodeGroup>

<Tip>[See more examples →](./examples)</Tip>

## Join the Community

<Columns cols={2}>
  <Card title="GitHub" icon="github" href="https://github.com/lastmile-ai/mcp-eval">
    Report issues and contribute
  </Card>
  <Card title="Discord" icon="discord" href="https://discord.gg/mcp-eval">
    Get help and share experiences
  </Card>
</Columns>

