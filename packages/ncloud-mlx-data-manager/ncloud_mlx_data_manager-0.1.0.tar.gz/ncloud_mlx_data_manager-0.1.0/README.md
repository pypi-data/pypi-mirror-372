# ML expert Platform Data Manager SDK

ML expert Platform Data Manager SDKëŠ” **Naver Cloud Platformì˜ ML expert Platform Data Manager**ì™€ í˜¸í™˜ë˜ëŠ” ë°ì´í„°ì…‹ ê´€ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ SDKëŠ” **Hugging Face Datasets APIì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜**ë˜ì–´ ê¸°ì¡´ Hugging Face ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

- **ğŸ¤— Hugging Face Datasets í˜¸í™˜**: ê¸°ì¡´ Hugging Face Datasets APIì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- **â˜ï¸ ML expert Platform Data Manager ì—°ë™**: Naver Cloud Platformì˜ ML expert Platform Data Managerì™€ ì™„ë²½ ì—°ë™
- **ğŸ“Š ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ ì§€ì›**: CSV, JSON, Parquet, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, í…ìŠ¤íŠ¸ ë“± ëª¨ë“  í˜•ì‹ ì§€ì›
- **âš¡ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë°**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ë„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
- **ğŸ”„ ê°„í¸í•œ ë°ì´í„° ì „ì²˜ë¦¬**: map, filter, select ë“± ê°•ë ¥í•œ ë°ì´í„° ë³€í™˜ ê¸°ëŠ¥
- **ğŸ” ë³´ì•ˆ**: ì•ˆì „í•œ API í‚¤ ê¸°ë°˜ ì¸ì¦ ì‹œìŠ¤í…œ

## ğŸ“¦ ì„¤ì¹˜

### pipì„ ì‚¬ìš©í•œ ì„¤ì¹˜

```bash
pip install ncloud-mlx[data-manager]
```

## âš™ï¸ í™˜ê²½ ì„¤ì •

ML expert Platform Data Manager SDKë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# ML expert Platform Data Manager ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
export MLX_ENDPOINT_URL="your-mlx-endpoint-url"

# API í‚¤ ì„¤ì •
export MLX_APIKEY="your-api-key-here"
```

## ğŸƒâ€â™‚ï¸ ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from mlx.sdk.data import load_dataset, login
from datasets import concatenate_datasets

# ML expert Platform Data Managerì— ë¡œê·¸ì¸
login("your-api-key")

# ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ
local_dataset = load_dataset("./path/to/your/data")

# ML expert Platform Data Managerì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ
remote_dataset = load_dataset("mlx-data-manager/dataset-name")

# ë°ì´í„°ì…‹ ê²°í•©
combined_dataset = concatenate_datasets([
    local_dataset["train"], 
    remote_dataset["train"]
])

# ML expert Platform Data Managerì— ì—…ë¡œë“œ
combined_dataset.push_to_hub("mlx-data-manager/new-dataset")
```

### ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ

ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```python
from mlx.sdk.data import load_dataset

# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("mlx-data-manager/large-dataset", streaming=True)

# ë°ì´í„°ë¥¼ ì¦‰ì‹œ ë°˜ë³µ ì²˜ë¦¬
for example in dataset["train"]:
    print(example)
    break  # ì²« ë²ˆì§¸ ì˜ˆì œë§Œ ì¶œë ¥
```

### ë°ì´í„° ì „ì²˜ë¦¬

```python
from mlx.sdk.data import load_dataset
from transformers import AutoTokenizer

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("mlx-data-manager/text-dataset")

# í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶”ê°€
dataset_with_length = dataset.map(
    lambda x: {"length": len(x["text"])}
)

# í† í¬ë‚˜ì´ì œì´ì…˜
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
tokenized_dataset = dataset.map(
    lambda x: tokenizer(x['text']), 
    batched=True
)
```

## ğŸ“– ì§€ì›í•˜ëŠ” ë°ì´í„° í˜•ì‹

ML expert Platform Data Manager SDKëŠ” ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:

- **í…ìŠ¤íŠ¸**: CSV, JSON, TXT, Parquet
- **ì´ë¯¸ì§€**: JPEG, PNG, WebP, TIFF
- **ì˜¤ë””ì˜¤**: WAV, MP3, FLAC
- **ë¹„ë””ì˜¤**: MP4, AVI, MOV
- **ê¸°íƒ€**: Arrow, Feather, Excel

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### ë°ì´í„°ì…‹ ì„¤ì • ë° ë¶„í• 

```python
from datasets import get_dataset_config_names, get_dataset_split_names

# ì‚¬ìš© ê°€ëŠ¥í•œ ì„¤ì • í™•ì¸
configs = get_dataset_config_names("mlx-data-manager/dataset-name")
print(f"Available configs: {configs}")

# ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„í•  í™•ì¸
splits = get_dataset_split_names("mlx-data-manager/dataset-name")
print(f"Available splits: {splits}")
```

### íƒœê·¸ ë° ë²„ì „ ê´€ë¦¬

```python
from huggingface_hub import create_tag, list_repo_refs

# íƒœê·¸ ìƒì„±
create_tag(
    repo_id="mlx-data-manager/dataset-name",
    repo_type="dataset",
    tag="v1.0",
    tag_message="First stable release"
)

# íƒœê·¸ ëª©ë¡ í™•ì¸
refs = list_repo_refs(repo_id="mlx-data-manager/dataset-name", repo_type="dataset")
print([tag.name for tag in refs.tags])
```

### ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ

```python
from huggingface_hub import create_repo, upload_large_folder

# ì €ì¥ì†Œ ìƒì„±
create_repo(
    repo_id="mlx-data-manager/large-dataset",
    repo_type="dataset"
)

# ëŒ€ìš©ëŸ‰ í´ë” ì—…ë¡œë“œ
upload_large_folder(
    repo_id="mlx-data-manager/large-dataset",
    repo_type="dataset",
    folder_path="./path/to/large/dataset"
)
```

## ğŸ” ë°ì´í„°ì…‹ ê²€ìƒ‰

```python
from huggingface_hub import list_datasets

# ë°ì´í„°ì…‹ ê²€ìƒ‰
datasets = list_datasets(search="keyword")
for dataset in datasets:
    print(f"Dataset: {dataset.id}")
```

## ğŸ“Š ì§€ì›í•˜ëŠ” ML í”„ë ˆì„ì›Œí¬

ML expert Platform Data Manager SDKëŠ” ë‹¤ìŒ ML í”„ë ˆì„ì›Œí¬ì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤:

- **PyTorch** (2.0+)
- **TensorFlow** (2.6+)
- **JAX** (3.14+)
- **NumPy**
- **Pandas**
- **Polars**

```python
# PyTorch DataLoaderì™€ í•¨ê»˜ ì‚¬ìš©
from torch.utils.data import DataLoader

dataset = load_dataset("mlx-data-manager/dataset-name")
dataloader = DataLoader(dataset["train"], batch_size=32)

# Pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = dataset["train"].to_pandas()
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì˜¤ë¥˜**
   ```bash
   # í™˜ê²½ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
   echo $MLX_ENDPOINT_URL
   echo $MLX_APIKEY
   ```

2. **ìºì‹œ ì •ë¦¬**
   ```shell
   $ rm -rf ~/.cache
   ```

3. **ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ**
   - ë°©í™”ë²½ ì„¤ì • í™•ì¸
   - ML expert Platform Data Manager ì—”ë“œí¬ì¸íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” [Apache License 2.0](LICENSE) í•˜ì— ë¼ì´ì„ ìŠ¤ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.

## ğŸ”— ê´€ë ¨ í”„ë¡œì íŠ¸

- [Hugging Face Datasets](https://github.com/huggingface/datasets)
- [Hugging Face Hub](https://github.com/huggingface/huggingface_hub)

---

ML expert Platform Data Manager SDKë¡œ ë” íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ê´€ë¦¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”! ğŸš€
